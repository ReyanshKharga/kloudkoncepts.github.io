{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kloud Koncepts","text":"<p>Learn AWS, Kubernetes, DevOps, Microservices, Service Mesh, CI/CD, Logging, Monitoring, and more all in one place.</p> <p>Check out the Amazon EKS Masterclass: Kubernetes and Microservices on EKS course.</p>"},{"location":"devops-and-sre-interview-questions/","title":"Introduction","text":"<p>Start here: Computer Networking Questions</p>"},{"location":"devops-and-sre-interview-questions/linux/","title":"Interview Questions on Linux","text":""},{"location":"devops-and-sre-interview-questions/linux/#question-1-what-is-the-load-average-in-linux-top-command-and-how-is-it-calculated","title":"Question 1: What is the load average in Linux <code>top</code> command, and how is it calculated?","text":"<p>Load average in Linux, as displayed in the htop command, represents the average system load over a specific time period. It is a measure of the CPU and I/O activity on the system. The load average is typically displayed in three values, corresponding to the last 1, 5, and 15 minutes.</p> <p>Here's a breakdown of how it's calculated:</p> <ol> <li> <p>Single-Core System:</p> <p>If the load average is <code>1.0</code>, it means the system is fully utilized. For example, a load average of <code>2.0</code> on a single-core system suggests that the system is taking twice the time to process tasks than it would if it were idle.</p> </li> <li> <p>Multi-Core System:</p> <p>On a system with multiple cores, a load average of <code>1.0</code> indicates that one core is fully utilized. A load average of <code>2.0</code> on a dual-core system means that both cores are fully utilized.</p> </li> <li> <p>Understanding Load Average Values:</p> <p>The load average values represent the average number of processes that are either in a runnable state or waiting for I/O.</p> <p>A load average of <code>0.5</code> on a single-core system, for instance, means that the system was, on average, half utilized over the specified time period.</p> </li> <li> <p>Interpreting Load Average:</p> <p>Values below the number of available cores generally indicate a healthy system.</p> <p>Values significantly higher than the number of cores may suggest a bottleneck, and the system might be struggling to keep up with the load.</p> </li> <li> <p>Time Intervals:</p> <p>The three load average values correspond to different time intervals \u2013 1 minute, 5 minutes, and 15 minutes. They provide a sense of the system's load over short, medium, and longer durations.</p> </li> <li> <p>Calculating Load Average:</p> <p>The load average is calculated by tracking the number of processes in the system's run queue, both running and waiting for resources, over a specific time period.</p> </li> </ol> <p>In summary, load average is a valuable metric for understanding how busy a system is and whether it's handling the current workload effectively. High load averages can indicate potential performance issues, prompting further investigation into resource constraints or inefficient processes.</p>"},{"location":"devops-and-sre-interview-questions/linux/#question-2-what-do-the-columns-pri-ni-virt-res-shr-and-s-signify-in-the-htop-command","title":"Question 2: What do the columns <code>PRI</code>, <code>NI</code>, <code>VIRT</code>, <code>RES</code>, <code>SHR</code>, and <code>S</code> signify in the <code>htop</code> command?","text":"<p>In the <code>htop</code> command, which is an interactive process viewer for Unix systems, the <code>PRI</code>, <code>NI</code>, <code>VIRT</code>, <code>RES</code>, <code>SHR</code>, and <code>S</code> columns represent various attributes of the processes.</p> <p>Here's an explanation of each:</p> <ol> <li> <p>PRI (Priority):</p> <p>This column shows the priority of the process. The priority is a value that determines the order in which processes are scheduled to run. Lower values indicate higher priority. In the context of htop, positive values represent user priority, and negative values represent system priority.</p> </li> <li> <p>NI (Nice value):</p> <p>The nice value represents the \"niceness\" of a process. It is a user-space priority value that ranges from -20 to 19. A lower nice value means higher priority. Processes with a higher nice value are considered less important and receive fewer CPU resources.</p> </li> <li> <p>VIRT (Virtual Memory):</p> <p>This column displays the total virtual memory used by a process. Virtual memory includes both physical RAM and swap space on disk. It represents the total address space that a process is able to access, including both used and unused portions.</p> </li> <li> <p>RES (Resident Set Size):</p> <p>The RES column shows the resident set size, which is the portion of a process's memory that is held in RAM (non-swapped). It indicates the amount of physical memory that a process is actively using.</p> </li> <li> <p>SHR (Shared Memory):</p> <p>This column displays the amount of shared memory used by a process. Shared memory is memory that can be simultaneously accessed by multiple processes. It allows processes to share data and communicate with each other more efficiently.</p> </li> <li> <p>S (Status):</p> <p>The status column indicates the current status of the process. Common status codes include:</p> <ul> <li><code>R</code>: Running</li> <li><code>S</code>: Sleeping</li> <li><code>D</code>: Disk sleep (waiting for I/O)</li> <li><code>Z</code>: Zombie (terminated but not yet reaped by its parent)</li> <li><code>T</code>: Traced or stopped</li> </ul> </li> </ol> <p>Understanding these columns can provide valuable insights into the resource usage and behavior of processes on your system. Keep in mind that <code>htop</code> provides real-time updates, making it a powerful tool for monitoring and managing processes interactively.</p>"},{"location":"devops-and-sre-interview-questions/networking/","title":"Interview Questions on Computer Networking","text":""},{"location":"devops-and-sre-interview-questions/networking/#question-1-explain-the-osi-model-and-its-layers-how-do-these-layers-relate-to-networking","title":"Question 1: Explain the OSI model and its layers. How do these layers relate to networking?","text":"<p>Imagine the OSI (Open Systems Interconnection) model as a way to understand how computers and devices communicate with each other over a network. It's like a recipe with different layers, and each layer has a specific job.</p> <p>Here are the seven layers of the OSI model, presented top-down, starting with the application layer (layer 7) that directly interacts with the end user and moving downward to the physical layer (layer 1).</p> <p>7. Application Layer </p> <p>This layer is where you directly interact with network services and software applications. It includes web browsers, email clients, and more. This layer enables you to perform various tasks and services over the network, making it your gateway to everything you do online.</p> <p>Application Layer</p> <p>Think of it as the user interface, where you access services like email, web browsing, or file sharing.</p> <p>6. Presentation Layer</p> <p>This layer acts as a translator and protector of data. It's like the language interpreter, responsible for translating data into a format that both the sender and receiver can understand. Additionally, it handles data compression and encryption, safeguarding data during transmission. In essence, it ensures that data is presented in a way that both the sender and receiver can comprehend and keeps it secure by encoding it, similar to how you might encrypt sensitive information before sending it online.</p> <p>Let's use an example to illustrate how the Presentation Layer works to translate and decode data.</p> <p>Translation at the Sender's Device:</p> <p>Suppose you're sending a message with an emoji, let's say a \"smiling face\" emoji \ud83d\ude0a, from your smartphone to your friend's smartphone. Your message goes through the following process:</p> <ol> <li>You select the \"smiling face\" emoji in your messaging app.</li> <li>The Presentation Layer on your device translates the emoji into a standardized code or encoding, such as Unicode. In the case of the \"smiling face\" emoji, it might be represented as \"U+1F60A\" in Unicode.</li> </ol> <p>Transmission over the Network:</p> <p>Your device then sends this encoded data, \"U+1F60A,\" over the network using the HTTP (Hypertext Transfer Protocol) to your friend's device.</p> <p>Decoding at the Receiver's Device:</p> <ol> <li> <p>Your friend's device receives the data, which includes \"U+1F60A.\"</p> </li> <li> <p>The Presentation Layer on your friend's device decodes the \"U+1F60A\" Unicode representation back into the \"smiling face\" emoji \ud83d\ude0a that your friend can understand and see in their messaging app.</p> </li> </ol> <p>So, the Presentation Layer translates the emoji from its visual representation into a standardized encoding for transmission, and then, at the receiving end, it decodes the encoded data back into the original emoji, ensuring that both sender and receiver can see the same \"smiling face\" emoji in their messaging apps.</p> <p>Presentation Layer</p> <p>It's like translating a letter from one language to another so that both the sender and receiver can understand it, and it also keeps your data secure by encoding it.</p> <p>5. Session Layer</p> <p>This layer is like a conversation manager in the digital world. It initiates, maintains, and ends communication sessions between devices, ensuring orderly and secure data exchange. It handles tasks such as session establishment, maintenance, and termination, as well as synchronization between devices, error recovery, and checkpointing. Essentially, it's the manager overseeing communication between devices, making sure data flows smoothly and securely during tasks like online gaming or video conferencing.</p> <p>Session Layer</p> <p>It's like keeping track of a phone call. It starts when you call someone, continues while you talk, and ends when you hang up.</p> <p>4. Transport Layer</p> <p>This layer manages data's smooth and reliable transfer between devices. This layer handles tasks such as breaking data into smaller segments, numbering them for proper sequencing, and error-checking to guarantee the integrity of data. It relies on protocols like TCP (Transmission Control Protocol) for connection-oriented, error-reliable communication and UDP (User Datagram Protocol) for faster, connectionless data transfer. It's responsible for making sure your online activities like web browsing and video streaming happen seamlessly.</p> <p>Transport Layer</p> <p>It's like the organizer of a conversation, ensuring that data gets to the right place in the correct order.</p> <p>3. Network Layer</p> <p>This layer is like the postal service of computer networks. It's responsible for routing data across larger networks, deciding the best path for data to travel, and ensuring it reaches its destination. This layer handles tasks such as addressing devices with logical IP addresses, maintaining a routing table for efficient data movement, and using protocols like IP to manage the delivery of data packets.</p> <p>Network Layer</p> <p>It's essentially the traffic director of the network, guiding data packets to their intended locations.</p> <p>2. Data Link Layer</p> <p>This layer manages how data is packaged and sent over a network. It divides data into frames for clear transmission, assigns unique MAC addresses for source and destination, performs error checking, regulates data flow, controls access to the network medium, and plays a key role in technologies like Ethernet and Wi-Fi.</p> <p>Data Link Layer</p> <p>It's like the organizer of data packages, ensuring they are correctly addressed, free from errors, and sent efficiently in local network segments while preventing collisions.</p> <p>1. Physical Layer</p> <p>This is the lowest layer. It deals with the actual physical connection between devices. Think of it like the wires, cables, and the hardware that make your devices connect to the network. </p> <p>Physical Layer</p> <p>It's all about sending and receiving raw bits (0s and 1s).</p> <p>These layers work together to ensure that data can be sent and received across a network. Each layer has its specific job, and they rely on each other to get the data where it needs to go, whether it's browsing the internet, sending an email, or any other network-related task.</p> <p>So, the OSI model is like a set of instructions that helps devices communicate effectively, ensuring that your data is sent and received correctly in a complex networked world.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-2-what-is-the-difference-between-tcp-and-udp-and-when-would-you-use-one-over-the-other","title":"Question 2: What is the difference between TCP and UDP, and when would you use one over the other?","text":"<p>TCP (Transmission Control Protocol): TCP is a communication protocol used for sending and receiving data over networks. It's like a detailed and organized postal service. When you send data using TCP, it ensures that your data arrives at its destination accurately, and in the right order. It's like sending a letter with a return receipt to be absolutely certain it got there.</p> <p>UDP (User Datagram Protocol): UDP is another communication protocol for sending data over networks. It's more like a speedy courier service. When you use UDP, it's all about sending data as fast as possible. It doesn't check if every piece of data arrives, and it doesn't care about the order. It's like sending postcards \u2013 they might get there quickly, but some might be lost along the way.</p> <p>Now, let's look at the differences:</p> <ol> <li> <p>Reliability:</p> <ul> <li>TCP: It's highly reliable. <code>TCP</code> ensures that data arrives without errors and in the correct order.</li> <li>UDP: It's less reliable. <code>UDP</code> doesn't guarantee that all data will arrive, and it doesn't check for order.</li> </ul> </li> <li> <p>Speed:</p> <ul> <li>TCP: It's a bit slower than <code>UDP</code> because of all the checks and verifications it does.</li> <li>UDP: It's faster because it doesn't spend time double-checking everything.</li> </ul> </li> <li> <p>Use Cases:</p> <ul> <li>TCP: Use it for tasks where accuracy and completeness are crucial, like web browsing, email, file transfers, and situations where you need all the data to be correct.</li> <li>UDP: Use it when speed is more important, like in real-time applications such as online gaming, video streaming, or live video calls, where a slight delay is acceptable.</li> </ul> </li> </ol> <p>In a nutshell, you'd choose <code>TCP</code> when you want to be sure that your data gets there correctly and in order, even if it means a bit of a delay. On the other hand, you'd go with <code>UDP</code> when speed is critical, and it's okay to lose some data here and there, like in real-time applications where a slight delay would be very noticeable.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-3-what-is-dns-and-how-does-it-work-explain-the-difference-between-an-a-record-and-a-cname-record","title":"Question 3: What is DNS, and how does it work? Explain the difference between an A record and a CNAME record.","text":"<p>DNS (Domain Name System):</p> <p>DNS is like a phone book for the internet. It helps you find websites and services using human-friendly names (like www.example.com) instead of complicated numerical IP addresses that computers use to locate each other.</p> <p>How it Works:</p> <p>When you type a website's name (e.g., \"www.example.com\") into your browser, your computer doesn't understand that. So, it asks a DNS server for help. The DNS server is like a librarian who knows where all the books (websites) are in the library (internet). It looks up the name you provided and tells your computer the corresponding IP address. Your computer then uses that IP address to connect to the right website's server and load the page.</p> <p>Difference between <code>A</code> Record and <code>CNAME</code> Record:</p> <ul> <li>An <code>A</code> record is like a direct address in the DNS system. It says, \"This domain name (like www.example.com) goes to this specific IP address (like 192.168.1.1).\"</li> <li>A <code>CNAME</code> record is like a shortcut. It says, \"This domain name (like photos.example.com) is the same as another domain (like images.example.com).\"</li> <li>So, with a <code>CNAME</code>, you can point multiple domain names to the same place. It's like saying that both \"photos.example.com\" and \"images.example.com\" lead to the same website, making it easier to manage.</li> </ul> <p>In a nutshell, DNS helps you find websites on the internet by translating human-friendly names into computer-friendly addresses. A records directly connect a domain name to an IP address, while CNAME records provide shortcuts to other domain names, making it handy for managing multiple names that lead to the same place.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-4-what-is-latency-and-how-can-it-be-minimized-in-a-network","title":"Question 4: What is latency, and how can it be minimized in a network?","text":"<p>Latency:</p> <p>Latency is like the waiting time on the internet. It's the delay between sending a request (like clicking a link) and getting a response (like a webpage loading). It's how long you wait for things to happen online.</p> <p>Minimizing Latency:</p> <p>To make latency as short as possible:</p> <ol> <li>Use Faster Connections: If you have a faster internet connection, it's like having a quicker highway for your data to travel.</li> <li>Choose Nearby Servers: It's like going to a restaurant that's close by rather than far away. Servers closer to you respond faster.</li> <li>Optimize Data: Compress data and use efficient coding to make it smaller, so it takes less time to travel.</li> <li>Reduce Congestion: Fewer cars on the road mean less traffic. Avoid busy times on the internet. Use Content Delivery Networks (CDNs): CDNs are like having branches of a store in many locations. They store web content closer to you, so you get it faster.</li> <li>Upgrade Hardware: Better devices and computers process data faster, reducing wait times.</li> </ol> <p>Remember, minimizing latency is all about making your internet experience quicker, like making sure your pizza arrives at your door faster after you order it.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-5-what-is-a-subnet-mask-and-how-is-it-used-in-ip-addressing","title":"Question 5: What is a subnet mask, and how is it used in IP addressing?","text":"<p>Subnet Mask:</p> <p>A subnet mask is like a special tool that helps your computer know who is in the same neighborhood on the internet. It's a set of numbers that's used to separate IP addresses into two parts: the network part (the neighborhood) and the host part (the house within the neighborhood).</p> <p>How it's Used in IP Addressing:</p> <ul> <li> <p>Imagine an IP address is like a street address with a house number and a neighborhood. The subnet mask helps your computer figure out which part is the neighborhood and which part is the house number.</p> </li> <li> <p>For example, if you have an IP address like <code>192.168.1.50</code> and a subnet mask like <code>255.255.255.0</code>, your computer knows that the \"192.168.1\" part is the neighborhood, and \"50\" is the specific house.</p> </li> <li> <p>It's used to determine if the computer you want to talk to is on the same neighborhood network or a different one. If it's in the same neighborhood, your computer can communicate directly. If it's in a different neighborhood, your computer talks to a router to find the right path.</p> </li> </ul> <p>In short, a subnet mask is like a dividing line that helps your computer know which part of an IP address is the neighborhood and which part is the specific house. It's essential for making sure data goes to the right place on the internet.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-6-what-is-the-purpose-of-having-a-router","title":"Question 6: What is the purpose of having a router?","text":"<p>When a computer wants to talk to another computer on a different local network (neighborhood), it's like living in one neighborhood and trying to reach someone in a different neighborhood, maybe across town.</p> <p>To make this work, you need a \"router.\" A router is like a bridge that connects different neighborhoods (networks) together. It knows how to find the right path to the other neighborhood.</p> <p>The subnet mask helps your computer figure out if the destination computer is in the same neighborhood or a different one. If it's in the same neighborhood, your computer talks directly to it. If it's in a different neighborhood, your computer asks the router to send the message to the right place, just like you'd ask for directions to get to a different neighborhood.</p> <p>So, subnet masks help your computer know whether the destination computer is nearby (in the same network) or far away (in a different network). When they're nearby, they can chat directly, but for far-away connections, they need a router to help them find the way. It's all about efficient and organized communication on the internet.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-7-what-does-hop-mean-in-networking","title":"Question 7: What does hop mean in networking?","text":"<p>In networking, a \"hop\" refers to each intermediate device or router that data packets pass through on their journey from a source to a destination. These devices are like stepping stones that help the data travel across a network. Each time data moves from one router to the next, it's counted as one hop.</p> <p>For example, let's say you're sending a message from your computer to a server on the internet. The data might first go through your home router, then to your Internet Service Provider's (ISP) router, and from there, it might pass through several more routers in your ISP's network and potentially through routers at data centers and across the backbone of the internet. Each transition between routers counts as a hop.</p> <p>Hops are important because they can affect the speed and efficiency of data transmission. More hops generally mean more delays in data delivery. Therefore, network engineers work to optimize routing to reduce the number of hops and make data travel faster and more efficiently.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-8-explain-the-concept-of-a-load-balancer-how-can-load-balancing-improve-the-performance-and-reliability-of-a-service","title":"Question 8: Explain the concept of a load balancer. How can load balancing improve the performance and reliability of a service?","text":"<p>Load Balancer:</p> <p>A load balancer is like a traffic cop for websites and apps. It stands in front of a bunch of servers (computers) and helps spread out the work evenly, like a traffic cop directing cars so no single road gets too jammed.</p> <p>How It Works:</p> <ul> <li> <p>Imagine you have a popular website, and lots of people want to visit. Instead of one server doing all the work, a load balancer splits the traffic among many servers.</p> </li> <li> <p>It looks at each incoming request (like someone trying to access a web page) and decides which server should handle it. It's like the traffic cop deciding which road to send each car.</p> </li> <li> <p>This balancing makes sure no server gets overwhelmed, keeps the website running fast, and if one server has a problem, the load balancer can send traffic to the healthy ones. It's like having backup roads ready if one gets blocked.</p> </li> </ul> <p>Improving Performance and Reliability:</p> <ul> <li> <p>Load balancing improves performance by making sure no server is working too hard. It's like sharing the load among many helpers, so they don't get tired.</p> </li> <li> <p>It makes services more reliable because if one server has trouble, the load balancer can quickly switch to another one, so the website or app doesn't crash. It's like having a spare car when yours breaks down, so you can keep going.</p> </li> </ul> <p>In a nutshell, a load balancer helps websites and apps handle lots of visitors smoothly by spreading the work across multiple servers, keeping things fast and reliable, just like a traffic cop manages busy intersections to prevent traffic jams.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-9-what-is-ssltls-and-how-does-it-contribute-to-network-security","title":"Question 9: What is SSL/TLS, and how does it contribute to network security?","text":"<p><code>SSL/TLS</code> is like a secret code that keeps your internet conversations safe.</p> <p>SSL (Secure Sockets Layer) and TLS (Transport Layer Security):</p> <p>Imagine you're sending a letter to a friend, and you want it to be private. SSL and TLS are like a magical envelope that scrambles your letter into a secret code before sending it.</p> <p>How It Works:</p> <ul> <li> <p>When you visit a secure website (you'll see \"https://\" in the address), <code>SSL/TLS</code> creates a secure connection between your computer and the website's server.</p> </li> <li> <p>It's like having a secret language that only you and the website can understand. So, when you send sensitive information, like passwords or credit card numbers, it's like sending a secret message that no one else can easily read.</p> </li> </ul> <p>Contributing to Network Security:</p> <ul> <li> <p><code>SSL/TLS</code> is a big part of keeping your information safe on the internet. It helps protect your data from eavesdroppers who might try to intercept it.</p> </li> <li> <p>This security is important for online shopping, banking, and pretty much any time you send private information. It's like having a locked safe for your digital secrets.</p> </li> </ul> <p>So, <code>SSL/TLS</code> is like putting your data in a locked, secret envelope when you send it over the internet, making sure it's safe from prying eyes and contributing to network security.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-10-explain-how-ssltls-and-https-secure-your-web-browsing-with-a-step-by-step-process","title":"Question 10: Explain How SSL/TLS and HTTPS Secure Your Web Browsing with a Step-by-Step Process?","text":"<p>Let's walk through the process, step by step, from when a server implements <code>HTTPS</code> using a certificate to when a user tries to access a website using <code>HTTPS</code>:</p> <ol> <li> <p>Certificate Installation:</p> <p>The process begins with the website's server administrator obtaining an <code>SSL/TLS</code> certificate. This certificate is usually obtained from a trusted Certificate Authority (CA). The administrator installs the certificate on the web server. This certificate includes a public key and other information about the website.</p> </li> <li> <p>Configuration:</p> <p>The server administrator configures the web server software (e.g., Apache, Nginx, IIS) to enable <code>HTTPS</code>. This involves specifying the certificate file, setting encryption protocols, and configuring security parameters.</p> </li> <li> <p>User Access:</p> <p>When a user attempts to access the website via <code>HTTPS</code>, they type \"https://\" followed by the website's domain name in their web browser's address bar.</p> </li> <li> <p>DNS Resolution:</p> <p>The user's web browser sends a request to a Domain Name System (DNS) server to resolve the domain name (e.g., www.example.com) into an IP address.</p> </li> <li> <p>Server Hello:</p> <p>The web server receives the user's <code>HTTPS</code> request and responds with a \"Server Hello\" message, indicating that it's ready to initiate a secure connection. The server also sends its digital certificate. It's like the server extending a warm welcome and establishing the foundation for a secure communication channel.</p> </li> <li> <p>Certificate Verification:</p> <p>The user's web browser checks the digital certificate received from the server. It confirms that the certificate is signed by a trusted Certificate Authority (CA), ensuring the server's identity. If not, the browser issues a warning.</p> </li> <li> <p>Key Exchange:</p> <p>After verifying the certificate, the browser generates a unique, symmetric encryption key. This key is then encrypted with the server's public key (from the certificate) and sent back to the server.</p> </li> <li> <p>Data Encryption:</p> <p>Both the user's browser and the server now have the same encryption key. They use it to encrypt and decrypt data sent between them. The secure SSL/TLS connection is established, ensuring that all data transmitted is encrypted and secure.</p> </li> <li> <p>Secure Communication:</p> <p>The user's browser displays a padlock icon or other security indicators to show that the connection is secure. From this point, all data sent between the user and the website is encrypted and protected from eavesdropping.</p> </li> <li> <p>User Interaction:</p> <p>The user can now interact with the website, send sensitive information (like login credentials or payment details), and receive secure data from the server. In this process, HTTPS provides a secure and encrypted connection, ensuring that the data exchanged between the user's web browser and the website's server remains confidential and protected. This security is vital for online transactions, secure logins, and safeguarding sensitive information on the internet.</p> </li> </ol>"},{"location":"devops-and-sre-interview-questions/networking/#question-11-what-is-the-difference-between-an-a-record-and-an-aaa-record","title":"Question 11: What is the difference between an A record and an AAA record?","text":"<p>The difference between an <code>A</code> record and an <code>AAAA</code> record lies in the type of IP addresses they are associated with:</p> <p><code>A</code> Record (Address Record):</p> <ul> <li> <p>An <code>A</code> record is used to map a domain name (like \"example.com\") to an <code>IPv4</code> address (e.g., <code>192.0.2.1</code>).</p> </li> <li> <p>IPv4 addresses are the older, more common type of IP addresses, represented as a series of four sets of numbers, separated by periods (e.g., <code>192.0.2.1</code>). These addresses are still widely used but are becoming scarce due to the limited number of available combinations.</p> </li> </ul> <p><code>AAAA</code> Record (Quad-A Record):</p> <ul> <li> <p>An <code>AAAA</code> record, on the other hand, maps a domain name to an <code>IPv6</code> address (e.g., <code>2001:0db8:85a3:0000:0000:8a2e:0370:7334</code>).</p> </li> <li> <p>IPv6 addresses are the newer type of IP addresses designed to address the <code>IPv4</code> address exhaustion issue. They are represented as a longer series of hexadecimal digits, separated by colons (e.g., <code>2001:0db8:85a3:0000:0000:8a2e:0370:7334</code>). <code>IPv6</code> provides a vastly larger address space.</p> </li> </ul> <p>In summary, the key difference is that <code>A</code> records are used with <code>IPv4</code> addresses, while <code>AAAA</code> records are used with <code>IPv6</code> addresses. These records are essential for domain name resolution, allowing you to connect to websites and other online services using the appropriate IP version.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-12-what-happens-when-you-type-googlecom-in-your-web-browser","title":"Question 12: What happens when you type google.com in your web browser?","text":"<p>Let's walk through the process step by step, from the moment you type \"google.com\" into your browser's address bar:</p> <ol> <li> <p>User Input:</p> <p>You open your web browser and type \"google.com\" into the address bar.</p> </li> <li> <p>DNS Resolution Request:</p> <ul> <li> <p>Your web browser sends a DNS resolution request to your local DNS resolver, which is usually provided by your Internet Service Provider (ISP).</p> </li> <li> <p>This request is like asking the local librarian for a book's location in a library.</p> </li> </ul> </li> <li> <p>Local DNS Resolver Check:</p> <p>The local DNS resolver checks if it already knows the IP address associated with \"google.com.\" If it does, it provides the IP address directly. This is similar to the librarian telling you the book's location without checking the catalog.</p> </li> <li> <p>Recursive DNS Query:</p> <p>If the local DNS resolver doesn't have the IP address, it performs a recursive DNS query. It contacts the root DNS server to find out which DNS server knows the authoritative DNS server for \"google.com.\"</p> <p>Note</p> <p>A recursive DNS query is a process by which a DNS resolver, typically operated by your Internet Service Provider (ISP) or a public DNS service (e.g., Google's 8.8.8.8), seeks the complete answer to a DNS query on your behalf.</p> </li> <li> <p>Root DNS Server:</p> <p>The root DNS server doesn't know the IP address for \"google.com,\" but it directs the local DNS resolver to the top-level domain (TLD) DNS server responsible for \".com.\"</p> </li> <li> <p>Top-Level Domain (TLD) DNS Server:</p> <p>The TLD DNS server for \".com\" is contacted. It doesn't have the exact IP address either but can guide the local DNS resolver to the authoritative DNS server for \"google.com.\"</p> </li> <li> <p>Authoritative DNS Server:</p> <p>The authoritative DNS server for \"google.com\" is finally contacted. It holds the IP address associated with \"google.com.\"</p> </li> <li> <p>IP Address Retrieval:</p> <p>The authoritative DNS server sends the IP address for \"google.com\" back to the local DNS resolver.</p> </li> <li> <p>Cache Update:</p> <p>The local DNS resolver updates its cache with the IP address for \"google.com\" so that it can respond quickly to future requests for the same domain.</p> </li> <li> <p>DNS Response:</p> <p>The local DNS resolver sends the IP address to your web browser.</p> </li> <li> <p>Establishing Connection:</p> <p>Your web browser establishes a connection to the web server associated with the provided IP address for \"google.com.\"</p> </li> <li> <p>Data Retrieval:</p> <p>Your browser sends an HTTP request to the server for the specific webpage you want, such as the Google homepage.</p> </li> <li> <p>Web Server Processing:</p> <p>Google's web server processes the request and generates the HTML content for the Google homepage.</p> </li> <li> <p>Data Transmission:</p> <p>The HTML content is transmitted from the web server back to your browser.</p> </li> <li> <p>Rendering the Webpage:</p> <p>Your browser receives the HTML content and renders it as the familiar Google homepage, including text, images, links, and search fields.</p> </li> <li> <p>Additional Resources:</p> <p>Your browser may request additional resources (like CSS files and JavaScript) to fully load and display the webpage.</p> </li> </ol> <p>In summary, when you type \"google.com\" in your browser, a detailed sequence of DNS resolution steps is initiated to find the IP address associated with that domain. Once the IP address is obtained, your browser connects to Google's server to retrieve and display the webpage content. DNS is crucial in translating human-readable domain names into the actual IP addresses needed to navigate the internet.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-13-what-is-a-firewall-and-how-does-it-enhance-network-security","title":"Question 13: What is a firewall, and how does it enhance network security?","text":"<p>Firewall:</p> <p>A firewall is like a security guard for your computer or network. It's a barrier that decides what can come in and go out, just like a guard at the entrance of a building.</p> <p>How It Works:</p> <p>Imagine your computer is a castle, and the internet is the outside world. A firewall stands at the castle gates and checks everything that wants to come in or go out. It examines data (like messages and files) to see if they are safe or if they might be like sneaky intruders.</p> <p>Enhancing Network Security:</p> <ul> <li> <p>The firewall's job is to keep the bad stuff out. It can block viruses, hackers, and other harmful things from getting into your computer or network.</p> </li> <li> <p>It can also prevent your private information from accidentally leaking out, like a guard making sure nothing valuable is taken from the castle.</p> </li> </ul> <p>In a nutshell, a firewall is like a protective guard that watches over your computer or network, making sure only the good things get through and keeping the bad things out, enhancing your security in the digital world.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-14-what-are-vlans-virtual-lans-and-why-are-they-used-in-network-design","title":"Question 14: What are VLANs (Virtual LANs), and why are they used in network design?","text":"<p>VLANs (Virtual LANs):</p> <p>Think of VLANs as digital fences inside a big office building. In a huge office, you might want different departments like marketing, IT, and sales to have their own space. VLANs do something similar but in the digital world.</p> <p>How They Work:</p> <ul> <li> <p>Imagine your office has a single big network cable like a big, shared hallway. Without VLANs, everyone uses the same hallway, which can get crowded and confusing.</p> </li> <li> <p>With VLANs, it's like having separate, invisible walls or corridors within that big hallway. Each department has its own space, and they can't see or interfere with each other.</p> </li> </ul> <p>Why Use VLANs in Network Design:</p> <ul> <li> <p>VLANs are used to organize and secure networks. They help keep different parts of a network separate, even if they're physically connected to the same network.</p> </li> <li> <p>For example, you can put the HR department's data on one VLAN and the finance department's data on another. This way, HR can't accidentally access finance data, making the network more secure.</p> </li> </ul> <p>Technical Details:</p> <ul> <li> <p>VLANs use special tags on data packets to keep them separated. Devices that are part of a VLAN recognize these tags and know which group the data belongs to.</p> </li> <li> <p>Network switches are the devices that manage VLANs. They decide which data goes to which VLAN based on the tags.</p> </li> <li> <p>VLANs also reduce network traffic because they only send data to the devices in the same VLAN, reducing unnecessary data clutter.</p> </li> </ul> <p>In summary, VLANs are like invisible dividers in a large office, helping organize and secure the network by keeping different parts separate. This makes it easier to manage and more secure, especially in larger networks where different departments or groups need their own space.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-15-explain-the-difference-between-stateless-and-stateful-firewalls-when-would-you-use-one-over-the-other","title":"Question 15: Explain the difference between stateless and stateful firewalls. When would you use one over the other?","text":"<ol> <li> <p>Stateless Firewall:</p> <p>A stateless firewall is like a simple gatekeeper that watches the traffic but doesn't remember what happened before. It treats each piece of data separately, just like checking IDs at a party without knowing who was there before.</p> <ul> <li> <p>How It Works:</p> <p>When data comes in or goes out, the stateless firewall looks at individual pieces of information but doesn't consider the history or context. It makes decisions based only on that data.</p> </li> <li> <p>When to Use:</p> <p>You might use a stateless firewall for basic protection when you don't need to remember past interactions. It's like a bouncer who just checks IDs and doesn't care if you've been to the party before.</p> </li> </ul> </li> <li> <p>Stateful Firewall:</p> <p>A stateful firewall is like a smart guard who remembers who's been in and out of the party. It keeps track of connections and knows if data packets belong to an established, trusted conversation.</p> <ul> <li> <p>How It Works:</p> <p>This firewall keeps a record of ongoing conversations and ensures that incoming data matches the context of an established connection. It's like remembering someone's face from earlier in the party.</p> </li> <li> <p>When to Use:</p> <p>You'd use a stateful firewall when you want more advanced security. It's like having a vigilant host at the party who checks if you've been invited before and if you're following the rules of the event.</p> </li> </ul> </li> </ol> <p>In summary, a stateless firewall simply looks at individual data packets, while a stateful firewall remembers the context of connections. You might use a stateless firewall for basic protection and a stateful one for more advanced security, especially when you need to track and manage ongoing interactions on your network.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-16-what-is-bgp-border-gateway-protocol-and-why-is-it-important-in-internet-routing","title":"Question 16: What is BGP (Border Gateway Protocol), and why is it important in internet routing?","text":"<p>BGP (Border Gateway Protocol):</p> <p>Think of BGP as the boss of all internet traffic cops. It's the protocol that helps the internet know how to send data from one place to another, like GPS for the internet. How It Works:</p> <p>Imagine the internet as a massive map with cities (networks). BGP is like the GPS system that helps data packets find the quickest route from one city to another.</p> <p>Why It's Important:</p> <ul> <li> <p>BGP is crucial because it helps the internet stay organized and efficient. It constantly updates routes, so data takes the shortest path to its destination.</p> </li> <li> <p>It's like making sure delivery trucks always use the best, traffic-free routes to reach their destinations quickly.</p> </li> </ul> <p>Technical Details:</p> <ul> <li> <p>BGP routers, managed by big companies and internet service providers, share information about the fastest routes. They update this info constantly.</p> </li> <li> <p>BGP makes sure data packets travel efficiently by choosing the quickest paths, and it can reroute traffic if a path becomes slow or goes down.</p> </li> </ul> <p>In a nutshell, BGP is like the internet's navigation system, ensuring data packets find the fastest way to their destination. It's vital for keeping the internet running smoothly and efficiently, just like a traffic cop directing cars on busy roads.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-17-what-is-a-vpn-virtual-private-network-and-how-does-it-work-to-secure-network-communications","title":"Question 17: What is a VPN (Virtual Private Network), and how does it work to secure network communications?","text":"<p>Think of the internet like a big highway with lots of cars (data) traveling on it. When you connect to the internet, your data is like a car on that highway, and anyone can see it if they want to.</p> <p>Now, a VPN is like a special tunnel for your car. It's a secure, private road within the internet highway. When you use a VPN, your data travels through this tunnel instead of on the open highway.</p> <p>Here's how it works to keep your data safe:</p> <ul> <li> <p>Encryption: Inside the VPN tunnel, your data is encrypted, which means it's turned into a secret code. It's like putting your data inside a locked box. This makes it really hard for anyone to understand or steal your information.</p> </li> <li> <p>Anonymity: When you use a VPN, your real location and identity are hidden. It's like wearing a mask while driving. People can't easily tell where you are or who you are.</p> </li> <li> <p>Secure Connection: Your data goes from your device to the VPN server (the start of the tunnel) before going out to the wider internet. This extra step adds a layer of security.</p> </li> <li> <p>Changing Location: Many VPNs have servers all around the world. When you connect to one of these servers, it can make it look like your data is coming from a different place. This can help protect your privacy and access content that might be blocked in your country.</p> </li> </ul> <p>So, a VPN is like your private, secret, and safe road on the internet. It keeps your data hidden and secure while you browse, work, or do anything online. It's especially useful when you're using public Wi-Fi, like at a coffee shop or airport, where others could try to snoop on your internet traffic.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-18-what-is-a-cdn-content-delivery-network-and-why-would-you-use-it-for-your-web-applications","title":"Question 18: What is a CDN (Content Delivery Network), and why would you use it for your web applications?","text":"<p>What is a CDN?</p> <p>Imagine you have a favorite toy store, but it's far away from your home. You want to get your toys quickly and easily. A CDN is like having mini versions of that store in various places all around your neighborhood.</p> <p>Here's why it's useful:</p> <ul> <li> <p>Faster Delivery: Instead of traveling a long way to the faraway store, you go to the mini store nearby. CDNs are like those mini stores, but for websites and web apps. They have copies of website content (like images, videos, and text) in many places all over the world. When you visit a website that uses a CDN, you're actually getting content from the mini store closest to you. This makes everything load much faster.</p> </li> <li> <p>Reliability: If one mini store is busy or has a problem, you can easily go to the next one. CDNs are built to be reliable, so even if one part of the network has issues, your website or app can still work well because it can switch to a different mini store (server).</p> </li> <li> <p>Reducing Traffic Jams: Imagine a big traffic jam on the highway to your favorite store. It'll take a long time to get there. CDNs help reduce this \"internet traffic jam\" by spreading out the load. Many people can access a website at once, and CDNs make sure everyone can get their stuff quickly without causing slowdowns.</p> </li> <li> <p>Global Reach: CDNs have mini stores all over the world. This means your website can be super fast for people in different countries too. It's like having a store that can serve customers all over the globe without any delays.</p> </li> <li> <p>Security: CDNs often include security features to protect against things like hackers and bad internet traffic. They're like having security guards at your favorite store to keep it safe.</p> </li> </ul> <p>So, in simple terms, a CDN is like a bunch of mini stores for websites and web apps all over the world. They make the internet faster, more reliable, and safer for everyone who uses them. That's why they're really handy for web applications.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-19-what-is-dhcp-and-what-is-it-used-for","title":"Question 19: What is DHCP, and what is it used for?","text":"<p>What is DHCP?</p> <p>DHCP, or \"Dynamic Host Configuration Protocol,\" is like a friendly organizer for devices in a network. Its job is to give each device on a network its own special address (called an IP address), so they can talk to each other and the internet.</p> <p>Imagine you're in a big school with many classrooms, and each classroom has a name (the IP address). Instead of telling everyone which classroom to go to, the school has a guide (DHCP). When a new student (device) arrives, the guide says, \"Here's your classroom, and here's your seat.\"</p> <p>What is it used for?</p> <ul> <li> <p>Saves You Time: Instead of manually figuring out which classroom you should go to (setting your IP address), DHCP does it for you. This is super handy, especially in large networks.</p> </li> <li> <p>No Confusion: It ensures that no two students (devices) end up in the same classroom (with the same IP address). Just like in a school, you don't want two people using the same desk!</p> </li> <li> <p>Flexibility: You can change classrooms (get a new IP address) whenever you want. Maybe you want to sit in a different classroom tomorrow? DHCP makes that easy.</p> </li> <li> <p>Efficiency: It not only assigns IP addresses but can also give you important information, like where the library (DNS server) is or who the principal is (router's address). This helps you navigate the school (network) more effectively.</p> </li> </ul> <p>In the tech world, DHCP makes connecting to networks as easy as finding your classroom in a school. It takes care of the details so you can focus on using the network without worrying about IP addresses.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-20-what-is-arp-address-resolution-protocol-in-networking-and-how-does-it-work","title":"Question 20: What is ARP (Address Resolution Protocol) in networking and how does it work?","text":"<p>In networking, APR typically stands for \"Address Resolution Protocol\". It is like a phone book for computers on a local network. </p> <p>Here's what it does:</p> <p>Imagine you're in a big office building with lots of rooms. Each room has a unique number, just like every computer on a network has a unique address called an IP address.</p> <p>Now, when you want to visit someone's office, you usually know their name but not their room number. So, you look in the office building's phone book to find their name and get their room number.</p> <p>In networking, ARP does something similar:</p> <ol> <li> <p>Mapping IP to MAC Addresses: Every device on a local network has an IP address (like a room number) and a unique hardware address called a MAC address (like a person's name). When one computer wants to talk to another on the same network, it uses ARP to find the MAC address that corresponds to the IP address.</p> </li> <li> <p>Resolving IP Addresses: If a computer wants to send data to another computer by knowing its IP address, it uses ARP to figure out the corresponding MAC address. This is important because data is actually sent to MAC addresses on a local network, not IP addresses.</p> </li> </ol> <p>So, ARP is like the phone book that helps devices on a local network find each other by translating IP addresses to MAC addresses, making communication between devices possible.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-21-why-is-data-divided-into-segments-for-transmission-over-a-network","title":"Question 21: Why is data divided into segments for transmission over a network?","text":"<p>Data is divided into segments for several important reasons:</p> <ol> <li> <p>Efficient Transmission: Large amounts of data can be more efficiently transmitted in smaller, manageable chunks. Smaller segments are less likely to get lost or corrupted during transmission.</p> </li> <li> <p>Error Handling: Dividing data into segments allows for better error handling. If a segment is lost or corrupted during transmission, only that segment needs to be retransmitted, rather than the entire large file.</p> </li> <li> <p>Optimizing Network Resources: Segmenting data helps in optimizing network resources. It allows multiple applications and devices to share the same network connection. Segmentation ensures fairness, so no single application monopolizes the network.</p> </li> <li> <p>Support for Different Types of Data: Different types of data require different treatment. Segmentation allows data to be prepared for specific handling and processing at different network layers.</p> </li> <li> <p>Flow Control: Segmentation helps with flow control, ensuring that the sender doesn't overwhelm the receiver with data. It allows for a more balanced and controlled transfer of information.</p> </li> <li> <p>Congestion Management: Segmenting data helps network devices manage congestion. If too much data is sent at once, it can lead to network congestion. Segmentation can prevent this from happening.</p> </li> </ol> <p>So, while it might seem more straightforward to send all the data at once, segmenting it into smaller pieces has numerous advantages in terms of efficiency, reliability, and the effective use of network resources. This is why protocols like TCP (Transmission Control Protocol) segment data for transmission over the internet.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-22-what-is-the-difference-between-segments-and-frame","title":"Question 22: What is the difference between Segments and Frame?","text":"<p>Segments and Frames are like containers for data when it's sent over a network.</p> <p>Segments are used when data needs to travel a long distance over the internet. They're like pieces of a puzzle. Imagine you're mailing a jigsaw puzzle to a friend. You might put each piece in a separate envelope, so they reach your friend safely. These separate envelopes are like segments.</p> <p>Frames, on the other hand, are used in your local network, like your Wi-Fi or Ethernet connection at home. Think of frames as individual packages within an envelope. Each package (frame) contains a part of the jigsaw puzzle (your data) and also some information on the outside of the package to make sure it gets to the right place.</p> <p>So, when you're sending data over the internet, you use segments to break it into manageable pieces. When that data reaches your local network, those segments are placed into frames, like putting jigsaw puzzle pieces into smaller packages. This helps your data get to its destination safely and in the right order.</p> <p>Let's break down what happens when you send a \"hello world\" message on WhatsApp to your friend. We'll go through each step in the process and see how segments and frames play a role.</p> <ol> <li> <p>Message Creation (Application Layer):</p> <ul> <li>You open your WhatsApp application, type and send the message \"hello world.\"</li> <li>WhatsApp at the application layer has this message ready for transmission.</li> </ul> </li> <li> <p>Segmentation (Transport Layer):</p> <ul> <li> <p>The \"hello world\" message from WhatsApp is relatively large compared to the network's maximum transmission unit (MTU), which is the largest frame size that can be sent over the network.</p> </li> <li> <p>The transport layer, in this case, might use the Transmission Control Protocol (TCP) to break the message into smaller, manageable segments. For example, it could divide it into two segments: \"hello\" and \" world.\"</p> </li> <li> <p>These segments are like dividing a long message into separate parts for easier handling.</p> </li> </ul> </li> <li> <p>Packetization (Network Layer):</p> <ul> <li>Before the segments are turned into frames, they are further divided into packets at the network layer.</li> <li>Each packet includes the segment and an IP (Internet Protocol) header, which contains:<ul> <li>Source IP address: Your device's public or private IP address.</li> <li>Destination IP address: Your friend's device's public or private IP address.</li> <li>Other information like Time-to-Live (TTL) and protocol information.</li> </ul> </li> </ul> </li> <li> <p>Frame Creation (Data Link Layer):</p> <ul> <li> <p>The packets, now with IP headers, are passed to the data link layer for further processing.</p> </li> <li> <p>The data link layer, which in the case of your Wi-Fi or Ethernet connection, uses frames for transmission, will add control information to each packet to create frames.</p> </li> <li> <p>Each frame typically includes:</p> <ul> <li>Source MAC address: The MAC address of your device.</li> <li>Destination MAC address: The MAC address of your friend's device.</li> <li>Error-checking information.</li> <li>The packet itself: Including the IP header and the segment (\"hello\" or \" world\").</li> </ul> </li> <li> <p>These frames are like putting your packet into envelopes. Each envelope has a sender address, a recipient address, and the data packet, which includes the IP information and your segment.</p> </li> </ul> </li> <li> <p>Transmission (Physical Layer):</p> <ul> <li>The frames are then sent over your network connection, like Wi-Fi or Ethernet. Each frame is individually transmitted from your device to the network infrastructure (routers and switches).</li> </ul> </li> <li> <p>Receiving (Data Link Layer):</p> <ul> <li>On the recipient's side, their device receives these frames.</li> <li>The data link layer checks the MAC addresses to ensure that the frames are meant for the recipient's device.</li> </ul> </li> <li> <p>Deframing (Network Layer):</p> <ul> <li>Once the frames are validated, they are passed to the network layer, which extracts the packets.</li> <li>The network layer reads the IP header to determine the source and destination IP addresses and forwards the packets to the appropriate network.</li> </ul> </li> <li> <p>Segment Reassembly (Transport Layer):</p> <ul> <li>At the transport layer on the recipient's device, the segments \"hello\" and \" world\" are reassembled from the packets.</li> </ul> </li> <li> <p>Message Delivery (Application Layer):</p> <ul> <li>Finally, the reassembled message \"hello world\" is handed over to the WhatsApp application on your friend's device for display.</li> </ul> </li> </ol> <p>In this scenario, segments were used to break down the message into smaller units, making it more manageable for the network. Frames were used to add necessary information for local network communication (like addresses) and ensure data integrity during transmission.</p> <p>Remember, these processes happen at different layers of the OSI model to ensure that your message gets from your device to your friend's device while maintaining data integrity and efficient transmission.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-23-explain-how-does-data-travel-between-two-networks-using-hello-world-message","title":"Question 23: Explain how does data travel between two networks using 'hello world' message?","text":"<p>Here's a detailed explanation of how data travels between a sender in Network A and a receiver in Network B when sending a \"hello world\" message, for example, over WhatsApp.</p> <ol> <li> <p>Message Creation (Application Layer):</p> <p>You compose the \"hello world\" message in your WhatsApp application.</p> </li> <li> <p>Segmentation (Transport Layer):</p> <p>The \"hello world\" message is too large to be sent in one piece over the network. So, it's divided into two segments: \"hello\" and \" world.\"</p> </li> <li> <p>Packetization (Network Layer):</p> <p>Each segment, \"hello\" and \" world,\" is further divided into packets at the network layer. These packets include IP addresses.</p> <p>Your device (in Network A) adds the source IP address, which could be your public or private IP address, to each packet.</p> <p>The destination IP address is set to your friend's public or private IP address.</p> <p>So, at this stage, you have packets like this:</p> <p>Packet 1 (for \"hello\"):</p> <ul> <li>Source IP: Your IP (Network A)</li> <li>Destination IP: Friend's IP (Network B)</li> <li>Data: \"hello\"</li> </ul> <p>Packet 2 (for \" world\"):</p> <ul> <li>Source IP: Your IP (Network A)</li> <li>Destination IP: Friend's IP (Network B)</li> <li>Data: \" world\"</li> </ul> </li> <li> <p>Frame Creation (Data Link Layer - Network A):</p> <p>Before these packets can be sent over your local network (Network A), they need to be enclosed in frames.</p> <p>Each packet, along with its IP header and data, is transformed into frames.</p> <p>The Data Link Layer adds information such as the source MAC address (your device's MAC address) and a destination MAC address.</p> <p>Frame 1 (for Packet 1 - \"hello\"):</p> <ul> <li>Source MAC: Your MAC (in Network A)</li> <li>Destination MAC: Address of your Network A router/switch</li> <li>The packet (\"hello\" data and IP header)</li> </ul> <p>Frame 2 (for Packet 2 - \" world\"):</p> <ul> <li>Source MAC: Your MAC (in Network A)</li> <li>Destination MAC: Address of your Network A router/switch</li> <li>The packet (\" world\" data and IP header)</li> </ul> </li> <li> <p>Transmission (Physical Layer - Network A):</p> <p>These frames are then sent over your local network in Network A, which could be Wi-Fi or Ethernet. Each frame is transmitted from your device to the network infrastructure (like your router or switch).</p> </li> <li> <p>Routing to Network B:</p> <p>Your network infrastructure (router) in Network A examines the destination IP address in the packets. It realizes that the destination IP address belongs to Network B. It routes the packets towards the gateway or router that connects Network A to Network B.</p> </li> <li> <p>Frame Reception (Data Link Layer - Network B):</p> <p>At the border router or gateway of Network B (where your friend's device is located), the frames are received.</p> </li> <li> <p>Frame Forwarding (Data Link Layer - Network B):</p> <p>The destination MAC address is checked to ensure that the frames are meant for your friend's device in Network B. If the destination MAC matches your friend's device, the frames are forwarded to their device.</p> </li> <li> <p>Frame to Packet Conversion (Network Layer - Network B):</p> <p>The frames are converted back into packets at the Data Link Layer in Network B.</p> </li> <li> <p>Segment Reassembly (Transport Layer - Network B):</p> <p>At the Transport Layer, the packets are used to reassemble the original segments. The \"hello\" and \" world\" segments are reconstructed.</p> </li> <li> <p>Message Delivery (Application Layer - Network B):</p> <p>Finally, the reassembled message \"hello world\" is delivered to your friend's WhatsApp application in Network B for display.</p> </li> </ol>"},{"location":"devops-and-sre-interview-questions/networking/#question-24-what-is-a-switch-in-networking","title":"Question 24: what is a switch in networking?","text":"<p>A switch in networking is a critical network device that operates at the data link layer (Layer 2) of the OSI model. Its primary function is to connect devices within a local area network (LAN) and efficiently manage the traffic between them. Here are some key characteristics and functions of a network switch:</p> <ol> <li> <p>Device Connection: Switches are used to interconnect various devices within a network, such as computers, printers, servers, and other networking equipment. They typically have multiple ports to connect these devices.</p> </li> <li> <p>Data Link Layer Operation: Switches operate at the data link layer, where they use MAC (Media Access Control) addresses to make decisions about how to forward data packets. MAC addresses are unique hardware addresses assigned to network interface cards (NICs) in devices.</p> </li> <li> <p>Packet Forwarding: When a device connected to a switch sends data, the switch examines the destination MAC address in the data packet. It then uses its internal MAC address table to determine which port the packet should be forwarded to. This process is more efficient than broadcasting data to all devices on the network, which is what happens in a hub-based network.</p> </li> <li> <p>Traffic Segmentation: Switches create collision domains for connected devices. This means that data transmitted by one device on the network doesn't interfere with the traffic of other devices. This segmentation enhances network performance.</p> </li> <li> <p>Efficiency and Performance: Switches are designed to optimize network performance. They store and forward data packets and can provide dedicated bandwidth to each connected device. This results in faster and more efficient data transmission compared to hubs.</p> </li> <li> <p>MAC Address Learning: Switches learn the MAC addresses of devices connected to their ports by observing the source MAC addresses of incoming packets. They maintain a MAC address table, associating MAC addresses with specific switch ports.</p> </li> <li> <p>Broadcast Control: While switches don't eliminate all broadcasts, they do limit the scope of broadcast traffic. Broadcasts are typically forwarded to all ports in the same VLAN (Virtual Local Area Network), which helps prevent unnecessary broadcast traffic from overwhelming the network.</p> </li> <li> <p>VLAN Support: Managed switches offer VLAN support, allowing network administrators to logically segment a single physical switch into multiple virtual switches, each with its own set of ports and network configuration. This enhances network security and efficiency.</p> </li> <li> <p>Managed vs. Unmanaged: Switches come in both managed and unmanaged variants. Unmanaged switches work out of the box without configuration, while managed switches offer more control and configuration options for network administrators.</p> </li> </ol> <p>In summary, a network switch is a key component in local area networks, enabling efficient and intelligent traffic management, reducing collision domains, and improving network performance by forwarding data based on MAC addresses.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-25-what-is-border-gateway-in-networking","title":"Question 25: What is Border Gateway in networking?","text":"<p>In networking, a \"Border Gateway\" typically refers to a \"Border Gateway Router\" or simply \"Border Router.\" These are specialized routers that play a crucial role in connecting different networks, such as your local network to the internet or two separate networks.</p> <p>Here's what you need to know about border gateways:</p> <ol> <li> <p>Network Boundary: A border gateway serves as the boundary or border between two distinct networks. It's often positioned at the edge of a network and connects it to an external network, such as the global internet or another organization's network.</p> </li> <li> <p>Routing Between Networks: Border gateways are responsible for routing traffic between these networks. They use routing protocols to determine the best path for data to travel from one network to another. Common routing protocols used by border gateways include BGP (Border Gateway Protocol) for internet connections and other routing protocols for internal network connections.</p> </li> <li> <p>Network Security: Border routers often have built-in security features to protect the network they connect. This includes features like firewall capabilities, access control lists, and security policies to filter incoming and outgoing traffic. They play a crucial role in protecting a network from external threats.</p> </li> <li> <p>Network Address Translation (NAT): Border gateways may perform Network Address Translation, which allows multiple devices on a private network to share a single public IP address when communicating with external networks. This is a common practice to conserve public IP addresses.</p> </li> <li> <p>Load Balancing: In some cases, border gateways can be used for load balancing incoming traffic across multiple servers or data centers. This ensures that network resources are efficiently used and that no single server becomes overwhelmed.</p> </li> <li> <p>Redundancy and Failover: High-availability and redundancy are often implemented in border gateway configurations to ensure network uptime. If one border router fails, traffic can be automatically redirected through another border router to maintain network connectivity.</p> </li> <li> <p>Interconnection with Internet Service Providers (ISPs): For internet connections, border routers connect to one or more Internet Service Providers (ISPs). They manage the exchange of data between your network and the internet. BGP is commonly used for this purpose to advertise the network's routes to the internet and to learn the routes to external destinations.</p> </li> <li> <p>Policy Control: Border gateways allow organizations to implement policies and rules regarding data traffic entering and leaving their network. This is important for network management, security, and compliance with regulations.</p> </li> </ol> <p>In summary, a border gateway is a key component in network infrastructure, serving as the connection point between different networks. It's responsible for routing data, enforcing security policies, managing connections to the internet, and playing a critical role in ensuring network availability and performance.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-26-what-is-internet-exchange","title":"Question 26: What is Internet Exchange?","text":"<p>An Internet Exchange, often referred to as an Internet Exchange Point (IXP), is a physical location where different Internet Service Providers (ISPs), content delivery networks (CDNs), and other network operators connect their networks to exchange internet traffic.</p> <p>The primary purpose of an Internet Exchange is to facilitate the efficient and direct exchange of data between these networks, reducing the need for data to travel through multiple intermediaries, which can improve network performance, reduce latency, and lower costs. Here are the key aspects of an Internet Exchange:</p> <ol> <li> <p>Traffic Exchange: Internet Exchanges enable network operators to exchange traffic directly, or \"peer,\" with one another. This peer-to-peer connection allows for more efficient data routing and reduced reliance on third-party transit providers.</p> </li> <li> <p>Reduced Latency: By exchanging traffic directly, data can take shorter and more direct paths between networks. This can reduce latency and improve the speed of internet services.</p> </li> <li> <p>Cost Savings: By avoiding the use of expensive transit providers for data exchange, network operators can reduce their operational costs, which can ultimately lead to cost savings for consumers.</p> </li> <li> <p>Content Delivery: Internet Exchanges often host content delivery networks (CDNs) and large content providers. This ensures that popular content is readily available to users, reducing the load on individual networks and improving content delivery speed.</p> </li> <li> <p>Network Resilience: Internet Exchanges can enhance network resilience and fault tolerance. By connecting to multiple exchange points, network operators can ensure that data can still flow if one connection or exchange point experiences issues.</p> </li> <li> <p>Network Neutrality: Internet Exchanges typically operate on principles of network neutrality, meaning they provide an open and non-discriminatory platform for network operators to exchange traffic. All participating networks are treated equally.</p> </li> <li> <p>Regional and Global Exchanges: Internet Exchanges can be regional, serving a specific geographic area, or global, connecting networks from around the world. Large global exchanges are often located in major data center hubs.</p> </li> <li> <p>Peering Agreements: Network operators must establish peering agreements to connect to an Internet Exchange. These agreements outline the terms and conditions under which traffic will be exchanged, including issues like traffic ratios and acceptable use policies.</p> </li> <li> <p>Route Servers: Internet Exchanges often provide route servers, which simplify the process of establishing peering agreements. Network operators can connect to these route servers, making it easier to exchange traffic with multiple peers.</p> </li> <li> <p>Benefits for End Users: While Internet Exchanges primarily serve network operators, the benefits ultimately extend to end users who experience faster, more reliable, and cost-effective internet services.</p> </li> </ol> <p>In summary, an Internet Exchange is a crucial component of the internet infrastructure, facilitating direct data exchange between network operators and promoting efficient, cost-effective, and high-performance internet connectivity. These exchanges play a key role in improving the overall internet experience for users and supporting the growth of the digital economy.</p> <p>Let's explore life without and with an Internet Exchange using a real-life example involving two companies.</p> <p>Life Without an Internet Exchange (IX):</p> <p>Imagine two companies, Company A and Company B, located in the same city. They want to exchange a lot of digital information daily, like emails, data files, and video calls. However, without an Internet Exchange, their data has to travel a longer route:</p> <ul> <li> <p>No Direct Connection: Company A and Company B connect to the internet through different internet service providers (ISPs). When Company A wants to send data to Company B, it has to travel through its ISP's network, then to a larger ISP (often in a different city), and finally to Company B's ISP. This can be like sending a package through multiple courier services before it reaches its destination.</p> </li> <li> <p>Data Travel: The data has to go through these intermediary networks, and each network adds a bit of delay. It's similar to how a package might sit in sorting facilities during a long journey.</p> </li> <li> <p>Increased Costs: Both companies pay their ISPs for this data transfer, and the more data they send, the more they have to pay. It's like paying for every mile your package travels through different courier services.</p> </li> <li> <p>Latency: Due to this roundabout route, data can take longer to get from Company A to Company B. It's like sending a letter with a long address that goes through multiple postal offices before reaching the recipient.</p> </li> </ul> <p>Life With an Internet Exchange (IX):</p> <p>Now, let's see what happens when an Internet Exchange comes into play:</p> <ul> <li> <p>Direct Connection: With an Internet Exchange, Company A and Company B can directly connect their networks. They plug into the same central place where many other companies also connect. It's like having a meeting point in the city where all companies drop off and pick up their packages.</p> </li> <li> <p>Faster Data Exchange: When Company A wants to send data to Company B, it's a short hop between them. It's similar to handing over a package directly to the recipient without going through multiple couriers. This makes data transfer faster and more efficient.</p> </li> <li> <p>Cost Savings: Since they're not sending data through various networks, they can save on costs. It's like sharing transportation costs when carpooling to work instead of using separate taxis.</p> </li> <li> <p>Lower Latency: Data travels quickly because it has a shorter path to follow. It's like sending a letter across the street instead of across the city, reducing the time it takes to reach the recipient.</p> </li> </ul> <p>In this real-life example, an Internet Exchange serves as a central point where companies can directly connect their networks, leading to faster, cost-effective, and more efficient data exchange. It's like having a local post office where everyone in the neighborhood can easily exchange letters, packages, and information without the need for long, complicated journeys. Internet Exchanges help make the internet faster and more affordable for everyone.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-27-what-is-computer-networking-and-how-has-it-changed-over-time","title":"Question 27: What is computer networking, and how has it changed over time?","text":"<p>Computer networking is the practice of connecting multiple computers and devices to enable them to communicate and share resources.</p> <p>It has evolved significantly since its inception:</p> <ul> <li> <p>Foundation and Early History (1950s-1960s): Computer networking began with the development of mainframe computers in the 1950s. These large machines were used for scientific and military purposes and required methods to communicate and share data. The earliest networks were point-to-point connections for data transfer.</p> </li> <li> <p>The ARPANET (1969): The birth of modern computer networking is often credited to the ARPANET, a research project funded by the U.S. Department of Defense. It introduced packet switching, a method of dividing data into small packets for transmission, which later became the basis for the internet. ARPANET initially connected four universities, and it grew from there.</p> </li> <li> <p>The Internet Emerges (1980s): As ARPANET expanded, it evolved into the internet. The adoption of TCP/IP (Transmission Control Protocol/Internet Protocol) as the standard for data exchange allowed for seamless communication between different networks. The internet rapidly grew beyond the academic and military communities.</p> </li> <li> <p>Commercialization and the World Wide Web (1990s): The 1990s saw the commercialization of the internet, with businesses and individuals gaining access. The World Wide Web, introduced by Tim Berners-Lee in 1991, brought a user-friendly interface for accessing information and services on the internet.</p> </li> <li> <p>Broadband and Modern Internet (2000s-Now): High-speed broadband connections became widely available, enabling multimedia content and online services. The internet expanded to become an integral part of daily life, connecting billions of people and devices globally.</p> </li> </ul> <p>How It Works:</p> <p>Computer networking functions by connecting devices through various hardware and software components:</p> <ul> <li> <p>Network Infrastructure: This includes routers, switches, and access points. Routers direct data between different networks, while switches connect devices within a local network. Access points provide wireless connectivity.</p> </li> <li> <p>Protocols: Data transmission relies on communication protocols like TCP/IP, which govern how data is formatted, transmitted, and received. Protocols ensure data reaches its destination reliably.</p> </li> <li> <p>Addresses: Devices on a network are identified by unique IP addresses. This addressing system enables data packets to be routed to the correct destination.</p> </li> <li> <p>Data Transmission: Data is broken into packets, which are sent over the network. Routers and switches guide the packets along the most efficient path to their destination.</p> </li> <li> <p>Internet Service Providers (ISPs): ISPs provide access to the global internet. They connect users to the broader network, often through wired or wireless connections.</p> </li> <li> <p>Security: Security measures like firewalls and encryption are essential to protect data and networks from unauthorized access and cyber threats.</p> </li> <li> <p>Application Layer: The application layer encompasses the various services and applications that users interact with, including web browsers, email clients, and online games.</p> </li> </ul> <p>Computer networking continues to evolve, with advancements such as 5G, the Internet of Things (IoT), and cloud computing shaping its future. It plays a vital role in connecting people, businesses, and devices, facilitating communication and the exchange of information worldwide.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-28-explain-the-world-wide-web-www-in-detail","title":"Question 28: Explain the World Wide Web (WWW) in detail.","text":"<p>The World Wide Web (WWW), often referred to as the \"web,\" is a fundamental component of the internet that revolutionized the way people access and share information. It was created by Tim Berners-Lee in 1991 while working at CERN, the European Organization for Nuclear Research. </p> <p>Here's a detailed explanation of what the World Wide Web is and what it did:</p> <ol> <li> <p>Fundamental Concept: The World Wide Web is a system of interconnected documents and resources, linked through hyperlinks. These documents are typically in the form of web pages containing text, images, videos, and other media.</p> </li> <li> <p>Key Components:</p> <ul> <li> <p>HTML (Hypertext Markup Language): HTML is the standard language used to create web pages. It defines the structure and content of a page, including headings, paragraphs, links, and multimedia elements.</p> </li> <li> <p>URL (Uniform Resource Locator): A URL is the web address used to identify and locate specific web resources. It consists of a protocol (e.g., \"http\" or \"https\"), a domain name, and a path.</p> </li> <li> <p>HTTP (Hypertext Transfer Protocol): HTTP is the protocol used for transferring web pages and data over the internet. It allows web browsers to request web pages from web servers.</p> </li> </ul> </li> <li> <p>Hyperlinks: The web introduced hyperlinks, or simply \"links.\" These are elements within web pages that allow users to navigate to other web pages, both within the same website and across different websites. Links make it easy to connect related information.</p> </li> <li> <p>Universal Access: The World Wide Web was designed to be an open and accessible platform for sharing and accessing information. Anyone with an internet connection and a web browser can access web content.</p> </li> <li> <p>Multi-Media Content: The web is not limited to text; it also supports multimedia content. This means web pages can include images, audio, videos, and interactive elements, making it a rich and diverse medium for communication.</p> </li> <li> <p>Search Engines: The rise of search engines like Google revolutionized the web by providing efficient and user-friendly ways to find specific information on the vast and ever-expanding web. Users can enter keywords and receive relevant web page results.</p> </li> <li> <p>E-commerce and Online Services: The World Wide Web has transformed commerce. It enables online shopping, banking, booking services, and a wide range of online applications. Companies and individuals can conduct business and offer services globally.</p> </li> <li> <p>Social Media and Collaboration: The web facilitated social networking platforms, such as Facebook and Twitter, which allow people to connect, communicate, and share content. It also enabled collaborative tools and platforms for remote work and communication.</p> </li> <li> <p>Blogging and Content Creation: The web allowed anyone to become a content creator through blogs, vlogs, and other platforms. This democratization of content creation gave rise to diverse voices and perspectives.</p> </li> <li> <p>Education and Information Sharing: Educational institutions and organizations use the web to provide access to knowledge and resources. Online courses, e-learning platforms, and open educational resources are common on the web.</p> </li> <li> <p>Global Impact: The World Wide Web has had a profound global impact, reshaping how information is disseminated, how business is conducted, and how people connect. It has contributed to increased globalization, democratization of information, and economic growth.</p> </li> </ol> <p>In summary, the World Wide Web fundamentally transformed the internet into a dynamic, multimedia-rich platform for information sharing, communication, and collaboration. Its impact on society, commerce, and education is immeasurable, and it continues to evolve as new technologies and applications emerge.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-29-how-was-internet-used-before-www","title":"Question 29: How was internet used before www?","text":"<p>Before the World Wide Web (WWW) became a dominant force on the internet, the internet was primarily used for various purposes, often involving text-based communications and resource sharing.</p> <p>Here's a detailed explanation of how the internet was used before the WWW:</p> <ol> <li> <p>Email: Email, or electronic mail, was one of the earliest and most widely used applications on the pre-WWW internet. It allowed users to exchange text-based messages and files. Email was widely adopted in academic, government, and corporate settings, serving as a precursor to modern email systems.</p> </li> <li> <p>Usenet Newsgroups: Usenet was a distributed discussion system that enabled users to participate in text-based group discussions. It was organized into newsgroups covering a wide range of topics, from technology and science to hobbies and entertainment. Users could post and respond to messages, fostering a sense of community.</p> </li> <li> <p>FTP (File Transfer Protocol): FTP was used for transferring files between computers on the internet. It allowed users to upload and download files, such as software, documents, and data, from FTP servers. This was crucial for sharing data and software in the early internet days.</p> </li> <li> <p>Telnet: Telnet was a protocol that allowed remote access to other computers over the internet. Users could log into remote systems and operate them as if they were physically present. This was especially valuable for remote administration and accessing resources on different platforms.</p> </li> <li> <p>Gopher: Gopher was a text-based protocol that predated the WWW and provided a way to access and retrieve documents and files organized in a hierarchical menu-like structure. It was used for sharing information and resources across the internet.</p> </li> <li> <p>Archie and Veronica: Archie was a tool for searching and indexing FTP archives, making it easier to find files on the internet. Veronica complemented Archie by searching for items on Gopher servers. These tools were precursors to modern search engines.</p> </li> <li> <p>Bulletin Board Systems (BBS): Before the internet became widely accessible, BBSs were popular. Users would connect to BBSs via modems to read and post messages, share files, and play online games. These were localized online communities.</p> </li> <li> <p>Academic and Research Collaboration: The early internet was used for academic and research purposes, allowing scientists, researchers, and students to share data and collaborate on projects. It played a pivotal role in the development of the internet's infrastructure.</p> </li> <li> <p>Military and Government Communication: The internet, originally conceived as ARPANET, was developed for military and research purposes. It continued to be a vital communication tool for military and government agencies before its civilian expansion.</p> </li> <li> <p>Limited Commercial Use: While not as prevalent as today, some commercial entities used the internet for purposes like exchanging business data or providing limited services to customers. However, widespread e-commerce as seen today was not present.</p> </li> </ol> <p>In summary, the pre-WWW internet was a predominantly text-based and resource-sharing network. It was used for communication, research, file sharing, and collaboration among academics, researchers, and technology enthusiasts. The introduction of the World Wide Web in the early 1990s brought a graphical and user-friendly interface that revolutionized how people accessed and shared information on the internet, expanding its reach to a broader audience and enabling multimedia content.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-30-what-is-the-difference-between-telnet-and-ssh","title":"Question 30: What is the difference between telnet and SSH?","text":"<p>Telnet and SSH are both network protocols used for remote access to computer systems, but they differ significantly in terms of security and functionality.</p> <p>Here's a comparison of the two:</p> <p>Telnet:</p> <ol> <li> <p>History: Telnet, which stands for \"Telecommunication Network,\" is one of the oldest network protocols, dating back to the early days of computer networking in the late 1960s. It was originally developed for remote access and control of mainframe computers.</p> </li> <li> <p>Functionality: Telnet provides a text-based, unencrypted way to establish a remote terminal session with another computer. It allows users to log in to a remote system, run commands, and access files as if they were physically present at the remote computer. It essentially transmits keystrokes and receives text-based responses.</p> </li> <li> <p>Security: Telnet does not provide any encryption or authentication mechanisms. Data transmitted via Telnet is sent in plain text, making it highly vulnerable to eavesdropping and interception. As a result, Telnet is considered insecure and is not recommended for use over untrusted networks, such as the public internet.</p> </li> </ol> <p>SSH:</p> <ol> <li> <p>History: Secure Shell (SSH) was developed as a response to the inherent security vulnerabilities of Telnet. It was first introduced in the early 1990s, with the SSH-1 protocol being the initial version. SSH-2, a more secure and widely adopted version, was introduced later.</p> </li> <li> <p>Functionality: SSH is a secure, encrypted network protocol that serves the same basic purpose as Telnet but with a focus on data security. It allows users to establish a secure and authenticated remote connection to a computer or server, enabling remote access, file transfers, and command execution.</p> </li> <li> <p>Security: SSH encrypts all data transmitted between the client and server, including login credentials and the commands sent during a session. It also provides robust authentication mechanisms, such as password-based, key-based, and certificate-based authentication. These security features make SSH highly resistant to eavesdropping and man-in-the-middle attacks, making it a preferred choice for secure remote access over untrusted networks.</p> </li> </ol> <p>In summary, Telnet and SSH are both protocols used for remote access, but SSH is the more secure option due to its encryption and authentication mechanisms. Telnet, while historically significant, is considered insecure for use over public networks. SSH has largely replaced Telnet in modern networking and system administration due to its enhanced security features, and it continues to be the standard for secure remote access to servers and network devices.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-31-what-is-the-difference-between-lan-and-wan","title":"Question 31: What is the difference between LAN and WAN?","text":"<p>LAN (Local Area Network) and WAN (Wide Area Network) are two distinct types of computer networks that differ in terms of their size, geographical coverage, and purpose.</p> <p>Here are the key differences between LAN and WAN:</p> <ol> <li> <p>Size and Geographical Coverage:</p> <ul> <li> <p>LAN (Local Area Network): A LAN is a network that covers a small geographic area, typically within a single building, campus, or a group of nearby buildings. It is designed for use in a limited area, such as a home, office, or school.</p> </li> <li> <p>WAN (Wide Area Network): A WAN, on the other hand, spans a large geographic area, which can encompass cities, regions, countries, or even continents. WANs connect LANs across greater distances, often utilizing public or private communication links.</p> </li> </ul> </li> <li> <p>Ownership and Control:</p> <ul> <li> <p>LAN: LANs are usually privately owned and controlled by a single organization or entity. They have a single administrator or IT team responsible for their maintenance and management.</p> </li> <li> <p>WAN: WANs may involve multiple organizations and service providers. They can be a combination of privately owned networks and public networks. Management and control are distributed among various entities.</p> </li> </ul> </li> <li> <p>Data Transfer Speed:</p> <ul> <li> <p>LAN: LANs typically offer high data transfer speeds, often reaching gigabit speeds or higher. This is because LANs operate over shorter distances with minimal latency.</p> </li> <li> <p>WAN: WANs, due to the longer distances and multiple interconnected networks, may have lower data transfer speeds in comparison to LANs. The speed of data transmission in WANs can vary depending on the technology and infrastructure used.</p> </li> </ul> </li> <li> <p>Technologies Used:</p> <ul> <li> <p>LAN: LANs commonly use technologies like Ethernet and Wi-Fi for data transmission within a limited area. LAN devices are often interconnected through switches and access points.</p> </li> <li> <p>WAN: WANs rely on various technologies, including leased lines, optical fiber, satellite links, and the internet. WANs may involve routers and specialized WAN equipment to connect geographically dispersed LANs.</p> </li> </ul> </li> <li> <p>Latency and Reliability:</p> <ul> <li> <p>LAN: LANs typically offer low latency and high reliability since they operate over short distances with minimal external interference.</p> </li> <li> <p>WAN: WANs may experience higher latency due to longer data transmission distances and the involvement of multiple networks and devices. Reliability can vary based on the quality and redundancy of the network infrastructure.</p> </li> </ul> </li> <li> <p>Examples:</p> <ul> <li> <p>LAN: Examples of LANs include a home network, a corporate office network, a school's network, or a small campus network.</p> </li> <li> <p>WAN: Examples of WANs include the internet, a network connecting multiple branch offices of a multinational corporation, or a national telecommunication network.</p> </li> </ul> </li> </ol> <p>In summary, LANs are local, smaller-scale networks designed for efficient communication within a limited area, while WANs are expansive networks that connect LANs across larger geographic regions. LANs offer high-speed, low-latency communication within a confined area, whereas WANs focus on long-distance connectivity, making them suitable for connecting multiple remote LANs or organizations.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-32-when-connected-to-wi-fi-how-does-data-traffic-move-and-what-are-the-roles-of-lan-wan-and-isps-in-this-process","title":"Question 32: When connected to Wi-Fi, how does data traffic move, and what are the roles of LAN, WAN, and ISPs in this process?","text":"<p>When you're connected to a Wi-Fi network, traffic flows through a series of interconnected networks, including the Local Area Network (LAN), Wide Area Network (WAN), and the Internet Service Provider (ISP).</p> <p>Here's an explanation of how traffic flows through these networks:</p> <ol> <li> <p>Local Area Network (LAN):</p> <ul> <li> <p>Your device connects to a Wi-Fi access point within your premises (e.g., your home or office). This access point is part of your local network, known as the Local Area Network (LAN).</p> </li> <li> <p>When you access resources within your LAN, such as shared files or printers, the traffic stays within your LAN. It does not go out to the broader internet.</p> </li> <li> <p>If you want to communicate with another device connected to the same access point or the same LAN, the traffic remains local, moving directly between devices within the LAN.</p> </li> </ul> </li> <li> <p>Access Point:</p> <ul> <li>The access point is responsible for facilitating wireless connections and bridging the gap between the wireless and wired networks. It takes data from connected devices and sends it to the LAN, and vice versa.</li> </ul> </li> <li> <p>Wide Area Network (WAN):</p> <ul> <li> <p>To access resources beyond your LAN, such as a website or a server located outside your premises, your data needs to travel from your LAN to a Wide Area Network (WAN). This usually involves your internet gateway, such as a router or modem.</p> </li> <li> <p>Your router is responsible for routing traffic between your LAN and the WAN. When you request a website or any online resource, your device sends the request to the router, which then forwards the request to the WAN.</p> </li> <li> <p>If you're accessing resources within the same WAN (e.g., a remote office connected via a VPN or a branch office), the traffic will traverse the WAN.</p> </li> </ul> </li> <li> <p>Internet Service Provider (ISP):</p> <ul> <li> <p>Your router connects to the Wide Area Network through your Internet Service Provider (ISP). The ISP is responsible for routing traffic between your location and the internet.</p> </li> <li> <p>Your data is sent from your router to your ISP, and from there, it traverses the ISP's network. The ISP is responsible for routing the data to its destination on the internet or bringing data to your network if you're downloading content.</p> </li> </ul> </li> <li> <p>Internet:</p> <ul> <li> <p>Once your data reaches the broader internet, it may go through multiple networks and routers owned by various organizations, including content providers, hosting companies, and other ISPs.</p> </li> <li> <p>For example, if you're accessing a website, your data may pass through servers, switches, and routers before reaching the web server hosting the site. The web server processes your request and sends the requested data (e.g., a web page) back through the same network path.</p> </li> </ul> </li> <li> <p>Return Traffic:</p> <ul> <li>When the requested data is sent back to your device (e.g., the web page you requested), it follows a reverse path, going through the same networks in the reverse order.</li> </ul> </li> </ol> <p>In summary, when you're connected to a Wi-Fi network, your traffic flows from your device to the LAN, then to the WAN, through your ISP, and finally onto the broader internet or to other remote networks. The same path is used for return traffic. Each of these network segments plays a crucial role in ensuring that your data reaches its destination and that you can access resources both within your LAN and on the internet.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-33-what-are-common-network-hardware-components-and-at-which-osi-model-layers-do-they-operate","title":"Question 33: What are common network hardware components and at which OSI model layers do they operate?","text":"<p>Computer networks consist of various hardware components that operate at different layers of the OSI (Open Systems Interconnection) model, which is a conceptual framework used to understand and standardize network communication.</p> <p>Here is a list of common network hardware components, along with the layers of the OSI model at which they primarily operate:</p> <ol> <li> <p>Network Interface Card (NIC):</p> <ul> <li> <p>Layer: Physical (Layer 1)</p> </li> <li> <p>Explanation: A NIC is a hardware component in computers and devices that connects to the physical network medium, such as Ethernet cables. It operates at the physical layer, handling the electrical and mechanical aspects of network connectivity.</p> </li> </ul> </li> <li> <p>Hub:</p> <ul> <li> <p>Layer: Physical (Layer 1)</p> </li> <li> <p>Explanation: Hubs are basic networking devices that operate at the physical layer. They simply receive data on one port and broadcast it to all other ports, making them less efficient and more prone to collisions in network traffic.</p> </li> </ul> </li> <li> <p>Switch:</p> <ul> <li> <p>Layer: Data Link (Layer 2)</p> </li> <li> <p>Explanation: Switches operate at the data link layer and use MAC addresses to forward data frames within a LAN. They are more intelligent than hubs, as they can selectively forward data to the appropriate destination device.</p> </li> </ul> </li> <li> <p>Router:</p> <ul> <li> <p>Layer: Network (Layer 3)</p> </li> <li> <p>Explanation: Routers operate at the network layer and are responsible for forwarding data between different networks. They use IP addresses to determine the best path for data to reach its destination.</p> </li> </ul> </li> <li> <p>Firewall:</p> <ul> <li> <p>Layer: Network (Layer 3) and Transport (Layer 4)</p> </li> <li> <p>Explanation: Firewalls can operate at both the network and transport layers. They filter network traffic based on predefined rules, allowing or blocking data packets as they traverse the network.</p> </li> </ul> </li> <li> <p>Layer 3 Switch:</p> <ul> <li> <p>Layer: Network (Layer 3)</p> </li> <li> <p>Explanation: Layer 3 switches combine the functions of traditional switches and routers. They can make routing decisions based on IP addresses, improving network performance in certain scenarios.</p> </li> </ul> </li> <li> <p>Access Point (AP):</p> <ul> <li> <p>Layer: Data Link (Layer 2) and Physical (Layer 1)</p> </li> <li> <p>Explanation: Access points bridge the gap between wired and wireless networks. They operate at both the data link layer (for MAC address handling) and the physical layer (for wireless transmission).</p> </li> </ul> </li> <li> <p>Modem:</p> <ul> <li> <p>Layer: Physical (Layer 1)</p> </li> <li> <p>Explanation: Modems (modulator-demodulator) are used to convert digital data from a computer into analog signals for transmission over analog networks (e.g., telephone lines) or vice versa. They primarily operate at the physical layer.</p> </li> </ul> </li> <li> <p>Bridge:</p> <ul> <li> <p>Layer: Data Link (Layer 2)</p> </li> <li> <p>Explanation: Bridges are used to connect two or more network segments and filter traffic based on MAC addresses. They operate at the data link layer, similar to switches.</p> </li> </ul> </li> <li> <p>Gateway:</p> <ul> <li> <p>Layer: Varies (can operate at different layers)</p> </li> <li> <p>Explanation: Gateways are devices or software that enable communication between networks that use different protocols or technologies. They can operate at various layers depending on the specific function they perform.</p> </li> </ul> </li> <li> <p>Load Balancer:</p> <ul> <li> <p>Layer: Application (Layer 7)</p> </li> <li> <p>Explanation: Load balancers distribute network traffic across multiple servers or resources to optimize performance. They often operate at the application layer and can balance traffic based on various criteria.</p> </li> </ul> </li> </ol> <p>It's important to note that while these hardware components primarily operate at specific OSI model layers, they often work in conjunction with higher-layer protocols and applications to enable end-to-end network communication. Additionally, some devices, like layer 3 switches and gateways, can span multiple layers depending on their configuration and functionality.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-34-how-does-router-determine-the-best-path-for-data-packets","title":"Question 34: How does router determine the best path for data packets?","text":"<p>Routers determine the best path for data packets based on the destination IP address in the packet's header. This process involves a mechanism called routing, and it primarily occurs at the Network Layer (Layer 3) of the OSI model.</p> <p>Here's how routers determine the best path for data packets:</p> <ol> <li> <p>Routing Table: Routers maintain a routing table that contains information about the available networks and the associated paths or next hops. This table is crucial for determining how to forward data packets.</p> </li> <li> <p>Destination IP Address: When a router receives a data packet, it examines the destination IP address in the packet's header. The router needs to decide which interface or next-hop router to send the packet to based on this destination address.</p> </li> <li> <p>Longest Prefix Match: The router performs a longest prefix match on the destination IP address. It compares the destination IP address to the entries in its routing table and looks for the most specific match. The entry with the longest matching prefix is chosen.</p> </li> <li> <p>Default Route: If no specific match is found in the routing table, the router may have a default route (0.0.0.0/0) that serves as a catch-all route. Data packets that do not match any specific routes are sent through the default route.</p> </li> <li> <p>Metric and Administrative Distance: In cases where multiple routes have the same prefix length, routers consider other factors to choose the best path. These factors include metrics (such as hop count, bandwidth, delay, or cost) and administrative distances, which are values that rank the trustworthiness of routing information sources (e.g., directly connected networks have a lower administrative distance).</p> </li> <li> <p>Routing Protocol: Routers may use various routing protocols to exchange routing information and build their routing tables. Common routing protocols include RIP (Routing Information Protocol), OSPF (Open Shortest Path First), EIGRP (Enhanced Interior Gateway Routing Protocol), and BGP (Border Gateway Protocol). Each protocol uses its own algorithm to determine the best path.</p> </li> <li> <p>Update and Recalculation: Routing tables are not static and are periodically updated based on changes in the network. Routers exchange routing updates with neighboring routers to reflect changes in network topology, like the addition or removal of network links or devices. When the routing table is updated, the router may recalculate the best path for data packets.</p> </li> <li> <p>Forwarding: Once the router determines the best path, it forwards the data packet to the appropriate interface or next-hop router, where it continues its journey toward its destination.</p> </li> </ol> <p>The process of routing allows routers to make intelligent decisions about how to route data packets efficiently and reliably within a network. Routing is fundamental to the operation of the internet and other large-scale networks, ensuring that data reaches its intended destination while traversing multiple interconnected routers and networks.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-35-what-is-a-wan-port","title":"Question 35: What is a WAN port?","text":"<p>A WAN port (Wide Area Network port) on a network device, typically a router or gateway, is a special port used to connect to a Wide Area Network. It is designed to connect the local network to an external network, usually the internet or a service provider's network. The WAN port serves as the gateway between the local network (LAN) and the broader network.</p> <p>What is connected to a WAN port:</p> <ol> <li> <p>Internet Service Provider (ISP): The primary device connected to the WAN port is the ISP's equipment, which provides the connection to the internet or the external network. This connection may involve various technologies, such as DSL, cable, fiber optics, or other dedicated lines.</p> </li> <li> <p>Router or Gateway: In a typical home or small office network setup, a router or gateway connects to the ISP's equipment via the WAN port. This device is responsible for managing the connection, performing Network Address Translation (NAT), and providing security features like firewall capabilities.</p> </li> <li> <p>Modem (in some cases): In situations where the ISP's connection is purely a physical medium like DSL or cable, a separate modem may be connected to the WAN port. The modem translates the ISP's signal into a form that the router can understand, allowing it to establish the internet connection.</p> </li> <li> <p>Network Devices: Beyond the router or gateway, all the devices on the local network are connected to the LAN ports of the router. These devices include computers, smartphones, printers, and other networked devices. The router manages the communication between the LAN devices and the external network through the WAN port.</p> </li> </ol> <p>The WAN port plays a crucial role in establishing the connection between your local network and the internet or another external network. It's the point where your network interacts with the broader world, enabling you to access online resources and communicate with other networks and devices beyond your local environment.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-36-how-does-nat-on-a-router-handle-multiple-devices-in-a-home-network-sharing-a-single-public-ip","title":"Question 36: How does NAT on a router handle multiple devices in a home network sharing a single public IP?","text":"<p>Let's walk through an example of how a router uses Network Address Translation (NAT) to manage multiple devices in a home network, all sharing a single public IP address.</p> <p>In this example, we have a home network with three devices: a desktop computer, a laptop, and a smartphone. The home network's public IP address is 203.0.113.1.</p> <ol> <li> <p>Device IP Addresses:</p> <ul> <li>Desktop Computer: 192.168.0.2</li> <li>Laptop: 192.168.0.3</li> <li>Smartphone: 192.168.0.4</li> </ul> </li> <li> <p>Device Connections:</p> <ul> <li>The desktop computer wants to access a website with the IP address 198.51.100.10 on the internet. It initiates an HTTP request.</li> </ul> </li> <li> <p>NAT Translation:</p> <ul> <li> <p>The router assigns a unique port number to this outbound connection. Let's say it assigns port 5001 for the desktop computer's request.</p> </li> <li> <p>The router creates a NAT translation entry in its mapping table, recording the source IP (192.168.0.2), the source port (5001), the destination IP (198.51.100.10), and the destination port (80 for HTTP).</p> </li> </ul> </li> <li> <p>Outbound Packet:</p> <ul> <li> <p>The desktop computer's request is sent out to the internet from the router's public IP address (203.0.113.1) with the source port 5001.</p> </li> <li> <p>It looks like this:</p> <ul> <li>Source IP: 192.168.0.2, Source Port: 5001</li> <li>Destination IP: 198.51.100.10, Destination Port: 80</li> </ul> </li> </ul> </li> <li> <p>Incoming Response:</p> <ul> <li>The website (198.51.100.10) processes the request and sends back the response.</li> <li>The response arrives at the router, which examines the destination port, which is 5001 in this case.</li> </ul> </li> <li> <p>NAT Table Lookup:</p> <ul> <li>The router checks its NAT mapping table for an entry that matches destination port 5001.</li> </ul> </li> <li> <p>Translation to Local Device:</p> <ul> <li>The router finds the corresponding NAT entry for the desktop computer with the source IP 192.168.0.2 and port 5001.</li> <li>It knows that this data packet is for the desktop computer. The router forwards the response to the desktop computer's internal IP address (192.168.0.2) with port 5001.</li> </ul> </li> <li> <p>Delivery to Desktop:</p> <ul> <li>The response arrives at the desktop computer, which recognizes it as the reply to its earlier request.</li> </ul> </li> </ol> <p>This process repeats for each device in the home network as they initiate connections. The router keeps track of the port numbers and devices using its NAT mapping table, ensuring that data packets are correctly delivered to the intended recipients within the network.</p> <p>NAT allows multiple devices to share the single public IP address (203.0.113.1), making efficient use of available IP addresses and enabling communication with external devices on the internet.</p>"},{"location":"devops-and-sre-interview-questions/networking/#question-37-what-is-tcpip-model-and-how-is-it-different-from-the-osi-model","title":"Question 37: what is TCP/IP model and how is it different from the OSI model?","text":"<p>The TCP/IP model and the OSI (Open Systems Interconnection) model are both conceptual frameworks that describe how network protocols work. These models help in understanding and standardizing the functions and communication processes within computer networks. </p> <p>While there are similarities between the two models, there are also key differences:</p> <p>TCP/IP Model:</p> <ul> <li> <p>Layers: The TCP/IP model has four layers:</p> <ol> <li> <p><code>Application Layer</code>: This is similar to the top three layers (Application, Presentation, and Session) in the OSI model. It handles end-user applications and high-level protocols like HTTP, FTP, and SMTP.</p> </li> <li> <p><code>Transport Layer</code>: This layer corresponds to the Transport Layer in the OSI model. It deals with end-to-end communication, providing services like reliable data transfer (TCP) and connectionless data transfer (UDP).</p> </li> <li> <p><code>Network Layer</code>: Similar to the Network Layer in the OSI model, this layer is responsible for routing and forwarding data packets between different networks. It employs the Internet Protocol (IP) for addressing and routing.</p> </li> <li> <p><code>Network Access Layer</code>: This layer is similar to the combination of the Data Link Layer and Physical Layer in the OSI model. It deals with the physical network hardware, including addressing (e.g., MAC addresses) and the actual transmission of data over the physical medium.</p> </li> </ol> </li> <li> <p>History: The TCP/IP model was developed by the U.S. Department of Defense and was the foundation for the early ARPANET (precursor to the modern Internet). It has been widely adopted and is the basis for the modern internet.</p> </li> <li> <p>Usage: The TCP/IP model is the basis for the internet and most modern networking. It is used as the reference model for the internet protocol suite, which includes IP, TCP, and UDP.</p> </li> </ul> <p>OSI Model:</p> <ul> <li> <p>Layers: The OSI model has seven layers:</p> <ol> <li> <p><code>Application Layer</code>: The Application Layer is responsible for end-user communication, providing services such as email, file transfer, and web browsing.</p> </li> <li> <p><code>Presentation Layer</code>: The Presentation Layer deals with data translation, encryption, and formatting to ensure data compatibility between different systems.</p> </li> <li> <p><code>Session Layer</code>: The Session Layer manages sessions and connections between applications, allowing them to establish, maintain, and terminate communication.</p> </li> <li> <p><code>Transport Layer</code>: The Transport Layer is responsible for end-to-end communication and data transport, ensuring reliable and orderly data delivery using protocols like TCP and UDP.</p> </li> <li> <p><code>Network Layer</code>: The Network Layer is concerned with routing data between different networks and assigns logical addresses, as seen in the Internet Protocol (IP).</p> </li> <li> <p><code>Data Link Layer</code>: The Data Link Layer handles the transfer of data between adjacent network nodes, addressing, error detection, and flow control.</p> </li> <li> <p><code>Physical Layer</code>: The Physical Layer deals with the actual transmission of data over the physical medium, specifying hardware connections, voltage levels, and signal encoding.</p> </li> </ol> </li> <li> <p>History: The OSI model was developed by the International Organization for Standardization (ISO) in the 1980s. It was an attempt to create a universal networking model that could be used globally.</p> </li> <li> <p>Usage: The OSI model is often used for teaching and conceptual understanding of networking, but it is not as commonly used as the TCP/IP model in practice. Some network technologies and protocols do follow the OSI model, but it's less pervasive than TCP/IP.</p> </li> </ul> <p>Key Differences:</p> <ol> <li> <p>Number of Layers: The OSI model has seven layers, while the TCP/IP model has four layers. The extra layers in the OSI model provide more granularity for understanding network processes.</p> </li> <li> <p>Real-World Implementation: The TCP/IP model is the basis for the actual implementation of the internet and most modern networks, making it more prevalent in practice.</p> </li> <li> <p>Historical Context: The OSI model was developed as a theoretical framework, while the TCP/IP model has a real-world history and is the foundation of the internet.</p> </li> </ol> <p>In summary, both models are used to understand networking concepts, but the TCP/IP model is more commonly used in practical networking, particularly in the context of the internet. The OSI model, with its additional layers, is often used for educational purposes and for a more detailed examination of network processes.</p>"},{"location":"devops-and-sre-interview-questions/thanos/","title":"All About Thanos","text":"<p>Thanos is an open-source extension for Prometheus, providing long-term storage and global querying capabilities for Prometheus metrics.</p> <p> </p>"},{"location":"devops-and-sre-interview-questions/thanos/#purpose-of-thanos","title":"Purpose of Thanos","text":"<ol> <li>Global View</li> <li>Long Term Storage</li> <li>High Availability</li> </ol>"},{"location":"devops-and-sre-interview-questions/thanos/#components-of-thanos","title":"Components of Thanos","text":"<p>Thanos is comprised of a set of components where each fulfills a specific role.</p> <ol> <li> <p>Sidecar: connects to Prometheus, reads its data for query and/or uploads it to cloud storage.</p> </li> <li> <p>Store Gateway: serves metrics inside of a cloud storage bucket.</p> </li> <li> <p>Compactor: compacts, downsamples and applies retention on the data stored in the cloud storage bucket.</p> </li> <li> <p>Receiver: receives data from Prometheus\u2019s remote write write-ahead log, exposes it, and/or uploads it to cloud storage.</p> </li> <li> <p>Ruler/Rule: evaluates recording and alerting rules against data in Thanos for exposition and/or upload.</p> </li> <li> <p>Querier/Query: implements Prometheus\u2019s <code>v1</code> API to aggregate data from the underlying components.</p> </li> <li> <p>Query Frontend: implements Prometheus\u2019s <code>v1</code> API to proxy it to Querier while caching the response and optionally splitting it by queries per day.</p> </li> </ol> <p>Let's see how we can transform existing prometheus setup to solve global view, high availability and long-term storage?</p>"},{"location":"devops-and-sre-interview-questions/thanos/#step-1-add-a-thanos-sidecar","title":"Step 1: Add a Thanos Sidecar","text":"<p>To have thanos enabled for prometheus we need to add a sidecar to our prometheus deployment.</p> <p>Thanos sidecar is a small go binary that has multiple features. The main features are:</p> <ol> <li>It translates the prometheus' internal <code>Remote Read API</code> to thanos gRPC API called <code>Store API</code>.</li> <li>It also reloads configuration of prometheus, rules or alerts automatically from for example k8s configmap.</li> </ol>"},{"location":"devops-and-sre-interview-questions/thanos/#step-2-add-a-stateless-thanos-querier","title":"Step 2: Add a Stateless Thanos Querier","text":"<p>Thanos Querier is a thanos component that essentially performs a PromQL (prometheus native query language) evaluation on global level instead of just one prometheus.</p> <p>It connects to the Store API via gRPC. </p> <p>The querier exposes the same HTTP API as prometheus does, so all the Grafana dashboards and tools works because they think it is just normal prometheus.</p> <p>It allows us to answer global queries, such as the number of series per cluster, by aggregating data from multiple Prometheus instances. This is possible because it has access to data from all Prometheus instances, enabling it to provide a global view of the metrics.</p> <p>The Thanos Querier addresses the issue of high availability (HA) on multiple replicas of Prometheus by pulling data from both replicas, deduplicating the signals, and filling in any gaps transparently to the Querier consumer. This built-in deduplication layer enables the Querier to provide a consistent view of the data from multiple Prometheus instances, ensuring accurate and reliable query results, even in HA setups.</p>"},{"location":"devops-and-sre-interview-questions/thanos/#step-3-configure-long-term-storage-for-metrics","title":"Step 3: Configure Long Term Storage for Metrics","text":"<p>For long-term storage, Thanos utilizes external storage.</p> <p>Configure an object storage like S3 and provide the credentials to the Thanos sidecar.</p> <p>When Prometheus generates a block in its time series database format, the sidecar takes this data block and uploads it to the specified object storage.</p> <p>It's crucial to clarify that this isn't a streaming API; instead, it performs lazy upload of all the data, typically after 2 hours when Prometheus produces a complete data block.</p> <p>To access the data stored in the object storage, there's another component known as the <code>store gateway</code>. The store gateway implements the same gRPC API as the sidecar and querier. Notably, it reads data from the object storage instead of directly from the Prometheus API.</p> <p>When querying recent data, the Thanos querier typically retrieves data directly from Prometheus running in the cluster via gRPC API exposed by sidecar. However, for long-term data, such as information from a year ago, it fetches the data from the object storage via gRPC API exposed by store gateway.</p> <p>It's important to bear in mind that querying data from a year ago may involve a substantial number of time series, which can be slow and sometimes impractical in a regular Prometheus setup. To address this, Thanos introduces another component called the <code>Compactor</code>, which serves two primary functions:</p> <ol> <li>It performs downsampling.</li> <li>It consolidates multiple Prometheus tsdb blocks into fewer tsdb blocks, thereby enhancing query speed.</li> </ol> <p>References:</p> <ul> <li>Thanos: Prometheus at Scale</li> </ul>"},{"location":"kubernetes-on-eks/","title":"Amazon EKS Masterclass: Kubernetes and Microservices on EKS","text":"<p>Master Kubernetes, Microservices, Logging, Monitoring, Service Mesh, DevOps, CI/CD, GitOps, and Go From Zero to Hero! </p> <p>This course on kubernetes and microservices is all about getting your hands dirty with practical learning \u2013 no nonsense, no jargon, just pure, actionable tutorials and demos with real code and manifest files.</p> <p>This course takes you on a journey from the fundamentals, perfect for beginners, and smoothly transitions to advanced topics. You'll not only get a solid start but also gain insights into how real-world production systems apply these concepts to create robust, fault-tolerant solutions.</p> <p>Dive in and get your hands dirty with real-world skills!</p>"},{"location":"kubernetes-on-eks/#what-will-you-learn-in-this-course","title":"What Will You Learn in This Course?","text":"You will be able to confidently write Kubernetes manifests for any kind of application and set up ci/cd pipeline to deploy it on EKS along with Logging, Monitoring and Service Mesh.  You will learn how to set up and configure popular tools like Prometheus, Grafana, Istio, Kiali, Jaegar, Argo CD etc. You will also set up CI/CD using DevOps. 1. You will learn Docker fundamentals. In this section you'll learn how to write Dockerfile for an application, create Docker image and run containers. 2. You will acquire an understanding of the Kubernetes Components and Cluster Architecture, which will serve as a solid foundation for this course. 3. You will learn YAML fundamentals and its importance in Kubernetes. 4. You will create Amazon EKS cluster using both the eksctl command line and config files. 5. You will gain clarity on the difference between imperative and declarative approaches. 6. You will learn Kubernetes fundamentals (Namespace, Pod, ReplicaSet, Deployment, Service, etc.) 7. You will learn advanced concepts in Kubernetes (Probes, Init-Containers, ConfigMap, Secrets, Requests and Limits, Service Quota, etc.) 8. You will learn about Storage in Kubernetes, Persistent Volume, PersistentVolume Claim and EBS CSI Driver to manage Amazon EBS in EKS. 9. You will add private nodegroup in EKS cluster and delete the public nodegroup. 10. You will learn about the LoadBalancer service type in detail; Both, the classic load balancer (CLB) and network load balancer (NLB). 11. You will learn about Kubernetes Service Account and IAM Role for Service Account. 12. You will deploy AWS Load Balancer Controller formerly known as AWS ALB Ingress Controller. 13. You will learn about Kubernetes Ingress resource in detail. (Instance/IP mode, Health Check, SSL, SSL Redirect, SSL self discovery, internal alb, etc.) 14. You will deploy External DNS in Kubernetes to automatically manage DNS records in Route 53. 15. You will learn how to create NLB using AWS Load Balancer Controller. 16. You will deploy Metrics Server to monitor Kubernetes resource utilization. 17. You will learn how to set up EKS cluster autoscaling, horizontal pod autoscaling (HPA), and vertical pod autoscaling (VPA). 18. You will learn how to use EKS with ECR (Amazon Elastic Container Registry). 19. You will learn about the Microservice architecture and deploy a simple 3-tier application following the Microservice architecture. 20. You will learn how to set up Logging for your EKS cluster and applications using Fluent Bit and CloudWatch. 21. You will learn how to set up Monitoring for your EKS cluster and applications using Prometheus and Grafana open-source tools. 22. You will learn about Istio Service Mesh and its architecture. 23. You will learn about Gateways, Virtual services, and Destination rules in Istio traffic management. 24. You will deploy Kiali and Jaegar monitoring tools for observability and distributed tracing. 25. You will deploy a meshed microservices architecture consisting of 3 microservices. 26. You will reconfigure Istio to deploy Application Load Balancer instead of Classic Load Balancer. 27. You will learn about Istio traffic routing features (Traffic Splitting, Canary Deployment, Fault Injection, Circuit Breaker, Retry Strategy). 28. You will set up CI/CD using AWS CodePipeline and CodeDeploy. 29. You will set up Argo CD, a declarative, GitOps continuous delivery tool for Kubernetes. 30. You will set up continuous deployment using Argo CD. 31. You will learn about Helm Charts, and some other advanced Kubernetes concepts. 32. You will learn about EKS fargate profiles, Amazon VPC CNI and many other topics."},{"location":"kubernetes-on-eks/#what-are-the-requirements-or-prerequisites-for-taking-this-course","title":"What Are the Requirements or Prerequisites for Taking This Course?","text":"<ul> <li> <p>This course is designed to be a hands-on and practical experience. To fully participate and follow along, it is recommended to have an active AWS account but it is not mandatory.</p> </li> <li> <p>No programming, Docker and Kubernetes experience is needed. You'll go from zero to hero.</p> </li> <li> <p>The essential requirement for this program is a genuine interest in acquiring practical skills that will set you apart from your colleagues.</p> </li> </ul>"},{"location":"kubernetes-on-eks/#who-is-this-course-for","title":"Who Is This Course For?","text":"<ul> <li> <p>This course is for beginners interested in learning Kubernetes, Microservices, DevOps, and CI/CD using Amazon EKS.</p> </li> <li> <p>This course is for architects, developers, and system administrators who want to deploy a well-architected microservices architecture on Amazon EKS.</p> </li> <li> <p>Anyone who wants to learn how companies deploy applications on Amazon EKS, and how they use various tools to set up logging, monitoring, DevOps, CI/CD, and Service Mesh.</p> </li> <li> <p>This course covers both fundamental and advanced topics in Kubernetes, Logging, Monitoring, Service Mesh, and CI/CD technologies. As such, it is suitable for individuals who wish to expand their knowledge and grasp more complex concepts.</p> </li> </ul>"},{"location":"kubernetes-on-eks/#about-the-instructor","title":"About the Instructor","text":"<p>Hi, I'm Reyansh Kharga, your instructor for the course <code>Amazon EKS Masterclass: Kubernetes and Microservices on EKS</code>.</p> <p>As a Solutions Architect, I bring a wealth of experience and a deep passion for crafting, designing, and implementing highly available architectures on public cloud platforms.</p> <p>My expertise primarily revolves around the dynamic realm of cloud computing, where I've cultivated a strong affinity for AWS, Azure, GCP, Kubernetes, Microservices, Service Mesh, and the intricate world of Monitoring Tools, including Prometheus, Grafana, and Kiali.</p> <p>Over the years, I've had the privilege of assisting a diverse array of startups and clients in the deployment of resilient, secure, and highly available infrastructures across industry-leading cloud platforms such as AWS, Azure, and Google Cloud Platform.</p> <p>My approach combines a harmonious blend of cloud-native solutions, third-party software, and open-source technologies, all geared towards delivering cost-effective and efficient results.</p> <p>Join me on this learning journey, and let's explore the intricacies of Amazon EKS and the fascinating universe of kubernetes and microservices in the cloud together. I'm excited to share my knowledge and experiences with you.</p>"},{"location":"kubernetes-on-eks/license/","title":"License","text":"<p>Copyright  2023 Kloud Koncepts by Reyansh Kharga</p> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Commercial use, including but not limited to the reselling of copies or contents of the software, or the use of the software for business purposes, is strictly prohibited.</p> </li> <li> <p>Any use or distribution of the software, including all associated code, documentation, and multimedia content (such as images, gifs, videos, and audio files), must include clear attribution to the owner, Reyansh Kharga.</p> </li> <li> <p>The software is provided \"as is,\" without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and noninfringement. In no event shall the owners or copyright holders be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the software or the use or other dealings in the software.</p> </li> <li> <p>Anyone using or distributing the software must prominently display the original copyright notice and license terms in all copies and related documentation.</p> </li> <li> <p>The software may not be distributed under a different name or branding that suggests it is a different or distinct product.</p> </li> <li> <p>The software may not be used in any way that suggests endorsement or affiliation with any individual or organization without explicit permission from the owner.</p> </li> <li> <p>The license may not be modified or reinterpreted in any way that contradicts or undermines the rights of the owner to control the use and distribution of the software.</p> </li> <li> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.</p> </li> <li> <p>By using the software, you agree to these conditions and acknowledge that the owner, Reyansh Kharga, is not responsible for any issues that may arise from the use of the software.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/","title":"Introduction to Amazon ECR","text":"<p>Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable.</p>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/#components-of-amazon-ecr","title":"Components of Amazon ECR","text":"<p>Registry: An Amazon ECR private registry is provided to each AWS account; you can create one or more repositories in your registry and store images in them.</p> <p>Authorization token: Your client (Docker CLI in our case) must authenticate to Amazon ECR registries as an AWS user before it can push and pull images.</p> <p>Repository: An Amazon ECR repository contains your Docker images.</p> <p>Image: You can push and pull container images to your repositories.</p> <p>Repository policy: You can control access to your repositories and the images within them with repository policies.</p> <p>Now, let's see how you can create repositories in ECR and push images to it.</p>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/#step-1-create-a-docker-image","title":"Step 1: Create a Docker Image","text":"<p>First, create a Dockerfile as follows:</p> <code>Dockerfile</code> <code>index.html</code> <pre><code>FROM nginx:latest\nCOPY ./index.html /usr/share/nginx/html/index.html\n</code></pre> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Nginx&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h2&gt;Hello from Nginx container&lt;/h2&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Next, build the image from the Dockerfile above:</p> <pre><code>docker build -t my-nginx-image .\n</code></pre> <p>Verify that the image was created correctly:</p> <pre><code>docker images --filter reference=my-nginx-image\n</code></pre> <p>Run a container from the image to verify the correctness of image:</p> <pre><code>docker run -it -p 3000:80 my-nginx-image\n</code></pre> <p>Open any browser and hit <code>localhost:3000</code> to view the html page from nginx Docker container.</p>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/#step-2-create-amazon-ecr-repository","title":"Step 2: Create Amazon ECR Repository","text":"<p>Create a repository to which we will later push the <code>my-nginx-image:latest</code> image.</p> <pre><code># Command template\naws ecr create-repository \\\n    --repository-name &lt;repository-name&gt; \\\n    --image-scanning-configuration scanOnPush=true \\\n    --region &lt;region-name&gt;\n\n# Actual command\naws ecr create-repository \\\n    --repository-name my-nginx-repository \\\n    --image-scanning-configuration scanOnPush=true\n</code></pre>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/#step-3-authenticate-to-your-ecr","title":"Step 3: Authenticate to your ECR","text":"<p>Before we can push the images to Amazon ECR, we need to retrieve an authentication token and authenticate your Docker client to your registry. That way, the docker command can push and pull images with Amazon ECR.</p> <p>The AWS CLI provides a <code>get-login-password</code> command to simplify the authentication process.</p> <pre><code>aws ecr get-login-password --region &lt;region-name&gt; | docker login --username AWS --password-stdin &lt;aws-account-id&gt;.dkr.ecr.&lt;region-name&gt;.amazonaws.com\n</code></pre> <p>Make sure to replace the <code>region-name</code> and <code>aws-account-id</code> placeholders with the appropriate values.</p> <p>You can also get the <code>get-login-password</code> command from AWS Console by clicking View push commands.</p> <p>Here's the output you will see if the command succeeds:</p> <pre><code>Login Succeeded\n</code></pre>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/#step-4-push-the-docker-image-to-amazon-ecr","title":"Step 4: Push the Docker image to Amazon ECR","text":"<p>Prerequisites:</p> <ul> <li>The minimum version of docker is installed: 1.7</li> <li>The Amazon ECR authorization token has been configured with docker login.</li> <li>The Amazon ECR repository exists and the user has access to push to the repository.</li> </ul> <p>List the images you have stored locally to identify the image to tag and push:</p> <pre><code>docker images\n</code></pre> <p>Tag the image to push to your repository:</p> <pre><code>docker tag my-nginx-image:latest &lt;ecr-repository-uri&gt;:&lt;version&gt;\n</code></pre> <p>Push the image to ECR:</p> <pre><code>docker push &lt;ecr-repository-uri&gt;:&lt;version&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/amazon-ecr/introduction-to-amazon-ecr/#step-5-pull-an-image-from-amazon-ecr","title":"Step 5: Pull an image from Amazon ECR","text":"<p>After your image has been pushed to your Amazon ECR repository, you can pull it from other locations like your local machine.</p> <pre><code>docker pull &lt;ecr-repository-uri&gt;:&lt;version&gt;\n</code></pre> <p>References:</p> <ul> <li>Amazon ECR</li> <li>Using Amazon ECR with the AWS CLI</li> </ul>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/eks-architecture/","title":"Architecture of Amazon EKS","text":"<p>Amazon EKS runs a single tenant Kubernetes control plane for each cluster.</p> <p>The control plane infrastructure isn't shared across clusters or AWS accounts.</p> <p>The control plane consists of at least two <code>API server</code> instances and three <code>etcd</code> instances that run across three Availability Zones within an AWS Region.</p> <p>It is the responsibility of AWS to maintain high availability, high performance and scalability of the EKS control plane.</p> <p> </p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/eks-architecture/#how-does-amazon-eks-work","title":"How Does Amazon EKS Work?","text":"<ol> <li> <p>Create an Amazon EKS cluster in the AWS Management Console or with the AWS CLI or one of the AWS SDKs.</p> </li> <li> <p>Launch managed or self-managed Amazon EC2 nodes, or deploy your workloads to AWS Fargate.</p> </li> <li> <p>When your cluster is ready, you can configure your favorite Kubernetes tools, such as <code>kubectl</code>, to communicate with your cluster.</p> </li> <li> <p>Deploy and manage workloads on your Amazon EKS cluster the same way that you would with any other Kubernetes environment. You can also view information about your workloads using the AWS Management Console.</p> </li> </ol> <p> </p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/eks-architecture/#node-groups-in-amazon-eks","title":"Node Groups in Amazon EKS","text":"<p>A node group can be thought of as a logical grouping of worker nodes that share the same configuration and Amazon Machine Image (AMI).</p> <p>Amazon EKS provides three types of node groups to run worker nodes in a Kubernetes cluster.</p> <ol> <li>EKS Managed Node Groups</li> <li>Self-Managed Node Groups</li> <li>Fargate Node Groups</li> </ol>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/eks-architecture/#1-managed-node-groups","title":"1. Managed Node Groups","text":"<p>Managed Node Groups are a feature of Amazon Elastic Kubernetes Service (EKS) that allow you to launch and manage worker nodes for your Kubernetes cluster in an easy and scalable way. Managed Node Groups are fully-managed, meaning that AWS handles all the underlying infrastructure and management tasks, such as node provisioning, scaling, and patching. This allows you to focus on your applications rather than worrying about the underlying infrastructure.</p> <p>Advantages of EKS Managed Node Groups:</p> <ul> <li>Fully managed by AWS</li> <li>Automatic scaling of worker nodes</li> <li>High availability and reliability</li> <li>Easy integration with other AWS services</li> </ul> <p>Disadvantages of EKS Managed Node Groups:</p> <ul> <li>Limited customization options</li> <li>AWS controls the underlying infrastructure</li> </ul>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/eks-architecture/#2-self-managed-node-groups","title":"2. Self-Managed Node Groups","text":"<p>Self-Managed Node Groups are an alternative to Managed Node Groups in Amazon Elastic Kubernetes Service (EKS). Unlike Managed Node Groups, Self-Managed Node Groups are not fully managed by AWS, and require more manual configuration and management on your part.</p> <p>With Self-Managed Node Groups, you are responsible for provisioning and configuring the underlying Amazon Elastic Compute Cloud (EC2) instances, as well as managing the lifecycle of the nodes. This includes tasks such as scaling, patching, and updating the nodes.</p> <p>Advantages of Self Managed Node Groups:</p> <ul> <li>More control over underlying infrastructure</li> <li>Ability to customize node configuration</li> <li>Can run on EC2 or on-premises servers</li> </ul> <p>Disadvantages of Self Managed Node Groups:</p> <ul> <li>Users are responsible for managing the worker nodes</li> <li>No automatic scaling</li> </ul>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/eks-architecture/#3-fargate-node-groups","title":"3. Fargate Node Groups","text":"<p>Fargate Node Groups are a feature of Amazon Elastic Kubernetes Service (EKS) that allows you to run Kubernetes pods on AWS Fargate, a serverless compute engine for containers. With Fargate Node Groups, you no longer need to manage the underlying EC2 instances for your Kubernetes cluster - instead, AWS manages the infrastructure for you.</p> <p>Advantages of Fargate Node Groups:</p> <ul> <li>Serverless computing</li> <li>Fully managed by AWS</li> <li>Automatic scaling</li> </ul> <p>Disadvantages of Fargate Node Groups:</p> <ul> <li>Limited customization options</li> <li>AWS controls the underlying infrastructure</li> </ul> <p>References:</p> <ul> <li>What is Amazon EKS?</li> <li>Amazon EKS architecture</li> </ul>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/","title":"Introduction to Amazon EKS","text":"<p>Amazon EKS (Elastic Kubernetes Service) is a fully-managed Kubernetes service offered by AWS.</p> <p>It simplifies the process of deploying, managing, and scaling containerized applications using Kubernetes on AWS infrastructure.</p> <p>With Amazon EKS, users can create Kubernetes clusters with just a few clicks, and easily deploy, manage, and scale containerized applications on top of them.</p> <p>EKS provides a highly-available and secure control plane for Kubernetes and automatically manages the scaling, patching, and maintenance of the underlying infrastructure.</p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/#key-features-of-amazon-eks","title":"Key Features of Amazon EKS","text":""},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/#1-fully-managed-kubernetes-service","title":"1. Fully-managed Kubernetes service","text":"<p>Amazon EKS is a fully managed service, meaning that AWS manages the control plane of Kubernetes for you, which includes the API server, etcd, and scheduler. This frees you up from the operational overhead of managing a Kubernetes control plane, allowing you to focus on deploying and scaling your containerized applications.</p> <p> </p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/#2-scalability","title":"2. Scalability","text":"<p>Amazon EKS is designed to be highly scalable. It can scale up or down automatically based on demand. Additionally, you can use Auto Scaling groups to automatically adjust the size of your worker node groups to meet the demands of your applications.</p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/#3-high-availability","title":"3. High availability","text":"<p>Amazon EKS is designed to be highly available, with multiple availability zones (AZs) to ensure that your Kubernetes cluster is always available even if an AZ goes down. Additionally, EKS automatically replaces unhealthy control plane instances and provides automated node recovery.</p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/#4-security-and-compliance","title":"4. Security and Compliance","text":"<p>Amazon EKS is designed to be secure and compliant with industry standards. EKS uses AWS Identity and Access Management (IAM) to manage user access to Kubernetes resources, and it also provides built-in security features like network isolation and encryption of data in transit and at rest.</p>"},{"location":"kubernetes-on-eks/amazon-eks-overview-and-architecture/introduction-to-amazon-eks/#5-seamless-integration-with-aws-services","title":"5. Seamless integration with AWS services","text":"<p>Amazon EKS is tightly integrated with other AWS services, including Amazon Elastic Container Registry (ECR), AWS Identity and Access Management (IAM), AWS CloudFormation, and more. This integration makes it easy to deploy and manage your containerized applications on AWS.</p> <p>Overall, Amazon EKS is a powerful and feature-rich Kubernetes service that can help you deploy and manage containerized applications on AWS with ease.</p>"},{"location":"kubernetes-on-eks/argo-cd/create-argo-cd-application/","title":"Create Argo CD Application","text":"<p>An ArgoCD application is a Kubernetes Custom Resource Definition (CRD) used to define and manage the deployment of an application to a Kubernetes cluster.</p> <p>An ArgoCD application resource defines the source repository, the target Kubernetes namespace, and the synchronization options, among other things.</p> <p>You can create Argo CD application from the Argo CD UI or declaratively using the YAML file.</p> <p>We'll set it up declaratively.</p>"},{"location":"kubernetes-on-eks/argo-cd/create-argo-cd-application/#step-1-prepare-kubernetes-manifest-files","title":"Step 1: Prepare Kubernetes Manifest Files","text":"<p>Create a private git repo and add manifest files. I'll name the git repo kubernetes-manifests (You can name anything you want). In that repository I'll create a folder for node app called node-app.</p> <p>Here's how the folder structure looks like:</p> <pre><code>|-- node-app\n\u2502   |-- 00-namespace.yml\n\u2502   |-- deployment.yml\n\u2502   |-- service.yml\n</code></pre> <code>00-namespace.yml</code> <code>deployment.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: nodeapp\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nodeapp\n  namespace: nodeapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre>"},{"location":"kubernetes-on-eks/argo-cd/create-argo-cd-application/#step-2-create-argo-cd-application","title":"Step 2: Create Argo CD Application","text":"<p>Now that we have our kubernetes manifest files ready, let's create Argo CD application.</p> <code>argo.master.yml</code> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: node-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/ReyanshKharga/kubernetes-manifests.git\n    targetRevision: master\n    path: node-app\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: node-app\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>Apply the manifest to create the Argo CD application:</p> <pre><code>kubectl apply -f argo-node-app.yml\n</code></pre> <p>The application will be deployed but it will have an error since our repo is private. We need to configure argo cd to access git repo.</p> <p>Credentials can be configured using Argo CD CLI:</p> <pre><code>argocd repo add https://github.com/argoproj/argocd-example-apps --username &lt;username&gt; --password &lt;password&gt;\n</code></pre> <p>Or you can use the UI. Navigate to <code>Settings/Repositories</code> and Click <code>Connect Repo using HTTPS</code> button and enter credentials.</p> <p>Click <code>Connect</code> to test the connection and have the repository added.</p> <p>You can also set up credentials to serve as templates for connecting repositories, without having to repeat credential configuration. For example, if you setup credential templates for the URL prefix <code>https://github.com/argoproj</code>, these credentials will be used for all repositories with this URL as prefix (e.g. <code>https://github.com/argoproj/argocd-example-apps</code>) that do not have their own credentials configured.</p> <p>To set up a <code>credential template</code> using the Web UI, simply fill in all relevant credential information in the Connect repo using SSH or Connect repo using HTTPS dialogues (as described above), but select Save as credential template instead of Connect to save the credential template. Be sure to only enter the prefix URL (i.e. <code>https://github.com/argoproj</code>) instead of the complete repository URL (i.e. <code>https://github.com/argoproj/argocd-example-apps</code>) in the field Repository URL</p>"},{"location":"kubernetes-on-eks/argo-cd/create-argo-cd-application/#argo-cd-application-for-helm","title":"Argo CD Application for Helm","text":"<p>What if your kubernetes objects are defined as helm charts? How do you write Argo CD application for that?</p> <p>Here's an example:</p> <code>argo.master.yml</code> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: node-app-helm\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/ReyanshKharga/kubernetes-manifests.git\n    targetRevision: master\n    path: node-app-helm\n    helm:\n      valueFiles:\n        - values.prod.yaml\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: node-app-helm\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>Here we have defined the repo, branch, path in the repo and which file to consider for helm values.</p> <p>References:</p> <ul> <li>Connect Repo in Argo CD</li> </ul>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/","title":"Introduction to Argo CD","text":"<p>Let's see how you can install Argo CD in you kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-1-create-namespace-for-arogo-cd","title":"Step 1: Create Namespace for Arogo CD","text":"<p>First, let's create a namespace for Argo CD:</p> <pre><code># Create argocd namespace\nkubectl create namespace argocd\n</code></pre>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-2-install-argo-cd","title":"Step 2: Install Argo CD","text":"<p>Now, let's install Argo CD in the <code>argocd</code> namespace that we created:</p> <pre><code># Install Argo CD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-3-verify-argo-cd-installation","title":"Step 3: Verify Argo CD Installation","text":"<pre><code># List pods\nkubectl get pods -n argocd\n\n# List services\nkubectl get svc -n argocd\n</code></pre>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-4-access-argo-cd-on-local-machine","title":"Step 4: Access Argo CD on Local Machine","text":"<p>Let's use port forward to access Argo CD on local machine:</p> <pre><code>kubectl port-forward svc/argocd-server 8080:443 -n argocd\n</code></pre> <p>Visit <code>localhost:80</code> on your local host machine and you will be able to access Argo CD.</p>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-5-retrieve-login-credentials","title":"Step 5: Retrieve Login Credentials","text":"<p>The initial password for the admin account/user is auto-generated and stored as clear text in the field <code>password</code> in a secret named <code>argocd-initial-admin-secret</code> in your Argo CD installation namespace.</p> <p>Let's retrieve the password from the <code>argocd-initial-admin-secret</code> as follows:</p> <pre><code>kubectl get secrets argocd-initial-admin-secret -n argocd -o yaml\n</code></pre> <p>Copy the value of the <code>password</code> field from the output of the above command. The password is base64 encoded.</p> <p>Let's decode the password:</p> <pre><code>echo &lt;encoded-password&gt; | base64 --decode\n</code></pre> <p>Now, use <code>admin</code> user with this password to login to Argo CD.</p> <p>Argo CD also provides Argo CD CLI to perform operations using CLI. The CLI lets you interact with the Argo CD server using a terminal window.</p>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-6-install-argo-cd-cli","title":"Step 6: Install Argo CD CLI","text":"<p>Use the following commands to install the Argo CD CLI:</p> <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre> <p>Verify CLI installation:</p> <pre><code>argocd version --client\n</code></pre> <p>Let's get the initial admin password using the Argo CD CLI:</p> <pre><code>argocd admin initial-password -n argocd\n</code></pre> <p>Login using CLI:</p> <pre><code>argocd login localhost:8080\n</code></pre> <p>List accounts and its details:</p> <pre><code># List accounts\nargocd account list\n\n# Get user specific details\nargocd account get --account admin\n</code></pre>"},{"location":"kubernetes-on-eks/argo-cd/install-argo-cd/#step-7-create-new-user","title":"Step 7: Create New User","text":"<p>Let's create new user <code>reyansh</code>. For this we need to update the <code>argocd-cm</code> ConfigMap.</p> <pre><code># set editor to nano\nexport KUBE_EDITOR=nano\n\n# Edit configmap\nkubectl edit configmap argocd-cm -n argocd\n</code></pre> <p>Add the following in the <code>argocd-cm</code> ConfigMap:</p> <pre><code>data:\n  accounts.reyansh: apiKey, login\n</code></pre> <p>Verify if user was created:</p> <pre><code># List accounts\nargocd account list\n\n# Get user specific details\nargocd account get --account reyansh\n</code></pre> <p>Set user password:</p> <pre><code>argocd account update-password \\\n  --account &lt;name&gt; \\\n  --current-password &lt;current-user-password&gt; \\\n  --new-password &lt;new-user-password&gt;\n</code></pre> <p>If you are managing users as the admin user, <code>current-user-password</code> should be the current admin password.</p> <p>As soon as additional users are created it is recommended to disable admin user. Update the configmap as follows:</p> <pre><code>data:\n  admin.enabled: \"false\"\n</code></pre> <p>References:</p> <ul> <li>Install Argo CD</li> <li>Install Argo CD CLI</li> </ul>"},{"location":"kubernetes-on-eks/argo-cd/introduction-to-argo-cd/","title":"Introduction to Argo CD","text":"<p>Argo CD is a declarative, GitOps continuous delivery (CD) tool for kubernetes.</p>"},{"location":"kubernetes-on-eks/argo-cd/introduction-to-argo-cd/#what-is-gitops","title":"What is GitOps?","text":"<p>GitOps uses Git repositories as a single source of truth to deliver infrastructure as code.</p> <p>Basically, any changes in the infrastructure should be via code in the repository and that should be the source of truth.</p>"},{"location":"kubernetes-on-eks/argo-cd/introduction-to-argo-cd/#why-argo-cd","title":"Why Argo CD?","text":"<p>Application deployment and lifecycle management should be automated, auditable, and easy to understand. This is lacking in other CI/CD tools like AWS CodeBuild, CodeDeploy, Jenkins etc.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/cluster-autoscaler-demo/","title":"Cluster Autoscaler Demo","text":"<p>Now that we have the Cluster Autoscaler deployed in our EKS cluster, let's see it in action.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/cluster-autoscaler-demo/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/cluster-autoscaler-demo/#objective","title":"Objective","text":"<p>Currently, we have 2 <code>t3.medium</code> nodes in <code>private-nodegroup</code>. We also have <code>min(2)</code> and <code>max(4)</code> nodes set for the nodegroup.</p> <pre><code># List nodes\nkubectl get nodes\n</code></pre> <p>Note</p> <p>Each <code>t3.medium</code> node has 2 core CPU and 4GB memory.</p> <p>We'll follow these steps to test the Cluster Autoscaler:</p> <ol> <li>We'll create a deployment with 2 replicas, each requesting 1 core CPU.</li> <li>We'll scale the deployment to add more replicas.</li> <li>We'll observe Cluster Autoscaler taking autoscaling actions to accommodate pending pods that couldn't be scheduled due to insufficient resources.</li> </ol> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/cluster-autoscaler-demo/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          requests:\n            cpu: \"1\"\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> <p>Note that each pod requests 1 core CPU. Also, we already have some workload running in the <code>kube-system</code> namespace. In the best case, the two nodes can accommodate two pods created by <code>my-deployment</code>.</p> <p>You will observe that both pods are scheduled and running without any issues.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/cluster-autoscaler-demo/#step-2-update-the-deployment","title":"Step 2: Update the Deployment","text":"<p>Now, let's update the deployment by setting the replicas to 4.</p> <pre><code># Apply the modified deployment\nkubectl apply -f deployment.yml\n\n# List pods\nkubectl get pods\n</code></pre> <p>You might notice the new pods getting stuck in the <code>Pending</code> state due to the lack of CPU resources.</p> <p>Describe the pod to determine why it is still in a <code>Pending</code> state:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>At this point, the Cluster Autoscaler will assess the autoscaling needs and launch additional instances in the node group.</p> <p>Once the new nodes are up and running, the new pods can be scheduled.</p> <p>List nodes and pods in watch mode:</p> <pre><code># List nodes\nkubectl get nodes -w\n\n# List pods\nkubectl get pods -w\n</code></pre> <p>You can view the autoscaling events in the <code>cluster-autoscaler</code> log:</p> <pre><code># View autoscaler logs\nkubectl logs -f deployment.apps/cluster-autoscaler -n kube-system\n</code></pre> <p>You can also use the following command to see kubernetes events:</p> <pre><code>kubectl events\n</code></pre> <p>Note</p> <p>When you run <code>kubectl events</code> command, it retrieves and displays a list of events from the cluster's Event API. These events can include information about pod scheduling, container lifecycle events, resource creation, deletion, errors, and much more.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/cluster-autoscaler-demo/#step-3-delete-the-deployment","title":"Step 3: Delete the Deployment","text":"<p>Let's delete the deployment:</p> <pre><code>kubectl delete -f deployment.yml\n</code></pre> <p>When the resources are freed you will notice that the autoscaling scales down the instances in the autoscaling group.</p> <p>You can view the autoscaling events in the <code>cluster-autoscaler</code> log:</p> <pre><code>kubectl logs -f deployment.apps/cluster-autoscaler -n kube-system\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/","title":"Deploy Kubernetes Cluster AutoScaler in EKS","text":"<p>The <code>Cluster Autoscaler</code> uses AWS scaling groups. It automatically adjusts the number of nodes in your cluster when pods fail or are rescheduled onto other nodes.</p> <p>The Cluster Autoscaler is typically installed as a Deployment in your cluster.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/#prerequisites","title":"Prerequisites","text":"<p>The Cluster Autoscaler requires the following tags on your Auto Scaling groups so that they can be auto-discovered.</p> Key Value <code>k8s.io/cluster-autoscaler/&lt;eks-cluster-name&gt;</code> <code>owned</code> <code>k8s.io/cluster-autoscaler/enabled</code> <code>true</code> <p>Note</p> <p>If you used <code>eksctl</code> to create your node groups, these tags are automatically applied.</p> <p>If you didn't use <code>eksctl</code>, you must manually tag your Auto Scaling groups with the tags mentioned above. Make sure to replace <code>&lt;eks-cluster-name&gt;</code> in <code>k8s.io/cluster-autoscaler/&lt;eks-cluster-name&gt;</code> with the actual name of your eks cluster.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/#step-1-download-the-required-iam-policy","title":"Step 1: Download the Required IAM Policy","text":"<p>First, get the IAM Policy from official git repository. It should look something like this:</p> <code>cluster-autoscaler-policy.json</code> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeAutoScalingInstances\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DescribeTags\",\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:DescribeLaunchTemplateVersions\"\n      ],\n      \"Resource\": [\"*\"]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:SetDesiredCapacity\",\n        \"autoscaling:TerminateInstanceInAutoScalingGroup\",\n        \"ec2:DescribeImages\",\n        \"ec2:GetInstanceTypesFromInstanceRequirements\",\n        \"eks:DescribeNodegroup\"\n      ],\n      \"Resource\": [\"*\"]\n    }\n  ]\n}\n</code></pre> <p>The above IAM policy permits the <code>Cluster Autoscaler</code> to describe and manage Auto Scaling Groups, instances, launch configurations, scaling activities, tags, and instance types. It also grants access to describe images, get instance types from instance requirements in EC2, and describe node groups in Amazon EKS.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/#step-2-create-iam-policy","title":"Step 2: Create IAM Policy","text":"<p>Create the policy with the following command. You can change the value for <code>policy-name</code> to a desired value.</p> <pre><code>aws iam create-policy \\\n    --policy-name AmazonEKSClusterAutoscalerPolicy \\\n    --policy-document file://cluster-autoscaler-policy.json\n</code></pre> <p>Note down the <code>ARN</code> of the policy that was created. We'll need it in the next step.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/#step-3-create-iam-role-and-service-account","title":"Step 3: Create IAM Role and Service Account","text":"<p>We'll use IAM Roles for Service Accounts (IRSA) to grant <code>Cluster Autoscaler</code> permission to AWS resources. So, let's create IRSA as follows:</p> <pre><code>eksctl create iamserviceaccount \\\n  --cluster=&lt;cluster-name&gt; \\\n  --namespace=kube-system \\\n  --name=cluster-autoscaler \\\n  --attach-policy-arn=&lt;policy-arn&gt; \\\n  --override-existing-serviceaccounts \\\n  --approve\n</code></pre> <p>This above command when executed will create an IAM Role and a Service Account in the EKS cluster. It will also annotate the service account with the role that it creates.</p> <p>Verify the service account:</p> <pre><code># List service accounts in kube-system namespace\nkubectl get sa -n kube-system\n\n# List service accounts in kube-system namespace with a filter\nkubectl get sa -n kube-system | grep cluster-autoscaler\n\n# View the service account definition in yaml format\nkubectl get sa cluster-autoscaler -n kube-system -o yaml\n</code></pre> <p>Also, go to AWS console and verify the IAM role that was created. You can get the role name from the annotation in the service account that was created.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/#step-4-deploy-the-cluster-autoscaler","title":"Step 4: Deploy the Cluster Autoscaler","text":"<p>With the service account ready, we can now move forward and deploy <code>Cluster Autoscaler</code>.</p> <p>First, download the YAML manifest for Cluster Autoscaler:</p> <pre><code># Download external-dns manifest\ncurl -o cluster-autoscaler-autodiscover.yaml https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml\n</code></pre> <p>Now, before we proceed with the installation of this manifest, we need to make some modifications to the YAML manifest we downloaded:</p> <ol> <li> <p>Replace <code>&lt;YOUR CLUSTER NAME&gt;</code> in container command of the <code>cluster-autoscaler</code> Deployment object with the name of your EKS cluster.</p> </li> <li> <p>Modify the <code>cluster-autoscaler</code> Deployment object by adding the <code>cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'</code> annotation in <code>.spec.template.metadata.annotations</code> section.</p> </li> <li> <p>Modify the <code>cluster-autoscaler</code> container of the <code>cluster-autoscaler</code> Deployment by adding the following options:</p> <ul> <li><code>--balance-similar-node-groups</code></li> <li><code>--skip-nodes-with-system-pods=false</code></li> </ul> <p>The final container command of the <code>cluster-autoscaler</code> Deployment should look something like this:</p> <pre><code>command:\n  - ./cluster-autoscaler\n  - --v=4\n  - --stderrthreshold=info\n  - --cloud-provider=aws\n  - --skip-nodes-with-local-storage=false\n  - --expander=least-waste\n  - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster\n  - --balance-similar-node-groups\n  - --skip-nodes-with-system-pods=false\n</code></pre> </li> <li> <p>Get the Deployment Image tag:</p> <p>Open the Cluster Autoscaler releases page from GitHub in a web browser and find the latest Cluster Autoscaler version that matches the kubernetes major and minor version of your cluster. </p> <p>For example, if the kubernetes version of your cluster is <code>1.28</code>, find the latest Cluster Autoscaler release that begins with <code>1.28</code>. Record the semantic version number (<code>1.28.n</code>) for that release to use in the next step.</p> <p>Tip</p> <p>Use the search bar to search a specific version. For example you can search <code>Cluster Autoscaler 1.28</code>. Note down the semantic version number. For example <code>1.28.0</code>.</p> </li> <li> <p>Modify the Deployment Image tag:</p> <p>Modify the YAML manifest file to set the <code>cluster-autoscaler</code> Deployment image tag to the version that you recorded in the previous step. In my case I have changed it to the following:</p> <pre><code>registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.0\n</code></pre> </li> <li> <p>Modify the YAML manifest file to omit the <code>cluster-autoscaler</code> ServiceAccount kubernetes object since we already created this using <code>eksctl</code>.</p> </li> </ol> <p>After all the modifications, the updated YAML manifest file should look something like this:</p> Expand to see the updated manifest <code>cluster-autoscaler-autodiscover.yaml</code> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"events\", \"endpoints\"]\n    verbs: [\"create\", \"patch\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods/eviction\"]\n    verbs: [\"create\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods/status\"]\n    verbs: [\"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"endpoints\"]\n    resourceNames: [\"cluster-autoscaler\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"watch\", \"list\", \"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources:\n      - \"namespaces\"\n      - \"pods\"\n      - \"services\"\n      - \"replicationcontrollers\"\n      - \"persistentvolumeclaims\"\n      - \"persistentvolumes\"\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"extensions\"]\n    resources: [\"replicasets\", \"daemonsets\"]\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"policy\"]\n    resources: [\"poddisruptionbudgets\"]\n    verbs: [\"watch\", \"list\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"statefulsets\", \"replicasets\", \"daemonsets\"]\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"storage.k8s.io\"]\n    resources: [\"storageclasses\", \"csinodes\", \"csidrivers\", \"csistoragecapacities\"]\n    verbs: [\"watch\", \"list\", \"get\"]\n  - apiGroups: [\"batch\", \"extensions\"]\n    resources: [\"jobs\"]\n    verbs: [\"get\", \"list\", \"watch\", \"patch\"]\n  - apiGroups: [\"coordination.k8s.io\"]\n    resources: [\"leases\"]\n    verbs: [\"create\"]\n  - apiGroups: [\"coordination.k8s.io\"]\n    resourceNames: [\"cluster-autoscaler\"]\n    resources: [\"leases\"]\n    verbs: [\"get\", \"update\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"create\",\"list\",\"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    resourceNames: [\"cluster-autoscaler-status\", \"cluster-autoscaler-priority-expander\"]\n    verbs: [\"delete\", \"get\", \"update\", \"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cluster-autoscaler\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-addon: cluster-autoscaler.addons.k8s.io\n    k8s-app: cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler\n    namespace: kube-system\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cluster-autoscaler\n  namespace: kube-system\n  labels:\n    app: cluster-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cluster-autoscaler\n  template:\n    metadata:\n      labels:\n        app: cluster-autoscaler\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '8085'\n    spec:\n      priorityClassName: system-cluster-critical\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n        fsGroup: 65534\n      serviceAccountName: cluster-autoscaler\n      containers:\n        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.0\n          name: cluster-autoscaler\n          resources:\n            limits:\n              cpu: 100m\n              memory: 600Mi\n            requests:\n              cpu: 100m\n              memory: 600Mi\n          command:\n            - ./cluster-autoscaler\n            - --v=4\n            - --stderrthreshold=info\n            - --cloud-provider=aws\n            - --skip-nodes-with-local-storage=false\n            - --expander=least-waste\n            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster\n            - --balance-similar-node-groups\n            - --skip-nodes-with-system-pods=false\n          volumeMounts:\n            - name: ssl-certs\n              mountPath: /etc/ssl/certs/ca-certificates.crt #/etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes\n              readOnly: true\n          imagePullPolicy: \"Always\"\n      volumes:\n        - name: ssl-certs\n          hostPath:\n            path: \"/etc/ssl/certs/ca-bundle.crt\"\n</code></pre> <p>Now, apply the modified manifest file in your EKS cluster to deploy the Cluster Autoscaler:</p> <pre><code>kubectl apply -f cluster-autoscaler-autodiscover.yaml\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/deploy-cluster-autoscaler-in-eks/#step-5-view-cluster-autoscaler-logs","title":"Step 5: View Cluster Autoscaler logs","text":"<p>After you have deployed the Cluster Autoscaler, you can view the logs and verify that it's monitoring your cluster load.</p> <p>View your Cluster Autoscaler logs using the following command:</p> <pre><code>kubectl logs -f deployment.apps/cluster-autoscaler -n kube-system\n</code></pre> <p>The output should look something like this:</p> <pre><code>I1205 06:53:27.855003       1 static_autoscaler.go:230] Starting main loop\nI1205 06:53:27.855390       1 filter_out_schedulable.go:65] Filtering out schedulables\nI1205 06:53:27.855400       1 filter_out_schedulable.go:132] Filtered out 0 pods using hints\nI1205 06:53:27.855405       1 filter_out_schedulable.go:170] 0 pods were kept as unschedulable based on caching\nI1205 06:53:27.855409       1 filter_out_schedulable.go:171] 0 pods marked as unschedulable can be scheduled.\nI1205 06:53:27.855414       1 filter_out_schedulable.go:82] No schedulable pods\nI1205 06:53:27.855423       1 static_autoscaler.go:419] No unschedulable pods\nI1205 06:53:27.855433       1 static_autoscaler.go:466] Calculating unneeded nodes\nI1205 06:53:27.855444       1 pre_filtering_processor.go:66] Skipping ip-192-168-100-52.ap-south-1.compute.internal - node group min size reached\nI1205 06:53:27.855449       1 pre_filtering_processor.go:66] Skipping ip-192-168-71-21.ap-south-1.compute.internal - node group min size reached\nI1205 06:53:27.855467       1 static_autoscaler.go:520] Scale down status: unneededOnly=false lastScaleUpTime=2022-12-05 05:49:57.069585724 +0000 UTC m=-3579.325882549 lastScaleDownDeleteTime=2022-12-05 05:49:57.069585724 +0000 UTC m=-3579.325882549 lastScaleDownFailTime=2022-12-05 05:49:57.069585724 +0000 UTC m=-3579.325882549 scaleDownForbidden=false isDeleteInProgress=false scaleDownInCooldown=false\nI1205 06:53:27.855487       1 static_autoscaler.go:533] Starting scale down\nI1205 06:53:27.855513       1 scale_down.go:918] No candidates for scale down\n</code></pre> <p>References:</p> <ul> <li>Cluster Autoscaler YAML For AWS</li> <li>Cluster Autoscaler IAM Policy</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/introduction-to-cluster-autoscaling/","title":"Introduction to Cluster Autoscaling in EKS","text":"<p>Autoscaling is a function that automatically scales your resources up or down to meet changing demands. This is a major kubernetes function that would otherwise require extensive human resources to perform manually.</p> <p>In this course, we'll mainly talk about Cluster Autoscaler, a well-known tool for adjusting the size of EKS clusters based on workload demands. While Karpenter is another useful autoscaling tool, we'll cover it as a bonus topic towards the end of the course.</p> <p>The kubernetes <code>Cluster Autoscaler</code> automatically adjusts the number of nodes in your cluster when pods fail or are rescheduled onto other nodes.</p> <p>The primary goal of the <code>Cluster Autoscaler</code> is to ensure that there are enough resources available to meet the demand of the applications running in the cluster, and to scale the cluster size up or down accordingly.</p>"},{"location":"kubernetes-on-eks/autoscaling/cluster-autoscaling/introduction-to-cluster-autoscaling/#how-does-cluster-autoscaler-work","title":"How Does Cluster Autoscaler Work?","text":"<p>Here's how Cluster Autoscaler works in kubernetes:</p> <p> </p> <ol> <li>Kubernetes Cluster Autoscaler detects pods in the <code>pending</code> state due to insufficient resources.</li> <li>The Cluster Autoscaler increases the desired number of instances in the Autoscaling Group.</li> <li>AWS Autoscaling provisions new nodes to match the desired instances count.</li> <li>Finally, the pending pods are scheduled on the new nodes that come up.</li> </ol> <p>References:</p> <ul> <li>Cluster Autoscaler</li> <li>Karpenter</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/","title":"Horizontal Pod Autoscaler (HPA) Demo","text":"<p>Now that you have an understanding of how HPA works, let's see it in action.</p>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#objective","title":"Objective","text":"<p>We'll follow these steps to test the Horizontal Pod Autoscaler (HPA):</p> <ol> <li>We'll create a <code>Deployment</code> and a <code>Service</code> object.</li> <li>We'll create <code>HorizontalPodAutoscaler</code> object for the deployment.</li> <li>We'll generate load on pods managed by the deployment.</li> <li>We'll observe HPA taking autoscaling actions to meet the increased demand.</li> </ol> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        resources:\n          limits:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> <p>Note that each pod can consume a maximum of <code>100m</code> CPU and <code>128Mi</code> memory.</p>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#step-3-create-hpa-for-the-deployment","title":"Step 3: Create HPA for the Deployment","text":"<p>Now, let's create a HPA for the deployment as follows:</p> <code>my-hpa.yml</code> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-deployment\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 50\n</code></pre> <p>Apply the manifest to create the HPA:</p> <pre><code>kubectl apply -f my-hpa.yml\n</code></pre> <p>Verify HPA:</p> <pre><code># List hpa\nkubectl get hpa\n\n# Describe hpa to view the events\nkubectl descripe hpa my-hpa\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#step-4-generate-load","title":"Step 4: Generate Load","text":"<p>Let's generate load on the pods managed by the deployment. On your local machine run the following command to generate the load:</p> <pre><code>while sleep 1; do seq 1000 | xargs -P100 -I{} curl -s &lt;load-balancer-dns&gt; &gt; /dev/null; done\n</code></pre> <p>The above command concurrently sends 1000 requests per second to the LoadBalancer service using 100 parallel processes.</p>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#step-5-monitor-pods-and-hpa-events","title":"Step 5: Monitor Pods and HPA Events","text":"<pre><code># List pods in watch mode\nkubectl get pods -w\n\n# List hpa in watch mode\nkubectl get hpa -w\n\n# View hpa events\nkubectl describe hpa my-hpa\n</code></pre> <p>You'll notice that as soon as any of the defined threshold is crossed the hpa scales the number of replicas to ensure the resource utilization is within the defined threshold.</p> <p>Here's a sample event from the hpa:</p> <pre><code>Conditions:\n  Type            Status  Reason              Message\n  ----            ------  ------              -------\n  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 2\n  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  SuccessfulRescale  5s    horizontal-pod-autoscaler  New size: 2; reason: cpu resource utilization (percentage of request) above target\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/horizontal-pod-autoscaler-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-hpa.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/introduction-to-horizontal-pod-autoscaler/","title":"Introduction to Horizontal Pod Autoscaler (HPA)","text":"<p>The kubernetes <code>Horizontal Pod Autoscaler (HPA)</code> automatically scales the number of pods in a deployment, replication controller, or replica set based on that resource's CPU utilization.</p> <p>This can help your applications scale out to meet increased demand or scale in when resources are not needed, thus freeing up your nodes for other applications. When you set a target CPU utilization percentage, the Horizontal Pod Autoscaler scales your application in or out to try to meet that target.</p> <p>The Horizontal Pod Autoscaler is a standard API resource in kubernetes that simply requires that a metrics source (such as the kubernetes metrics server) is installed on your Amazon EKS cluster to work. You do not need to deploy or install the Horizontal Pod Autoscaler on your cluster to begin scaling your applications.</p> <p>You need to set <code>requests</code> and/or <code>limits</code> for containers in order to allow HPA to calculate resource utilization percentage which it can then use to determine autoscaling actions.</p> <p>Note</p> <p>You must have a metrics source for example <code>metrics-server</code> installed in your kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/autoscaling/horizontal-pod-autoscaling/introduction-to-horizontal-pod-autoscaler/#how-does-hpa-work","title":"How does HPA Work?","text":"<p>Here's how Horizontal Pod Autoscaler (HPA) works in kubernetes:</p> <p> </p> <ol> <li>HPA continuously monitors the metrics server for resource usage.</li> <li>Based on the collected resource usage, HPA calculates the desired number of replicas.</li> <li>HPA updates the replica count of the deployment to the desired number.</li> <li>The <code>ReplicaSet</code> scales the app to desired replicas.</li> </ol> <p>References:</p> <ul> <li>Horizontal Pod Autoscaler</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/deploy-vertical-pod-autoscaler/","title":"Deploy Vertical Pod Autoscaler in EKS","text":"<p>Let's deploy the Vertical Pod Autoscaler (VPA) in our EKS cluster.</p> <ol> <li> <p>Clone the kubernetes/autoscaler GitHub repository:</p> <pre><code>git clone https://github.com/kubernetes/autoscaler.git\n</code></pre> </li> <li> <p>Change to the <code>vertical-pod-autoscaler</code> directory:</p> <pre><code>cd autoscaler/vertical-pod-autoscaler/\n</code></pre> </li> <li> <p>(Optional) If you have already deployed another version of the Vertical Pod Autoscaler, remove it with the following command:</p> <pre><code>./hack/vpa-down.sh\n</code></pre> </li> <li> <p>Deploy the Vertical Pod Autoscaler (VPA) to your cluster with the following command: </p> <pre><code>./hack/vpa-up.sh\n</code></pre> </li> <li> <p>Verify that the Vertical Pod Autoscaler pods have been created successfully:</p> <pre><code>kubectl get pods -n kube-system | grep vpa\n</code></pre> <p>The output will look somthing like this:</p> <pre><code>vpa-admission-controller-bb59bb7c7-g7vqr        1/1     Running   0              8s\nvpa-recommender-5555d76bfd-9rzz7                1/1     Running   0              11s\nvpa-updater-8b7f687dc-pnqkh                     1/1     Running   0              13s\n</code></pre> </li> </ol> <p>References:</p> <ul> <li>Deploy Vertical Pod Autoscaler</li> <li>kubernetes/autoscaler GitHub Repo</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/introduction-to-vertical-pod-autoscaler/","title":"Introduction to Vertical Pod Autoscaler (VPA)","text":"<p>The Kubernetes <code>Vertical Pod Autoscaler (VPA)</code> is a resource management component that helps optimize the resource allocation for pods in a kubernetes cluster. Unlike the Horizontal Pod Autoscaler (HPA), which adjusts the number of pod replicas based on CPU or memory utilization, the Vertical Pod Autoscaler focuses on adjusting the resource limits and requests for individual pod containers.</p> <p>In summary, the kubernetes Vertical Pod Autoscaler automatically adjusts the CPU and memory reservations for your pods to help \"right size\" your applications.</p> <p>Amazon EKS does not come pre-equipped with the Vertical Pod Autoscaler (VPA). To leverage VPA's capabilities for optimizing pod resource allocations, we need to deploy it on our EKS cluster.</p> <p>Note</p> <p>You must have a metrics source for example <code>metrics-server</code> installed in your kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/introduction-to-vertical-pod-autoscaler/#components-of-vertical-pod-autoscaler-vpa","title":"Components of Vertical Pod Autoscaler (VPA)","text":"<p>The VPA consists of 3 components:</p> <ol> <li> <p>Recommender: it monitors the current and past resource consumption and, based on it, provides recommended values for the containers' cpu and memory requests.</p> </li> <li> <p>Updater: it checks which of the managed pods have correct resources set and, if not, kills them so that they can be recreated by their controllers with the updated requests.</p> </li> <li> <p>Admission Controller: it sets the correct resource requests on new pods (either just created or recreated by their controller due to Updater's activity).</p> </li> </ol>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/introduction-to-vertical-pod-autoscaler/#how-does-vpa-work","title":"How Does VPA Work?","text":"<p>Here's how Vertical Pod Autoscaler (VPA) works in kubernetes:</p> <p> </p> <ol> <li>VPA recommender uses metrics server to continuously monitor the current and past resource consumption of pods.</li> <li>VPA recommender reads the <code>requests</code> and <code>limits</code> value of pods from VPA definition.</li> <li>VPA recommender calculates and provides recommended values for the containers' <code>cpu</code> and <code>memory</code> requests.</li> <li>VPA admission controller gets the resource recommendation for pods</li> <li>VPA admission controller sets the correct resource <code>requests</code> on new pods. For example, add <code>100m</code> CPU.</li> <li>VPA updater checks which of the managed pods have correct resources set.</li> <li>VPA updater kills the pods with incorrect resources set.</li> <li>Deployment recreates the pod to match the defined replicas with correct resource <code>requests</code>.</li> </ol>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/introduction-to-vertical-pod-autoscaler/#vpa-modes","title":"VPA Modes","text":"<p>There are four modes in which VPAs operate:</p> <ol> <li> <p>Auto: VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently, this is equivalent to \"Recreate\" (see below). Once restart free (\"in-place\") update of pod requests is available, it may be used as the preferred update mechanism by the \"Auto\" mode.</p> </li> <li> <p>Recreate: VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendation (respecting the Pod Disruption Budget, if defined). This mode should be used rarely, only if you need to ensure that the pods are restarted whenever the resource request changes. Otherwise, prefer the \"Auto\" mode which may take advantage of restart-free updates once they are available.</p> </li> <li> <p>Initial: VPA only assigns resource requests on pod creation and never changes them later.</p> </li> <li> <p>Off: VPA does not automatically change the resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.</p> </li> </ol>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/introduction-to-vertical-pod-autoscaler/#known-limitations-of-vpa","title":"Known Limitations of VPA","text":"<ol> <li>Whenever VPA updates the pod resources, the pod is recreated, which causes all running containers to be recreated. The pod may be recreated on a different node.</li> <li>Vertical Pod Autoscaler should not be used with the Horizontal Pod Autoscaler (HPA) on CPU or memory at this moment.</li> <li>VPA performance has not been tested in large clusters.</li> <li>VPA cannot guarantee that pods it evicts or deletes to apply recommendations (when configured in Auto and Recreate modes) will be successfully recreated. This can be partly addressed by using VPA together with Cluster Autoscaler.</li> <li>VPA recommendation might exceed available resources (e.g. Node size, available size, available quota) and cause pods to go pending. This can be partly addressed by using VPA together with Cluster Autoscaler.</li> <li>Multiple VPA resources matching the same pod have undefined behavior.</li> </ol> <p>References:</p> <ul> <li>Vertical Pod Autoscaler</li> <li>VPA GitHub Repo</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/","title":"Vertical Pod Autoscaler (VPA) Demo","text":"<p>Now that you have an understanding of how VPA works, let's see it in action.</p>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#objective","title":"Objective","text":"<p>We'll follow these steps to test the Vertical Pod Autoscaler (VPA):</p> <ol> <li>We'll create a <code>Deployment</code> and a <code>Service</code> object.</li> <li>We'll create a <code>VerticalPodAutoscaler</code> object for the deployment.</li> <li>We'll generate load on pods managed by the deployment.</li> <li>We'll observe VPA taking autoscaling actions to meet the increased demand.</li> </ol> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        resources:\n          requests:\n            cpu: \"10m\"\n            memory: \"10Mi\"\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> <p>Note that each pod can requests a minimum of <code>10m</code> CPU and <code>10Mi</code> memory.</p>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#step-3-create-vpa-for-the-deployment","title":"Step 3: Create VPA for the Deployment","text":"<p>Now, let's create a VPA for the deployment as follows:</p> <code>my-vpa.yml</code> <pre><code>apiVersion: \"autoscaling.k8s.io/v1\"\nkind: VerticalPodAutoscaler\nmetadata:\n  name: my-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: my-deployment\n  updatePolicy:\n    updateMode: \"Auto\"\n    minReplicas: 2 # Minimal number of replicas which need to be alive for Updater to attempt Pod eviction\n  resourcePolicy:\n    containerPolicies:\n      - containerName: '*' # The name of the container that the policy applies to.\n        minAllowed:\n          cpu: 10m\n          memory: 10Mi\n        maxAllowed:\n          cpu: 500m\n          memory: 500Mi\n        controlledResources: [\"cpu\", \"memory\"]\n</code></pre> <p>The <code>minAllowed</code> field prevents the VPA from recommending or setting resource requests below the specified minimum threshold, ensuring requests won't drop below that limit.</p> <p>The <code>maxAllowed</code> field prevents the VPA from recommending or setting resource requests above the specified maximum threshold, ensuring requests won't exceed that limit.</p> <p>Apply the manifest to create the VPA:</p> <pre><code>kubectl apply -f my-vpa.yml\n</code></pre> <p>Verify VPA:</p> <pre><code># List vpa\nkubectl get vpa\n\n# Describe vpa to view the events\nkubectl descripe vpa my-vpa\n</code></pre> <p>Warning</p> <p>The <code>.spec.replicas</code> value in deployment must be greater than or equal to <code>.spec.updatePolicy.minReplicas</code> value in VPA for the VPA to manage the pods for autoscaling.</p>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#step-4-generate-load","title":"Step 4: Generate Load","text":"<p>Let's generate load on the pods managed by the deployment. On your local machine run the following command to generate the load:</p> <pre><code>while sleep 1; do seq 1000 | xargs -P100 -I{} curl -s &lt;load-balancer-dns&gt; &gt; /dev/null; done\n</code></pre> <p>The above command concurrently sends 1000 requests per second to the LoadBalancer service using 100 parallel processes.</p>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#step-5-monitor-pods-and-vpa-events","title":"Step 5: Monitor Pods and VPA Events","text":"<pre><code># List pods in watch mode\nkubectl get pods -w\n\n# List vpa in watch mode\nkubectl get vpa -w\n\n# View vpa events\nkubectl describe vpa my-vpa\n</code></pre> <p>You'll notice that vpa recommender recommends a new value for resource requests and then vpa updater updates the resource requests of pods.</p> <p>Note</p> <p>The updater may take some time to apply the recommendation and you might have to wait before you can see the updated resource <code>requests</code> in pods. </p> <p>You can view the VPA logs as follows:</p> <pre><code># view logs from admission controller\nkubectl get pods -n kube-system | grep vpa-admission | kubectl logs -f `awk '{print $1}'` -n kube-system\n\n# view logs from recommender\nkubectl get pods -n kube-system | grep vpa-recommender | kubectl logs -f `awk '{print $1}'` -n kube-system\n\n# view logs from updater\nkubectl get pods -n kube-system | grep vpa-updater | kubectl logs -f `awk '{print $1}'` -n kube-system\n</code></pre>"},{"location":"kubernetes-on-eks/autoscaling/vertical-pod-autoscaling/vertical-pod-autoscaler-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-vpa.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/ci-cd-for-helm/ci-cd-for-helm-using-aws-codepipeline/","title":"CI CD for Helm Using AWS CodePipeline","text":"<p>Let's update our git repository to implement CI/CD for helm.</p>"},{"location":"kubernetes-on-eks/ci-cd-for-helm/ci-cd-for-helm-using-aws-codepipeline/#initialize-the-chart","title":"Initialize the Chart","text":"<p>First, let's prepare helm template for our application:</p> <pre><code>helm create helm-chart\n</code></pre> <p>This will create a folder called <code>helm-chart</code> in the repository.</p> <p>Remove everything from <code>templates/</code> folder and delete everything except <code>Chart.yaml</code>.</p> <p>In the <code>Chart.yaml</code> change the <code>name</code> field to <code>node-app</code> or anything you prefer.</p> <code>Chart.yaml</code> <pre><code>apiVersion: v2\nname: node-app\ndescription: A Helm chart for Kubernetes\n\n# A chart can be either an 'application' or a 'library' chart.\n#\n# Application charts are a collection of templates that can be packaged into versioned archives\n# to be deployed.\n#\n# Library charts provide useful utilities or functions for the chart developer. They're included as\n# a dependency of application charts to inject those utilities and functions into the rendering\n# pipeline. Library charts do not define any templates and therefore cannot be deployed.\ntype: application\n\n# This is the chart version. This version number should be incremented each time you make changes\n# to the chart and its templates, including the app version.\n# Versions are expected to follow Semantic Versioning (https://semver.org/)\nversion: 0.1.0\n\n# This is the version number of the application being deployed. This version number should be\n# incremented each time you make changes to the application. Versions are not expected to\n# follow Semantic Versioning. They should reflect the version the application is using.\n# It is recommended to use it with quotes.\nappVersion: \"1.16.0\"\n</code></pre> <p>Add <code>deployment.yaml</code> and <code>service.yml</code> templates in the <code>template/</code> directory.</p> <code>deployment.yaml</code> <code>service.yaml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.appName }}\nspec:\n  replicas: {{ .Values.replicas }}\n  selector:\n    matchLabels:\n      app: {{ .Values.appName }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.appName }}\n    spec:\n      containers:\n      - name: {{ .Values.appName }}\n        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}\n        ports:\n          - containerPort: {{ .Values.containerPort }}\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.appName }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app: {{ .Values.appName }}\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.targetPort }}\n</code></pre> <p>Now, add <code>values.yam</code>l, <code>values.dev.yaml</code>, and <code>values.prod.yaml</code>.</p> <code>values.yaml</code> <code>values.dev.yaml</code> <code>values.prod.yaml</code> <pre><code>appName: myapp\nreplicas: 1\n\nimage:\n  repository: IMAGE_REPOSITORY\n  tag: IMAGE_TAG\n  imagePullPolicy: Always\n\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 5000\n\ncontainerPort: 5000\n</code></pre> <pre><code>appName: myapp\nreplicas: 1\n\nimage:\n  repository: IMAGE_REPOSITORY\n  tag: IMAGE_TAG\n  imagePullPolicy: Always\n\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 5000\n\ncontainerPort: 5000\n</code></pre> <pre><code>appName: myapp\nreplicas: 2\n\nimage:\n  repository: IMAGE_REPOSITORY\n  tag: IMAGE_TAG\n  imagePullPolicy: Always\n\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 5000\n\ncontainerPort: 5000\n</code></pre>"},{"location":"kubernetes-on-eks/ci-cd-for-helm/ci-cd-for-helm-using-aws-codepipeline/#update-buildspec","title":"Update Buildspec","text":"<p>Update the <code>buidspec.yml</code> to deploy using helm.</p> <code>buidspec.yml</code> <pre><code>version: 0.2\nrun-as: root\n\nphases:\n\n  install:\n    commands:\n      # Install helm\n      - echo Installing helm...\n      - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n      - echo Verifying helm installation...\n      - helm version --short\n\n  pre_build:\n    commands:\n      # Verify if kubectl is installed\n      - echo Checking if kubectl is installed...\n      - kubectl version --client\n      # Verify if aws-cli is installed\n      - echo Checking if aws-cli is installed...\n      - aws --version\n      # Login to ECR\n      - echo Logging in to Amazon ECR...\n      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)\n      # GitHub commit hash to tag the image\n      - COMMIT_HASH=$CODEBUILD_RESOLVED_SOURCE_VERSION\n      # Additional latest tag\n      - IMAGE_TAG='latest'\n\n  build:\n    commands:\n      # Build the Docker image and add an additional latest tag\n      - echo Building the Docker image...     \n      - docker build -t $REPOSITORY_URI:$COMMIT_HASH .\n      - docker tag $REPOSITORY_URI:$COMMIT_HASH $REPOSITORY_URI:$IMAGE_TAG\n\n  post_build:\n    commands:\n      # Push the Docker images to ECR\n      - echo Pushing the Docker image with commit hash as tag...\n      - docker push $REPOSITORY_URI:$COMMIT_HASH\n      - echo Pushing the Docker image with the latest image tag...\n      - docker push $REPOSITORY_URI:$IMAGE_TAG\n      - echo Pushed images to ECR.\n      # Update kube config. This requires the codebuild role to have eks:DescribeCluster permission\n      - aws eks update-kubeconfig --name $CLUSTER_NAME\n      # Replace the IMAGE_REPOSITORY placeholder with ECR Repository URI\n      - sed -i \"s|IMAGE_REPOSITORY|$REPOSITORY_URI|g\" helm-chart/values.$ENV.yaml\n      # Replace the IMAGE_TAG placeholder with commit hash\n      - sed -i \"s|IMAGE_TAG|$COMMIT_HASH|g\" helm-chart/values.$ENV.yaml\n      # Create namespace if it doesn't exist\n      - kubectl get ns $NAMESPACE || kubectl create ns $NAMESPACE\n      # Upgrade chart. If a release by this name doesn't already exist, run an install.\n      - echo Upgrading chart...\n      - helm upgrade --install node-app helm-chart/ --values helm-chart/values.$ENV.yaml --namespace $NAMESPACE\n</code></pre> <p>In the Build Project, add <code>NAMESPACE</code> and <code>ENV</code> environment variables.</p> <p>At this point your git repository folder structure should look like the following:</p> <pre><code>|-- nodeapp\n\u2502   |-- helm-chart\n|   |   |-- templates\n|   |   |   |-- deployment.yaml\n|   |   |   |-- service.yaml\n\u2502   |-- k8s-manifests\n|   |   |-- 00-namespace.yml\n|   |   |-- deployment.yml\n|   |   |-- service.yml\n\u2502   |-- .dockerignore\n\u2502   |-- .gitignore\n\u2502   |-- Dockerfile\n\u2502   |-- buildspec-k8s-manifest.yml\n\u2502   |-- buildspec.yml\n\u2502   |-- package-lock.json\n\u2502   |-- package.json\n\u2502   |-- server.js\n</code></pre> <p>Note that we have renamed the previous <code>buildspec.yml</code> to <code>buildspec-k8s-manifest.yml</code> and the updated <code>buildspec.yml</code> to work with helm.</p> <p>Push changes to github and verify the deployment.</p>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/","title":"CI CD for EKS Using AWS CodePipeline","text":"<p>Now, let's see how you can set up CI/CD for a containerized application on EKS using AWS CodePipeline.</p>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-1-create-a-private-github-repository","title":"Step 1: Create a Private GitHub Repository","text":"<p>In the first step let's create a private github repository. Let's call it <code>node-app</code>. You can name it anything you prefer.</p> <p>Make sure to choose <code>gitignore</code> for Node.js application. This will create a <code>.gitignore</code> file for Node.js application.</p>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-2-clone-the-github-repository","title":"Step 2: Clone the GitHub Repository","text":"<p>Now, let's clone the repository on local machine so that we can make changes and push it to the repo.</p> <pre><code>git clone &lt;github-repository-url&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-3-test-the-application-locally","title":"Step 3: Test the Application Locally","text":"<p>This is a simple Node.js app using express. Make sure you have <code>Node.js</code> and <code>npm</code> installed on your local machine so that you can test it locally.</p> <pre><code># Verify if node is installed\nnode --version\n\n# Verify if npm is installed\nnpm --version\n</code></pre> <code>server.js</code> <code>package.json</code> <pre><code>const express = require('express')\nconst app = express()\nvar os = require('os');\n\nconst PORT = process.env.PORT || 5000;\n\napp.get('/', function (_req, res) {\n  let host = os.hostname();\n  data = {\n    Host: host,\n    Version: \"v1\"\n  }\n  res.status(200).json(data);\n});\n\napp.get('/health', function(_req, res) {\n  res.status(200).send('Healthy');\n});\n\napp.listen(PORT, () =&gt; {\n  console.log(`Server is running on port ${PORT}`);\n});\n</code></pre> <pre><code>{\n  \"name\": \"nodeapp\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"server.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"express\": \"^4.18.2\"\n  }\n}\n</code></pre> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- nodeapp\n\u2502   |-- .gitignore\n\u2502   |-- package.json\n\u2502   |-- server.js\n</code></pre> <p>Run the following command to install the npm packages:</p> <pre><code>npm install\n</code></pre> <p>Run the following command to test the app:</p> <pre><code>node server.js\n</code></pre> <p>Now, open another terminal and call localhost on port 5000.</p> <pre><code># Call root endpoint\ncurl localhost:5000\n\n## Call health endpoint\ncurl localhost:5000/health\n</code></pre> <p>Or, visit these URLs in any browser.</p>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-4-containerize-the-app-and-test-it-locally","title":"Step 4: Containerize the App and Test it Locally","text":"<p>Let's create a <code>Dockerfile</code> and create a Docker image out of it. Also, add a <code>.dockerignore</code> file.</p> <p>Note</p> <p>A <code>.dockerignore</code> is a configuration file that describes files and directories that you want to exclude when building a Docker image.</p> <code>Dockerfile</code> <code>.dockerignore</code> <pre><code>FROM node:18\n\n# Create app directory\nWORKDIR /usr/src/app\n\n# Copy package.json and package-lock.json\nCOPY package*.json ./\n\n# Install packages mentioned package.json\nRUN npm install\n\n# Bundle app source\nCOPY . .\n\nEXPOSE 5000\nCMD [ \"node\", \"server.js\" ]\n</code></pre> <pre><code># Dependency directories\nnode_modules/\njspm_packages/\n\n# Environment variable files\nconfig.env\n.env\n.env.development.local\n.env.test.local\n.env.production.local\n.env.local\n\n# Logs\nlogs\n*.log\n</code></pre> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- nodeapp\n\u2502   |-- Dockerfile\n\u2502   |-- .dockerignore\n\u2502   |-- .gitignore\n\u2502   |-- server.js\n</code></pre> <p>Run the following command to build the docker image:</p> <pre><code>docker build -t my-node-app .\n</code></pre> <p>Now, run a container from the docker image:</p> <pre><code>docker run --name my-node-container -p 5000:5000 my-node-app\n</code></pre> <p>Verify the application endpoints:</p> <pre><code>curl localhost:5000\ncurl localhost:5000/health\n</code></pre> <p>Stop and delete the container:</p> <pre><code># list containers\ndocker ps -a | grep my-node-container\n\n# stop the container\ndocker stop my-node-container\n\n# Delete the container\ndocker rm my-node-container\n</code></pre>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-5-create-amazon-ecr-repository","title":"Step 5: Create Amazon ECR Repository","text":"<p>Create ECR repository to which we will later push the <code>my-node-app:latest</code> image that we built.</p> <pre><code>aws ecr create-repository \\\n    --repository-name my-node-app \\\n    --image-scanning-configuration scanOnPush=true\n</code></pre>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-6-create-kubernetes-manifest-files","title":"Step 6: Create Kubernetes Manifest Files","text":"<p>Let's prepare kubernetes manifest files for our application.</p> <code>00-namespace.yml</code> <code>deployment.yml</code> <code>service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: nodeapp\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nodeapp\n  namespace: nodeapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: CONTAINER_IMAGE\n        ports:\n          - containerPort: 5000\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeapp-service\n  namespace: nodeapp\nspec:\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Note that <code>CONTAINER_IMAGE</code> is a placeholder that will be replace with actual ECR image URI during deployment.</p>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-7-create-buildspec-for-aws-codebuild","title":"Step 7: Create Buildspec for AWS CodeBuild","text":"<p>A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build.</p> <code>buildspec.yml</code> <pre><code>version: 0.2\nrun-as: root\n\nphases:\n\n  install:\n    commands:\n      - echo Nothing to Install.\n      - echo AWS CodeBuild image already has the required tools we need.\n      - echo We will verify it in the next stage.\n\n  pre_build:\n    commands:\n      # Check if kubectl is installed\n      - echo Checking if kubectl is installed...\n      - kubectl version --client\n      # Check if aws-cli is installed\n      - echo Checking if aws-cli is installed...\n      - aws --version\n      # Login to ECR\n      - echo Logging in to Amazon ECR...\n      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)\n      # GitHub commit hash to tag the image\n      - COMMIT_HASH=$CODEBUILD_RESOLVED_SOURCE_VERSION\n      # Additional latest tag\n      - IMAGE_TAG='latest'\n\n  build:\n    commands:\n      # Build the Docker image and add an additional latest tag\n      - echo Building the Docker image...     \n      - docker build -t $REPOSITORY_URI:$COMMIT_HASH .\n      - docker tag $REPOSITORY_URI:$COMMIT_HASH $REPOSITORY_URI:$IMAGE_TAG\n\n  post_build:\n    commands:\n      # Push the Docker images to ECR\n      - echo Pushing the Docker image with commit hash as tag...\n      - docker push $REPOSITORY_URI:$COMMIT_HASH\n      - echo Pushing the Docker image with the latest image tag...\n      - docker push $REPOSITORY_URI:$IMAGE_TAG\n      - echo Pushed images to ECR.\n      # Update kube config. This requires the codebuild role to have eks:DescribeCluster permission\n      - aws eks update-kubeconfig --name $CLUSTER_NAME\n      # Replace the CONTAINER_IMAGE placeholder with actual image URI\n      - sed -i \"s|CONTAINER_IMAGE|$REPOSITORY_URI:$COMMIT_HASH|g\" k8s-manifests/deployment.yml\n      # Apply any manifest changes in k8s-manifest folder. \n      - echo Applying kubernetes manifest changes...\n      - kubectl apply -f k8s-manifests\n</code></pre> <p>The script performs the following actions:</p> <ol> <li>Verifies whether the necessary dependencies, such as kubectl and aws-cli, are installed.</li> <li>Logs into Amazon ECR.</li> <li>Builds the Docker image.</li> <li>Pushes the image to ECR.</li> <li>Updates the deployment manifest by substituting the CONTAINER_IMAGE placeholder with the newly pushed image on ECR.</li> <li>Deploys the kubernetes manifests to EKS.</li> </ol> <p>At this point your git repository folder structure should look like the following:</p> <pre><code>|-- nodeapp\n\u2502   |-- k8s-manifests\n|   |   |-- 00-namespace.yml\n|   |   |-- deployment.yml\n|   |   |-- service.yml\n\u2502   |-- .dockerignore\n\u2502   |-- .gitignore\n\u2502   |-- Dockerfile\n\u2502   |-- buildspec.yml\n\u2502   |-- package-lock.json\n\u2502   |-- package.json\n\u2502   |-- server.js\n</code></pre>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-8-create-aws-codebuild-project","title":"Step 8: Create AWS CodeBuild Project","text":"<p>AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.</p> <p>Before we proceed, let's create an IAM role for CodeBuild project. It is important that you create role first otherwise a role will be created in <code>service-role</code> namespace and AWS EKS doesn't work with that role. This is a bug.</p> <p>Now, follow the instruction below to create a CodeBuild project:</p> <ol> <li>Go to AWS CodeBuild console</li> <li>Click on <code>Build projects</code> in the left navigation panel.</li> <li>Click on <code>Create build project</code> button on the top right corner.</li> <li>In the <code>Project configuration</code> section, provide the <code>Project name</code>. We'll name it <code>node-app</code>.</li> <li>In the <code>Source</code> section select <code>GitHub</code> as source provider.</li> <li>For Repository select <code>Repository in my GitHub account</code>.</li> <li>Select <code>Connect using OAuth</code> and click on <code>Connect to GitHub</code>.</li> <li>A new window will open for authorization.</li> <li>Click on <code>Authorize aws-codesuite</code> after you have selected the required permissions.</li> <li>In the <code>Source version</code> provide branch name. It should be <code>master</code> in our case.</li> <li>In the <code>Environment</code> section, select <code>Managed Image</code>.</li> <li>Select <code>Amazon Linux 2</code> as the <code>Operating system</code>.</li> <li>In the <code>Runtime</code> select <code>standard</code>.</li> <li>For <code>Image</code> select the latest version. <code>7.0</code> as of today.</li> <li>For <code>Image version</code> select <code>Always use the latest image for this runtime version</code>.</li> <li>Check the <code>Privileged</code> checkbox.</li> <li>In the <code>Service role</code> select <code>existing</code>.</li> <li>In the <code>Buildspec</code> section, select Use a <code>buildspec file</code>.</li> <li>In the <code>Buildspec name</code> provide the name of the buildspec file. It is <code>buildspec.yml</code> in our case.</li> <li>In the <code>Logs</code> section, check the <code>CloudWatch logs</code>.</li> <li>Leave the <code>Group name</code> empty. (Auto create).</li> <li>Leave the <code>Stream name</code> empty. (Auto create).</li> </ol> <p>We need to assign the required permissions to the role attached to the CodeBuild project. </p> <p>Open the service role attached to the codebuild project and assign the following permissions:</p> <ol> <li><code>AmazonEC2ContainerRegistryPowerUser</code>: Allows codebuild project to push image to the ECR repository.</li> <li>Add the following inline policy. This allows codebuild project to get required EKS permissions.</li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"eks:DescribeCluster\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>Note</p> <p>It is recommended to use granular permissions but for simplicity we are avoiding that.</p> <p>Also, edit the CodeBuild project and add the following environent variables:</p> <ol> <li><code>REPOSITORY_URI</code>: ECR repository URI that we created</li> <li><code>CLUSTER_NAME</code>: EKS cluster name</li> </ol> <p>Now, update the <code>aws-auth</code> ConfigMap to allow CodeBuild to apply k8s manifest files:</p> <pre><code># View the current ConfigMap\nkubectl get configmap aws-auth -n kube-system -o yaml\n\n# Edit the aws-auth ConfigMap\nexport KUBE_EDITOR=nano\nkubectl edit configmap aws-auth -n kube-system\n</code></pre> <p>Update the <code>aws-auth</code> ConfigMap by adding the following item in the <code>mapRoles</code> list as shown below:</p> <pre><code>mapRoles: |\n   - groups:\n     - system:masters\n     rolearn: &lt;codebuild-service-role-arn&gt;\n     username: &lt;codebuild-project-service-role-name&gt;\n</code></pre> <p>Now, trigger the CodeBuild project manually to test if deployment is working as expected.</p>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-9-create-aws-codepipeline","title":"Step 9: Create AWS CodePipeline","text":"<p>Create an AWS CodePipeline to trigger the build whenever a change is pushed to the repository in the <code>master</code> branch.</p> <p>Follow the instruction below to create the CodePipeline:</p> <ol> <li>Go to AWS CodePipeline console.</li> <li>Click on <code>Create pipeline</code>.</li> <li>Provide <code>Pipeline name</code>.</li> <li>In <code>Service role</code>, select <code>New service role</code>.</li> <li>Leave everything else as default.</li> <li>Click on <code>Next</code>.</li> <li>In Source provider, select <code>GitHub (Version 2)</code>.</li> <li>In <code>Connection</code>, click on <code>Connect to GitHub</code>.</li> <li>A new window will open to create a new connection.</li> <li>Provide connection name and authorize.</li> <li>In <code>GitHub Apps</code> click on <code>Install a new App</code>.</li> <li>Authorize and install app.</li> <li>In <code>Repository name</code> select the required repository.</li> <li>In <code>Branch name</code> select <code>master</code>.</li> <li>Check the box that says <code>Start the pipeline on source code change</code>.</li> <li>Leave everything else as default</li> <li>Click on <code>Next</code>.</li> <li>In <code>Build</code>, select <code>AWS CodeBuild</code> as <code>Build provider</code>.</li> <li>Select <code>Region</code>.</li> <li>Select <code>Project name</code>.</li> <li>Leave everything else as default.</li> <li>Click on <code>Next</code>.</li> <li>Skip deploy stage.</li> <li>Click on <code>Create pipeline</code>.</li> </ol>"},{"location":"kubernetes-on-eks/ci-cd-with-eks/ci-cd-for-eks-using-aws-codepipeline/#step-10-verify-the-working-of-cicd-pipeline","title":"Step 10: Verify the Working of CI/CD Pipeline","text":"<p>Make some changes to your repository and verify that the pipeline gets triggered automatically. </p> <p>Change the version of the app to <code>v2</code> and verify if the changes are reflected in EKS cluster.</p> <p>References:</p> <ul> <li>CodeBuild Service Role Issue</li> </ul>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/","title":"Create EKS Cluster eksctl","text":""},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Make sure you have <code>AWS CLI</code>, <code>kubectl</code>, and <code>eksctl</code> installed.</li> <li>Make sure you have configured the <code>AWS CLI</code>.</li> <li>Make sure the IAM user has Administrator access.</li> </ul> <p>Warning</p> <p>It's important to note that while we've simplified the process by granting Administrator access for convenience, it's generally recommended to provide more granular and specific permissions to IAM users based on their actual needs. This helps reduce potential security risks and ensures a more controlled and secure environment.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#methods-of-using-eksctl","title":"Methods of Using eksctl","text":"<p>There are two methods of using eksctl:</p> <ol> <li> <p>Imperative method: </p> <p>This method involves running <code>eksctl</code> commands directly in the terminal to create, modify, or delete EKS resources. For example, you can use the following command to create a new EKS cluster:</p> <pre><code>eksctl create cluster --name my-cluster --region ap-south-1\n</code></pre> <p>The above command creates a new EKS cluster with the name <code>my-cluster</code> in the <code>ap-south-1</code> region.</p> </li> <li> <p>Declarative method:</p> <p>This method involves defining a configuration file in YAML format that specifies the desired state of the EKS cluster and using <code>eksctl</code> to apply that configuration. </p> <p>For example, you can define a configuration file called <code>cluster.yaml</code> that creates an EKS cluster with two worker nodes as follows:</p> <code>cluster.yml</code> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: my-cluster\n  region: ap-south-1\n\nnodeGroups:\n  - name: my-nodegroup\n    instanceType: t2.small\n    desiredCapacity: 2\n</code></pre> <p>You can then apply this configuration by running the following command:</p> <pre><code>eksctl create cluster -f cluster.yaml\n</code></pre> <p>The above command creates a new EKS cluster with the name <code>my-cluster</code> in the <code>ap-south-1</code> region, and a worker node group with two <code>t2.small</code> instances.</p> <p>We will be using the declarative method in this course as it is much easier to specify complex parameters using configuration files, compared to the tedious task of specifying the same parameters directly through the command line.</p> </li> </ol>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#amazon-eks-pricing","title":"Amazon EKS Pricing","text":"<p>Heads up! If you're following along with this course on Amazon EKS, be aware that you will incur costs for the EKS infrastructure.</p> <p>Here are some tips for understanding EKS pricing:</p> <ul> <li>EKS cluster management is charged at $0.10 per hour per cluster, regardless of the number of nodes in the cluster.</li> <li>EC2 instances are used as worker nodes in an EKS cluster, and you will be charged for the hours these instances are running.</li> <li>EKS also uses other AWS services like load balancers, storage, and networking, which will incur additional costs.</li> <li>Be sure to monitor your AWS billing dashboard to avoid unexpected costs and set up billing alerts to stay informed about your usage.</li> </ul>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#step-1-create-key-pair-optional","title":"Step 1: Create Key Pair (Optional)","text":"<p>This step is required only if you want to enable <code>SSH</code> for worker nodes in the EKS cluster.</p> <ol> <li>Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/</li> <li>In the navigation pane, under Network &amp; Security, choose Key Pairs.</li> <li>Choose Create key pair</li> <li>Enter the desired name for the key pair (For example, <code>my-eks-key</code>)</li> <li>For Key pair type, choose <code>RSA</code></li> <li>Add tags if you want</li> <li>Click on Create key pair</li> </ol> <p>The private key will be downloaded. Keep it safely.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#step-2-prepare-configuration-file","title":"Step 2: Prepare Configuration File","text":"<p>Prepare configuration file in <code>YAML</code> format to specify the desired state of the EKS cluster.</p> <code>cluster.yml</code> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: my-cluster\n  region: ap-south-1\n  version: \"1.28\"\n\navailabilityZones:\n  - ap-south-1a\n  - ap-south-1b\n  - ap-south-1c\n\niam:\n  withOIDC: true\n\nmanagedNodeGroups:\n  - name: public-nodegroup\n    privateNetworking: false\n    instanceType: t3.medium\n    minSize: 2\n    maxSize: 2\n    volumeSize: 20\n    ssh:\n      allow: true\n      publicKeyName: my-eks-key\n    iam:\n      withAddonPolicies:\n        imageBuilder: true\n        autoScaler: true\n        externalDNS: true\n        certManager: true\n        appMesh: true\n        appMeshPreview: true\n        ebs: true\n        fsx: true\n        efs: true\n        awsLoadBalancerController: true\n        xRay: true\n        cloudWatch: true\n</code></pre> <p>Make sure to make the following changes in the above <code>cluster.yml</code> configuration file:</p> <ol> <li>Replace the value of <code>publicKeyName</code> with the name of the key pair you created in <code>Step 1</code>.</li> <li>Remove the <code>ssh</code> parameter if you don't want to enable <code>SSH</code> for worker nodes.</li> <li>Replace the <code>region</code> and <code>availabilityZones</code> fields if you are working in a different region.</li> </ol> <p>The provided configuration, when applied, will create an Amazon Elastic Kubernetes Service (EKS) cluster in the <code>ap-south-1</code> region with the name <code>my-cluster</code>. This cluster will span multiple availability zones: <code>ap-south-1a</code>, <code>ap-south-1b</code>, and <code>ap-south-1c</code>. It will also enable IAM (Identity and Access Management) with OIDC (OpenID Connect) for authentication.</p> <p>Additionally, the configuration specifies a managed node group named <code>public-nodegroup</code> with the following settings: instance type <code>t3.medium</code>, a fixed group size of 2 instances, each with a 20GB volume. <code>SSH</code> access is allowed, and it uses the specified public key <code>my-eks-key</code> for authentication. </p> <p>The IAM role for this node group is configured with various addon policies enabled, including features like image builder, auto-scaling, external DNS, certificate manager, App Mesh, App Mesh Preview, EBS (Elastic Block Store), FSx (Amazon FSx for Lustre), EFS (Elastic File System), AWS Load Balancer Controller, X-Ray, and CloudWatch.</p> <p>Note</p> <ul> <li> <p>The above cofiguration will create a dedicated VPC for the cluster but you can also configure it to use an existing VPC.</p> </li> <li> <p>You may choose to skip creating node groups and IAM OIDC provider at this stage and create it later. But we are creating them at this stage to have everything set up so that we can start using the Kubernetes cluster without additional effort.</p> </li> </ul>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#step-3-set-the-aws_profile-environment-variable","title":"Step 3: Set the AWS_PROFILE Environment Variable","text":"<p>Set the <code>AWS_PROFILE</code> environment variable if you are using a named aws profile as follows:</p> <pre><code># Command template\nexport AWS_PROFILE=&lt;my-aws-profile&gt;\n\n# Actual command\nexport AWS_PROFILE=eks-profile\n</code></pre> <p>Verify that the AWS CLI is configured properly.</p> <pre><code># Verify profile\naws configure list\n</code></pre>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#step-4-create-cluster","title":"Step 4: Create Cluster","text":"<p>Use <code>eksctl</code> to apply the <code>cluster.yml</code> configuration and create cluster.</p> <pre><code># Command template to create EKS cluster\neksctl create cluster -f &lt;file-name&gt;\n\n# Actual command\neksctl create cluster -f cluster.yml\n</code></pre> <p>This will create the EKS cluster and also update the kube config for the cluster in <code>~/.kube/config</code>.</p> <p>Kubeconfig is a configuration file used by Kubernetes to manage access to Kubernetes clusters. It is used by the Kubernetes command-line tool, <code>kubectl</code>, to connect to a Kubernetes cluster and perform administrative tasks.</p> <p>The <code>kubeconfig</code> file contains information about one or more Kubernetes clusters, the authentication credentials to access them, and the context of a user and namespace.</p> <p>In Kubernetes, a context is a way to set and switch between different Kubernetes clusters, users, and namespaces. Each context contains the configuration information needed to communicate with a specific Kubernetes cluster.</p> <p>View the kube config file:</p> <pre><code>cat ~/.kube/config\n</code></pre> <p>The <code>kubeconfig</code> file will be valid for 24 hours by default.</p> <p>You can update the kube config using the following command if needed:</p> <pre><code># Command template\naws eks update-kubeconfig --name &lt;eks-cluster-name&gt; --region &lt;region-name&gt;\n\n# Actual command\naws eks update-kubeconfig --name my-cluster --region ap-south-1\n</code></pre> <p>This will update the kube config and will be valid for 24 hours.</p> <p>View the name of the current context that <code>kubectl</code> is using:</p> <pre><code># Display the current context\nkubectl config current-context\n</code></pre>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#step-5-verify-eks-cluster-in-the-aws-console","title":"Step 5: Verify EKS Cluster in the AWS Console","text":"<p>Login to AWS console and verify that the cluster was created.</p> <p>Warning</p> <p>Make sure you are logged in with the same IAM user that you used to create the cluster or else you won't be able to see some of the details.</p> <p>You can also use <code>eksctl</code> or <code>AWS CLI</code> to list clusters as follows:</p> <pre><code># Use eksctl to list clusters\neksctl get cluster\n\n{OR}\n\n# Use AWS CLI to list clusters\naws eks list-clusters\n</code></pre>"},{"location":"kubernetes-on-eks/create-eks-cluster/create-eks-cluster/#step-6-verify-cloudformation-stack","title":"Step 6: Verify CloudFormation Stack","text":"<p>When you use <code>eksctl</code> to create an Amazon EKS cluster, it sets up a CloudFormation stack set, which in turn is used to create required AWS resources such as the EKS cluster, VPC, IAM roles, EC2 instances, etc.</p> <p>Go to AWS Console and verify the stack set created by <code>eksctl</code>.</p> <p>References:</p> <ul> <li>Getting started with eksctl</li> <li>Creating and managing clusters using eksctl</li> <li>Amazon EKS Pricing</li> </ul>"},{"location":"kubernetes-on-eks/create-eks-cluster/delete-eks-cluster/","title":"Delete EKS Cluster","text":"<p>We won't delete the cluster we created since we will be using it for further demos and tutorials. But if you need to delete the cluster, you can follow the instructions below.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/delete-eks-cluster/#delete-cluster","title":"Delete Cluster","text":"<p>You can delete the EKS cluster using the following commands:</p> <pre><code># Command template\neksctl delete cluster -f &lt;cluster-config-file&gt;\n{OR}\neksctl delete cluster --name &lt;cluster-name&gt; --region &lt;region_name&gt;\n\n# Actual command\neksctl delete cluster -f cluster.yml\n{OR}\neksctl delete cluster --name my-cluster --region ap-south-1\n</code></pre> <p>Note</p> <p>To delete an EKS cluster, only two parameters are required <code>name</code> and <code>region</code>. If you don't provide the <code>region</code> parameter, it will default to the region you set while configuring AWS CLI.</p> <p>When you use the configuration file to delete the cluster, <code>eksctl</code> basically extracts the <code>name</code> and <code>region</code> parameters of the cluster from configuration file and then uses them to delete the cluster.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/delete-eks-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>If for reason you get an error that says <code>Error: failed to delete all resources</code>, go to AWS Cloudformation and check the Stack info. You will find the reason as to why the deletion failed.</p> <p>You can then retry deleting the resources once you have identified and fixed the issue.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/verify-worker-nodes/","title":"Verify Worker Nodes","text":"<p>Let's verify if worker nodes were created.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/verify-worker-nodes/#step-1-list-node-groups","title":"Step 1: List Node Groups","text":"<p>List the node groups in the EKS cluster we created.</p> <pre><code># Command template\neksctl get nodegroup --cluster &lt;cluster-name&gt;\n\n# Actual command\neksctl get nodegroup --cluster my-cluster\n</code></pre>"},{"location":"kubernetes-on-eks/create-eks-cluster/verify-worker-nodes/#step-2-list-worker-nodes","title":"Step 2: List Worker Nodes","text":"<p>List the worker nodes in the EKS cluster we created.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"kubernetes-on-eks/create-eks-cluster/verify-worker-nodes/#more-useful-commands","title":"More Useful Commands","text":"<p>Here are some more useful <code>kubectl</code> commands:</p> <pre><code># Display the cluster info\nkubectl cluster-info\n\n# Get all worker nodes with expanded (aka \"wide\") output\nkubectl get nodes -o wide\n\n# Describe a node\nkubectl describe node &lt;node-name&gt;\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get no \nkubectl get node\nkubectl get nodes\n</code></pre> <p>Note</p> <p><code>node</code> is abbreviated as <code>no</code>.</p>"},{"location":"kubernetes-on-eks/create-eks-cluster/vpc-resources-created-by-eksctl/","title":"VPC Resources Created by eksctl","text":"<p>Based on the <code>ClusterConfig</code> we provided, Amazon EKS will create the following AWS VPC resources:</p> <ol> <li> <p>VPC</p> <p>eksctl creates a new VPC with a CIDR block of <code>192.168.0.0/16</code>.</p> </li> <li> <p>Internet Gateway</p> <p>An internet gateway is created and attached to the VPC to enable communication between the cluster and the public internet.</p> </li> <li> <p>Subnets</p> <p>Six subnets are created across 3 availability zones; three public and three private.</p> <p>Two subnets in each AZ; one private and one public.</p> <p>Here's the CIDR block of each subnet that is created:</p> <ul> <li> <p>subnets for ap-south-1a</p> <ul> <li>public:192.168.0.0/19</li> <li>private:192.168.96.0/19</li> </ul> </li> <li> <p>subnets for ap-south-1b</p> <ul> <li>public:192.168.32.0/19</li> <li>private:192.168.128.0/19</li> </ul> </li> <li> <p>subnets for ap-south-1c</p> <ul> <li>public:192.168.64.0/19</li> <li>private:192.168.160.0/19</li> </ul> </li> </ul> </li> <li> <p>Route Tables</p> <p>Four route tables are created as follows:</p> <ul> <li>One for public traffic. All the public subnets are associated with this route table.</li> <li>Three for private traffic; one per private subnet. Each private subnet is associated to one of these route tables.</li> </ul> </li> <li> <p>NAT Gateway</p> <p><code>eksctl</code> creates only one NAT gateway and attaches it to all the private route tables. This allows outbound internet traffic from the nodes in the private subnet.</p> <p>If you want NAT gateway to be created in each of the three availability zones for high availability, you can add the following fields in the <code>ClusterConfig</code>:</p> <pre><code>vpc:\nnat:\n    gateway: \"highly_available\"\n</code></pre> <p>With the above configuration eks<code>ctl will create a NAT gateway in each availability zone specified in the</code>availabilityZones` field.</p> <p>Warning</p> <p>Creating multiple NAT gateways will incur additional costs, and you should carefully consider the cost implications before enabling this feature.</p> </li> <li> <p>Security Groups</p> <p><code>eksctl</code> creates security groups for initial nodegroup and the control plane.</p> <p>If <code>SSH</code> was enabled, <code>eksctl</code> will also create a security group that allows <code>SSH</code> access.</p> <p><code>eksctl</code> will also create and manage a shared node security group that allows communication between unmanaged nodes and the cluster control plane and managed nodes.</p> </li> <li> <p>Elastic Network Interfaces (ENIs)</p> <p>ENIs are created and attached to the nodes for communication with other resources within the VPC.</p> <p>An Elastic Network Interface (ENI) is a logical networking component in AWS that represents a virtual network card.</p> </li> </ol> <p>Below is an architectural diagram illustrating the structure of the EKS cluster we've created:</p> <p> </p>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/","title":"Create a Customized Nginx Docker Image","text":"<p>Let's see how you can create your own Docker image based on your requirements. In this tutorial, we will create a customized version of the Nginx image that serves an HTML page we want.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-1-create-html-file-to-be-served-by-nginx","title":"Step 1: Create HTML File to Be Served by Nginx","text":"<p>By default, nginx serves the <code>index.html</code> file present in <code>/usr/share/nginx/html</code> directory. You can verify this by checking the content of <code>/etc/nginx/conf.d/default.conf</code>.</p> <p>We'll replace the default <code>index.html</code> with our customized <code>index.html</code> file.</p> <p>Let's create the customized <code>index.html</code> file as follows:</p> <code>index.html</code> <pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Nginx&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h2&gt;Hello from Nginx Container&lt;/h2&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-2-create-dockerfile","title":"Step 2: Create Dockerfile","text":"<p>A <code>Dockerfile</code> is like a step-by-step instruction manual for creating a <code>Docker</code> image. It's a plain text file that contains a series of commands and settings that define how to construct a container image.</p> <p>Create the <code>Dockerfile</code> as follows:</p> <code>Dockerfile</code> <pre><code>FROM nginx:latest\nCOPY ./index.html /usr/share/nginx/html/index.html\n</code></pre> <p>What does this <code>Dockerfile</code> do?</p> <ol> <li>It specifies the base image as the <code>latest</code> version of the official Nginx image.</li> <li>It copies the local <code>index.html</code> file into the Nginx container to replace the default one.</li> </ol> <p>Note</p> <p>A \"base image\" is the initial image used as a starting point when creating a custom Docker image.</p> <p>Here's what your folder structure should look like:</p> <pre><code>|-- my-folder\n\u2502   |-- index.html\n\u2502   |-- Dockerfile\n</code></pre>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-3-build-the-image","title":"Step 3: Build the Image","text":"<p>Now that we have the <code>Dockerfile</code> and custom <code>index.html</code> file ready, we can use the <code>docker build</code> command to create the Docker image as follows:</p> <pre><code># Command template\ndocker build -t &lt;image-name&gt;:&lt;image-tag&gt; &lt;path-to-Dockerfile&gt;\n\n# Actual command\ndocker build -t my-nginx-image:v1 .\n</code></pre> <p>What does the above command do?</p> <ol> <li>It creates an image from the <code>Dockerfile</code> we provide</li> <li>It assigns the image a tag of <code>v1</code></li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-4-list-images","title":"Step 4: List Images","text":"<p>List Docker images to verify if the image we built is present:</p> <pre><code>docker images\n</code></pre> <p>You should see the newly built image in the list.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-5-run-container-from-the-image","title":"Step 5: Run Container From the Image","text":"<p>Run a container from the newly created image as follows:</p> <pre><code>docker run -d --name my-nginx-container -p 81:80 my-nginx-image:v1\n</code></pre> <p>What does the above command do?</p> <ol> <li>It runs a container named <code>my-nginx-container</code> in detached mode, using the image <code>my-nginx-image:v1</code></li> <li>Additionally, it exposes port <code>80</code> of the container to port <code>81</code> on the host</li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-6-access-the-nginx-application","title":"Step 6: Access the Nginx Application","text":"<ol> <li> <p>Access the application from inside the container:     <pre><code># Start a shell session inside the container\ndocker exec -it my-nginx-container bash\n\n# Access the localhost endpoint\ncurl localhost\n</code></pre></p> <p>You'll notice that the custom Nginx page we provided is being served.</p> <p>You can also see the custom HTML we provided file as follows: <pre><code>cat /usr/share/nginx/html/index.html\n</code></pre></p> </li> <li> <p>Access the application from host machine:</p> <p>Since we've exposed our application to the host through port mapping, you can simply open any browser on your host and visit the following endpoint to access the application:</p> <pre><code>localhost:81\n</code></pre> <p>You'll see the custom nginx page we provided.</p> </li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-7-view-container-logs","title":"Step 7: View Container Logs","text":"<p>View the container logs as follows: <pre><code># Command template\ndocker logs -f &lt;container-id/container-name&gt;\n\n# Actual command\ndocker logs -f my-nginx-container\n</code></pre></p> <p>In your browser, visit <code>localhost:81</code> a few times, and you'll see the Nginx access log being streamed in the console.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/customized-nginx-docker-image/#step-8-clean-up","title":"Step 8: Clean Up","text":"<pre><code># Stop the container\ndocker stop my-nginx-container\n\n# Delete the container\ndocker rm my-nginx-container\n</code></pre> <p>Note</p> <p>A container needs to be stopped before it can be deleted. You can't delete a running container.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/","title":"Install Docker","text":"<p>Docker is an open platform for developing, shipping, and running applications.</p> <p>You can download and install Docker on the platform of your choice, including Mac, Linux, or Windows by following the platform-specific instructions provided below.</p> <p>Warning</p> <p>Given the ever-changing nature of the installation process, it is advisable to rely on the official documentation when installing Docker.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/#docker-engine-vs-docker-desktop","title":"Docker Engine vs Docker Desktop","text":"<p><code>Docker Engine</code> is the core software that allows you to create and run Docker containers, while <code>Docker Desktop</code> is a complete development environment that includes Docker Engine and several other tools and services for building and testing Docker applications on your local machine.</p> <p>Note</p> <p>Docker Desktop is the only way to install the Docker Engine on <code>Windows</code> and <code>macOS</code> operating systems.</p> <p>Docker Desktop is also available for <code>Linux</code>, although Linux users are free to install the Docker Engine separately.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/#install-docker-on-mac","title":"Install Docker on Mac","text":"<p>Docker Desktop Download URL: Install Docker Desktop on Mac</p> <p>You'll see two options:</p> <ol> <li>Docker Desktop for Mac with Intel chip</li> <li>Docker Desktop for Mac with Apple silicon</li> </ol> <p>To find out which option to choose, click on the Apple logo on top left corner of the Mac and click on <code>About This Mac</code>. Search for a line that mentions either <code>Chip</code> or <code>Processor</code>. </p> <p>If you find <code>M1</code> or <code>M2</code> in that line, it means the computer is powered by Apple Silicon. Conversely, if you see the word <code>Intel</code>, it indicates the machine is equipped with an Intel-based Core series processor.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/#install-docker-on-windows","title":"Install Docker on Windows","text":"<p>Docker Desktop Download URL: Install Docker Desktop on Windows</p> <p>You may see a message that says: <code>WSL2 installation is incomplete</code>. Make sure <code>WSL2</code> is insatlled.</p> <p>If during installation you see a checkbox that says Use <code>WSL2</code> instead of <code>Hyper-V</code> (recommended), make sure you check this checkbox.</p> <p>Windows Subsystem for Linux (WSL) is a feature of Windows that allows developers to run a Linux environment without the need for a separate virtual machine or dual booting.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/#install-docker-on-ubuntu-linux","title":"Install Docker on Ubuntu (Linux)","text":""},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/#method-1-docker-desktop","title":"Method 1: Docker Desktop","text":"<p>Download Deb Package: Install Docker Desktop on Ubuntu</p> <ul> <li> <p>For non-Gnome Desktop environments, gnome-terminal must be installed:</p> <pre><code>sudo apt install gnome-terminal\n</code></pre> </li> <li> <p>Install the package with apt as follows:</p> <pre><code>sudo apt-get update\nsudo apt-get install ./docker-desktop-&lt;version&gt;-&lt;arch&gt;.deb\n</code></pre> </li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/install-docker/#method-2-docker-engine","title":"Method 2: Docker Engine","text":"<p>Docker Engine comes bundled with Docker Desktop for Linux. But if you want to install only Docker Engine and not the Docker Desktop, you can do so by following the instructions below.</p> <p>Official Documentation URL: Install Docker Engine on Ubuntu</p> <p>Let's install Docker Engine using the repository.</p> <p>Step 1: Set up the repository</p> <ol> <li> <p>Update the apt package index and install packages to allow apt to use a repository over HTTPS:</p> <pre><code>sudo apt-get update\n\nsudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n</code></pre> </li> <li> <p>Add Docker\u2019s official GPG key:</p> <pre><code>sudo mkdir -m 0755 -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n</code></pre> </li> <li> <p>Use the following command to set up the repository:</p> <pre><code>echo \\\n\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> </li> </ol> <p>Step 2: Install Docker Engine</p> <ol> <li> <p>Update the apt package index:</p> <pre><code>sudo apt-get update\n</code></pre> </li> <li> <p>Install latest version of Docker Engine, containerd, and Docker Compose:</p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> </li> <li> <p>Verify that the Docker Engine installation is successful by running the hello-world image:</p> <pre><code>sudo docker run hello-world\n</code></pre> </li> </ol> <p>References:</p> <ul> <li>Install Docker Desktop on Mac</li> <li>Install Docker Desktop on Windows</li> <li>Install Docker Desktop on Ubuntu</li> <li>Install Docker Engine on Ubuntu</li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/","title":"Introduction to Docker","text":"<p>Have you ever been curious about Docker and why it's such a big deal? Well, you're in the right place. In this tutorial, we'll break it down step by step and figure out what Docker is and why it's important.</p> <p>Let's get started!</p> <p> </p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#architecture-of-a-traditional-machine","title":"Architecture of a Traditional Machine","text":"<p>Before we dive into Docker, it is important to understand the architecture of a traditional machine and how deployments work on it. </p> <p>You\u2019ll have a better understanding of Docker and its purpose if you understand the architecture of a traditional machine and the challenges encountered when deploying applications on them.</p> <p>On a traditional machine, you have hardware consisting of a CPU, RAM, storage, and other components. On this hardware, you install an operating system like Windows, Linux, or Mac. </p> <p>Next, you install the necessary libraries and dependencies on the operating system for running your application. Finally, you run your application, such as a Python application, an Nginx server, or a database like MySQL.</p> <p> </p> <p>Architecture of a Traditional Machine</p> <ul> <li>Layer1: Hardware</li> <li>Layer2: Operating System</li> <li>Layer3: Libraries and Binaries</li> <li>Layer4: Applications</li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#challenges-in-deploying-applications-on-a-traditional-machine","title":"Challenges in Deploying Applications on a Traditional Machine","text":"<p>Now, let\u2019s look at the problems in deploying applications on a traditional machine. Let me illustrate this with a story.</p> <p> </p> <p>Adam and Bob are colleagues at ABC Software Private Limited. Adam is a software developer, and Bob is a QA engineer.</p> <p>Adam has been working on a project using Node.js and MongoDB. He has added a cool new feature that is all set for the QA testing. He hands the feature over to Bob, the QA champ.</p> <p>Bob gives it a try on his computer, but the feature seems broken and doesn't work. Bob realizes something's off and sends a message to Adam, saying, \"There's a problem, the code you shipped is not working as expected.\u201d</p> <p>Adam is pretty sure that the feature is fine and doesn't have any problems. To make sure, he tries the feature on his own computer again. Turns out, he was right. The feature works perfectly on his computer without any issues.</p> <p>Adam scratches his head and goes to Bob, saying, \"Hey, the code is working fine on my machine. Why the hell isn't it working on yours?\u201d</p> <p>And that, folks, is just another day in the life of software adventures.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#reasons-behind-the-probelm","title":"Reasons Behind the Probelm","text":"<p>Why did that happen?</p> <p>How come the exact same code worked on one machine but failed to run on the other?</p> <p>Let\u2019s look into the possible reasons:</p> <ol> <li> <p>Bob might not have installed the libraries and dependencies correctly on his system, for example, Node.js and MongoDB in this case.</p> </li> <li> <p>Bob could've installed the wrong version of the dependencies which is incompatible with parts of the code that Adam shipped. For instance, he might have installed Node.js version 12 instead of version 14.</p> </li> <li> <p>It's also possible that Bob's computer is loaded with other software, causing conflicts with the required dependencies or libraries.</p> </li> <li> <p>And let's not overlook the fact that Bob could be using a completely different operating system than Adam's, one that is incompatible with the required dependencies and libraries.</p> </li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#the-catastrophe","title":"The Catastrophe","text":"<p>Warning</p> <p>What if Bob used the same operating system and dependencies as Adam, and the code worked on his computer?</p> <p>He would likely send it for production deployment. However, here's the challenge: We cannot ensure that the code will function properly on the production machine unless it is running the same operating system and has the required dependencies and libraries correctly installed.</p> <p>This particular scenario highlights one of the most significant challenges associated with deploying software in this manner.</p> <p>Additionally, this method of software development is error-prone because developers are required to install all the necessary libraries and dependencies on their machines, which are OS-dependent. It's very probable they'll skip steps or misconfigure some settings, leading to problems when trying to run the code without issues. Imagine setting up 10 more services apart from Node.js and MongoDB on your machine just to be able to run your code properly. You\u2019ll go crazy!</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#whats-the-solution","title":"What's the Solution?","text":"<p>One solution is to bundle everything together including the operating system, binaries, libraries, and dependencies, and then use the same bundle across environments.</p> <p> </p> <p>This is exactly what Virtual Machines (VMs) do.</p> <p>With virtual machines, you can bundle everything together, create an image or snapshot of the bundle, and then share it across environments.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#how-do-virtual-machines-vms-work","title":"How Do Virtual Machines (VMs) Work?","text":"<p>Imagine you have a regular computer with an operating system on it. Now, imagine adding a special layer on top of that computer. This special layer is a software called Hypervisor. Popular options for hypervisors include VMware and VirtualBox. This hypervisor does a cool trick - it creates and runs virtual machines.</p> <p>Think of virtual machines as another computer inside your real computer. The hypervisor lets your main computer act like a host, sharing its power (like the CPU and memory) with these virtual computers.</p> <p>Once the hypervisor is in place, you can install a full-fledged operating system on it. This new operating system is like a guest, hanging out inside the main computer.</p> <p>With the guest operating system set up, you can start adding the required binaries, libraries, and dependencies and finally run your applications.</p> <p> </p> <p>The cool thing about this guest operating system is that you can pack it up and share it. It's like making a copy of a game you really like so your friends can play too. In the world of virtual machines, this copy is called an \"image\".</p> <p>Now, anyone can use this image to create their own virtual machine and do their work on it.</p> <p>Architecture of a Virtual Machine</p> <ul> <li>Layer1: Hardware</li> <li>Layer2: Operating System</li> <li>Layer3: Hypervisor (E.g. VMware, VirtualBox)</li> <li>Layer4: Guest Operating System</li> <li>Layer5: Libraries and Binaries</li> <li>Layer6: Applications</li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#deployment-with-virtual-machines","title":"Deployment With Virtual Machines","text":"<p>Remember Adam and Bob? Here's the update on their story. The smart DevOps team used VMware to create a virtual machine. They added things like Node.js and MongoDB plus other stuff it needs. Then, they made a copy of everything - that's the image. They gave this image to Adam and Bob, and guess what? The same image will be used for production too.</p> <p>So, now Adam and Bob each have a copy of this special image. They can use it to create their own virtual machines and do their work. When Adam makes something and wants Bob to check it out, he knows that if it works on his virtual machine, it will work on Bob's and the Production machine too!</p> <p> </p> <p>So, with Virtual Machines in place, are things all good now? No!</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#disadvantages-of-virtual-machines","title":"Disadvantages of Virtual Machines","text":"<p>Although hypervisors and Virtual Machines offer\u00a0numerous advantages,\u00a0there are also some disadvantages to consider:</p> <p>1.\u00a0Performance overhead:\u00a0Running a full-fledged operating system inside a virtual machine requires additional processing overhead. This can lead to reduced performance compared to running the same operating system directly on the host hardware.</p> <p>2.\u00a0Resource limitations:\u00a0Each virtual machine requires a portion of the host system resources, such as CPU, memory, and storage. This can limit the number of virtual machines that can be run simultaneously on a given hardware.</p> <p>3.\u00a0Complexity:\u00a0The setup and management of virtual machines can be more complex than managing physical machines, and may require specialized knowledge and tools.</p> <p>4.\u00a0Licensing costs: Each virtual machine needs a full-fledged operating system and may need a license. This increases the overall cost of running an application.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#what-are-containers","title":"What are Containers?","text":"<p>Containers offer a solution to address the challenges associated with Virtual Machines.</p> <p>You can think of containers as virtual machines with the following differences:</p> <ol> <li> <p>Instead of a hypervisor, you need a container run-time such as <code>Docker</code> or <code>Containerd</code>.</p> </li> <li> <p>The container provides isolation between the host operating system and the application running inside the container, similar to virtual machines, but without the overhead of a full operating system, leading to better performance and resource utilization.</p> </li> <li> <p>Containers are more lightweight than virtual machines, as they share the host's operating system kernel, leading to better resource utilization and lower overhead.</p> </li> <li> <p>Containers can be deployed and started quickly, as they do not require the time-consuming process of booting up a complete virtual machine.</p> </li> </ol> <p> </p> <p>Architecture of Containers</p> <ul> <li>Layer1: Hardware</li> <li>Layer2: Operating System</li> <li>Layer3: Container Runtime (E.g. Docker)</li> <li>Layer4: Containers</li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a technology that provides a platform for creating, deploying, and managing containers.</p> <p>Think of Docker as a super handy toolbox for containers. It helps you create, manage, and run these nifty container things we talked about earlier.</p> <p>Docker takes care of all the hard work, like setting up containers and making sure they play nice with your computer. It's like having a container wizard by your side.</p> <p>Tip</p> <p>So, if containers are the cool kids, Docker is the captain of the cool kids - making everything easy and awesome.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#compatibility-of-docker-across-operating-systems","title":"Compatibility of Docker Across Operating Systems","text":"<p>In the beginning, Docker was all about Linux. It had a close bond with Linux. For example, you could not run Linux based docker container on a Windows machine because Linux-based containers can\u2019t use Windows OS kernel.</p> <p>But Docker had a grand vision: they wanted to spread the container magic to Windows and Mac users.</p> <p>That's where <code>Docker Desktop</code> stepped in. It functions as an intelligent mediator, introducing a portion of Linux functionality to both Windows and Mac environments. This unique component, referred to as a lightweight Linux kernel, serves as an intermediary, facilitating the seamless operation of containers across these distinct systems.</p> <p>With Docker Desktop, it's not just about Linux anymore. Windows and Mac users also get to share the container's goodness. It's like a universal language that these systems understand, making containers accessible and practical for everyone.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#architecture-of-docker","title":"Architecture of Docker","text":"<p>Now that we've got a handle on Docker and containers, let's dive into the nuts and bolts of how Docker actually works behind the scenes.</p> <p>Docker uses a client-server architecture. Think of the <code>Docker Client</code> as your control center. It's what you use to tell Docker what you want to do \u2013 like create, start, or stop containers. You talk to it directly. </p> <p>The <code>Docker Daemon</code> which is the server part, is like the backstage crew. When you give a command to the client, the daemon springs into action, doing the heavy lifting of building, running, and distributing your Docker containers.</p> <p>These two components \u2013 the Docker client and the Docker daemon \u2013 communicate using a REST API. This API functions as their communication bridge. When you send a request from the Docker client, it travels via this API to the Docker daemon, which then processes it and sends back the necessary response.</p> <p> </p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#docker-image-vs-container","title":"Docker Image vs Container","text":"<p>Think of an image as a snapshot or a blueprint of an application and its dependencies. Similar to an image in a virtual machine.</p> <p>It's like a template that contains everything needed to run your application, including the code, libraries, dependencies, configuration files, and environment variables. </p> <p>Example</p> <p>Let's look at a Node.js application. To start, you pick a lightweight operating system (Just a quick reminder, with Docker, you don't need a full-fledged operating system because the container shares the host's operating system kernel). </p> <p>After that, you bring in Node.js run time and npm which is the Node.js package manager, and then your app's code, configuration files, and environment variables. And finally, add the instructions on how to run your app. Once you've got everything set up, you make a package that can be shared anywhere or with others. This package is what's called a Docker Image.</p> <p>These images are read-only, which means you can't change anything inside them once they're created. They're designed to be consistent and portable across different environments.</p> <p>Once you have the Docker image ready, you can use it to run a container. Imagine a container as a running instance of your application based on a Docker Image.</p> <p>Remember, an image is a read-only template with instructions for creating a container while a container is a runnable instance of an image. </p> <p>Tip</p> <p>It's like using a recipe to cook a dish - the image is the recipe, and the container is the cooked meal.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#how-does-docker-know-what-to-include-in-an-image","title":"How Does Docker Know What to Include in an Image?","text":"<p>Well, Docker uses a special recipe called a <code>Dockerfile</code> to create your image just the way you want it. </p> <p>A <code>Dockerfile</code> is like a step-by-step instruction manual for creating a Docker image. It's a plain text file that contains a series of commands and settings that define how to construct a container image. With a Dockerfile, you outline everything needed to set up your application or environment inside a container.</p> <p>Here's an example Dockerfile:</p> <code>Dockerfile</code> <pre><code>FROM node:18\n\n# Create app directory\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\n\nRUN npm install\n\n# Bundle app source\nCOPY . .\n\nEXPOSE 5000\n\nCMD [ \"node\", \"server.js\" ]\n</code></pre> <p>Each line in a <code>Dockerfile</code> represents a specific action, like installing software, copying files, or configuring settings. When you build a Docker image using a <code>Dockerfile</code>, Docker reads and executes these instructions sequentially to create a consistent and reproducible image.</p> <p>In essence, a <code>Dockerfile</code> captures your application's requirements and the steps to get it up and running within a container. It's a powerful tool for automating the process of building and deploying containers, making your development and deployment workflows smoother and more reliable.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#docker-registry-and-repository","title":"Docker Registry and Repository?","text":"<p>When you run the <code>docker build</code> command on your host to build a Docker image from a Dockerfile, Docker stores the resulting image on the host machine where you executed the <code>docker build</code> command.</p> <p>What if you wish to securely store an image and have the ability to share it with others? This is where the Docker <code>registry</code> comes into play. The Docker registry acts as a centralized location where you can store images, making them accessible to both your team members and the public. </p> <p>To put it simply, think of the <code>registry</code> as a storage account for holding your images, while the <code>repository</code> within that storage account acts as a folder to neatly organize these images.</p> <p>Docker Hub is one of the most popular and widely used public Docker registries. It hosts a rich variety of ready-to-use Docker images customized for popular applications like <code>MySQL</code>, <code>Nginx</code>, <code>Node.js</code>, and others.</p> <p>You can create your own repository on Docker Hub, store images, and share them with others, or even maintain private repositories for internal use.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/introduction-to-docker/#summary","title":"Summary","text":"<p>In summary, Docker solves the problem of inconsistent application behavior across different machines by providing a way to package applications and their dependencies into portable, self-contained containers that can run consistently on any machine that has Docker installed in it.</p> <p> </p> <p>References:</p> <ul> <li>Docker Overview</li> <li>Virtual Machines</li> <li>Hypervisor</li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/","title":"Containerizing a Node.js and Express App","text":"<p>Let's see how we can containerize a <code>Node.js</code> and <code>Express</code> application.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-1-create-application-using-nodejs-and-express","title":"Step 1: Create Application Using Node.js and Express","text":"<p>The first step is to create an application using <code>Node.js</code> and <code>Express</code> framework.</p> <p>Create <code>server.js</code> and <code>package.json</code> files as follows:</p> <code>server.js</code> <code>package.json</code> <pre><code>const express = require('express')\nconst app = express()\nvar os = require('os');\n\nconst PORT = process.env.PORT || 5000;\n\n// Returns the hostname, version and other app details\napp.get('/', function (_req, res) {\nlet host = os.hostname();\ndata = {\n    Host: host,\n    Version: \"v1\"\n}\nres.status(200).json(data);\n});\n\napp.get('/health', function(_req, res) {\nres.status(200).send('Healthy');\n});\n\napp.get('/random', function(_req, res) {\nrandomNumber = Math.floor(Math.random() * 10) + 1;\nconsole.log(`The random number generated is: ${randomNumber}`);\nres.status(200).send(randomNumber.toString());\n});\n\napp.listen(PORT, () =&gt; {\nconsole.log(`Server is running on port ${PORT}`);\n});\n</code></pre> <pre><code>{\n    \"dependencies\": {\n        \"express\": \"^4.18.2\"\n    }\n}\n</code></pre> <p>Here's what your folder structure should look like:</p> <pre><code>|-- my-folder\n\u2502   |-- package.json\n\u2502   |-- server.js\n</code></pre> <p>The above Express app has the following routes:</p> <pre><code>GET /\nGET /health\nGET /random\n</code></pre> <p>What does each route return?</p> <ul> <li><code>GET /</code> returns a JSON object containing Host and Version.</li> <li><code>GET /health</code> returns the health status.</li> <li><code>GET /random</code> returns a random integer between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-2-test-the-application-locally","title":"Step 2: Test the Application Locally","text":"<p>Go to the folder containing <code>package.json</code> and run the following commands:</p> <pre><code># Install npm packages\nnpm install\n\n# Start the express server\nnode server.js\n</code></pre> <p>Note</p> <p>You must have <code>Node.js</code> installed on your local machine to run the above commands.</p> <p>If there are no errors, your Express application will begin serving requests on the specified port, which in our case is <code>5000</code>.</p> <p>Verify if the endpoints are working as expected: <pre><code>curl localhost:5000\ncurl localhost:5000/health\ncurl localhost:5000/random\n</code></pre></p> <p>You can also access the endpoints mentioned above directly from your browser.</p> <p>Note</p> <p>When you run <code>npm install</code>, it generates a <code>package-lock.json</code> file to record and lock down the specific versions of installed packages and their dependencies. This ensures consistency in package versions across different environments and collaborations.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-3-create-dockerfile","title":"Step 3: Create Dockerfile","text":"<p>Now, let's containerize our application. To do that, we first need to create a <code>Dockerfile</code> with instructions on how to build the image.</p> <p>Create the <code>Dockerfile</code> as follows:</p> <code>Dockerfile</code> <pre><code>FROM node:18\n\n# Create app directory\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\n\nRUN npm install\n\n# Bundle app source\nCOPY . .\n\nEXPOSE 5000\n\nCMD [ \"node\", \"server.js\" ]\n</code></pre> <p>Brief explanation of each command in Dockerfile</p> <ol> <li> <p><code>FROM node:18</code>: Specifies the base image as Node.js version 18, which serves as the foundation for your image.</p> </li> <li> <p><code>WORKDIR /usr/src/app</code>: Sets the working directory inside the container to <code>/usr/src/app</code>.</p> </li> <li> <p><code>COPY package*.json ./</code>: Copies the <code>package.json</code> and <code>package-lock.json</code> (if present) from the local directory into the container's working directory.</p> </li> <li> <p><code>RUN npm install</code>: Runs the <code>npm install</code> command inside the container to install the application's dependencies.</p> </li> <li> <p><code>COPY . .</code>: Copies the contents of the local directory (your Node.js application code) into the container's working directory.</p> </li> <li> <p><code>EXPOSE 5000</code>: Informs Docker that the container will listen on port 5000, though it doesn't actually publish the port to the host.</p> </li> <li> <p><code>CMD [ \"node\", \"server.js\" ]</code>: Specifies the command that will be executed when the container starts. In this case, it runs your Node.js application using the <code>server.js</code> script.</p> </li> </ol> <p>These steps are essential for building a Docker image for your <code>Node.js</code> application, including setting up the environment, copying code and dependencies, and defining how to run the application within the container.</p> <p>Here's what your folder structure should look like at this point:</p> <pre><code>|-- my-folder\n\u2502   |-- package.json\n\u2502   |-- server.js\n|   |-- Dockerfile\n</code></pre>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-4-build-the-docker-image","title":"Step 4: Build the Docker Image","text":"<p>With the application code and <code>Dockerfile</code> prepared, we are now ready to build the Docker image for our application.</p> <p>Build the Docker image as follows: <pre><code># Command template\ndocker build -t &lt;image-name&gt;:&lt;image-tag&gt; &lt;path-to-Dockerfile&gt;\n\n# Actual command\ndocker build -t my-node-express-image:v1 .\n</code></pre></p> <p>The above command will build an image named <code>my-node-express-image</code> with tag <code>v1</code>.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-5-verify-the-image","title":"Step 5: Verify the Image","text":"<p>List images and verify if the newly created image is present:</p> <pre><code>docker images\n</code></pre> <p>Look for the image <code>name</code> and the <code>tag</code> we specified while building the image.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-6-run-a-container-from-the-image","title":"Step 6: Run a Container From the Image","text":"<p>Now that we have the image ready, let's create a container from it and test the application.</p> <pre><code>docker run -d --name my-node-express-container -p 5001:5000 my-node-express-image:v1\n</code></pre> <p>What does the above command do?</p> <ol> <li>It runs a container named <code>my-node-express-container</code> in detached mode, using the image <code>my-node-express-image:v1</code>. </li> <li>Additionally, it exposes port <code>5000</code> of the container to port <code>5001</code> on the host</li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-7-test-the-application","title":"Step 7: Test the Application","text":"<ol> <li> <p>Test the application from within the container:</p> <ul> <li> <p>Start a shell session inside the container</p> <pre><code>docker exec -it my-node-express-container bash\n</code></pre> </li> <li> <p>Access the endpoints</p> <pre><code>curl localhost:5000\ncurl localhost:5000/health\ncurl localhost:5000/random\n</code></pre> </li> </ul> </li> <li> <p>Test the application from host computer (Outside the container):</p> <p>Open any browser on your host computer and hit the following endpoints to verify if port mapping is working as expected.</p> <pre><code>localhost:5001\nlocalhost:5001/health\nlocalhost:5001/random\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-8-view-container-logs","title":"Step 8: View Container Logs","text":"<pre><code># Command template\ndocker logs -f &lt;container-id/container-name&gt;\n\n# Actual command\ndocker logs -f my-node-express-container\n</code></pre>"},{"location":"kubernetes-on-eks/docker-fundamentals/nodejs-express-docker-image/#step-9-clean-up","title":"Step 9: Clean Up","text":"<pre><code># Stop the container\ndocker stop my-node-express-container\n\n# Remove the container\ndocker rm my-node-express-container\n</code></pre>"},{"location":"kubernetes-on-eks/docker-fundamentals/push-image-to-docker-hub/","title":"Publish Image to Docker Hub","text":"<p>To share your image with your team or the public, you should store it in a Docker registry. Docker Hub serves as a registry that enables you to either privately share your images with your team or make them accessible to the public.</p> <p>Let's see how you can do that.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/push-image-to-docker-hub/#step-1-create-an-account-on-docker-hub","title":"Step 1: Create an Account on Docker Hub","text":"<p>Visit Docker Hub and create an account.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/push-image-to-docker-hub/#step-2-create-a-repository","title":"Step 2: Create a Repository","text":"<p>A Docker <code>repository</code> is a collection of Docker images that are stored and made available for others to use and download.</p> <p>Let's create a public repository named <code>node-express</code>.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/push-image-to-docker-hub/#step-3-push-image-to-docker-repository","title":"Step 3: Push Image to Docker Repository","text":"<ol> <li> <p>Login to Docker Hub</p> <pre><code>docker login\n</code></pre> </li> <li> <p>Re-tag the Docker image you want to push</p> <pre><code># Command template\ndocker tag &lt;existing-image&gt;:&lt;tag&gt; &lt;docker-hub-username&gt;/&lt;repository-name&gt;:&lt;tag&gt;\n\n# Actual command\ndocker tag my-node-express-image:v1 reyanshkharga/node-express:v1\n</code></pre> </li> <li> <p>Push the image to repository</p> <pre><code># Command template\ndocker push &lt;docker-hub-username&gt;/&lt;repository-name&gt;:&lt;tag&gt;\n\n# Actual command\ndocker push reyanshkharga/node-express:v1\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/push-image-to-docker-hub/#step-4-verify-the-image-in-docker-hub","title":"Step 4: Verify the Image in Docker Hub","text":"<p>Go to Docker Hub and verify if the image is pushed. Now, if the image is public, anyone can pull the image.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/","title":"Run Container From a Public Docker Image","text":"<p>Before we create our own Docker image, let's see how we can use publicly available Docker images to run containers. </p> <p><code>Docker Hub</code> is one of the public registries you can use to download the official images for many popular services such as <code>Nginx</code>, <code>MySQL</code>, <code>MongoDB</code>, <code>Redis</code>, <code>Node.js</code>, <code>Java</code>, <code>Python</code>, etc.. These images are created either by the companies that developed these services or by the Docker community. </p> <p>Note</p> <p>Docker Hub hosts not only the official images but also an extensive collection of public images contributed by developers, serving diverse purposes and use cases.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#list-images-and-containers","title":"List Images and Containers","text":"<p>Execute the following <code>Docker CLI</code> commands to view images and containers on your host.</p> <p>List Docker Images: <pre><code>docker images\n</code></pre></p> <p>List Docker Containers: <pre><code>docker ps -a\n</code></pre></p> <p>You can also use the Docker Desktop GUI to view the images and containers on your host.</p> <p>At this point, you will see that you don't have any images or containers. </p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#run-an-nginx-container","title":"Run an Nginx Container","text":"<p>You can go to Docker Hub Explore Page and search for the images that you want to use.</p> <p>Execute the following command to run an Nginx container:</p> <pre><code>docker run nginx\n</code></pre> <p>Here's what happens when you run the above command</p> <ol> <li>The Docker Daemon recognizes that no tag is provided for the image, so it will use the <code>latest</code> tag.</li> <li>The Docker Daemon checks whether the <code>nginx:latest</code> image is present on the host where the command was executed.</li> <li>If the <code>nginx:latest</code> image is not present on the host, it will first pull the <code>nginx:latest</code> image from Docker Hub, which is the <code>default</code> registry.</li> <li>Now that the <code>nginx:latest</code> image is available on the host, the Daemon will use this image to run the container.</li> </ol> <p>The command above runs the container in the foreground and stream the container logs. You can press Ctrl+C to exit but the container will stop.</p> <p>To avoid that, you can use <code>--detach</code> or <code>-d</code> option to run the container in the background.</p> <pre><code>docker run -d nginx\n</code></pre> <p>Using specific image tag</p> <p>If you prefer to use a specific tag of the image instead of the <code>latest</code> tag to run the container, you can use the following syntax to do so.</p> <pre><code>docker run -d &lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre> <p>Assigning container name</p> <p>A random name is assigned to the container if you don't specify a name. You can use <code>--name</code> option to assign a desired name to the container.</p> <pre><code>docker run -d --name my-nginx-container nginx:latest\n</code></pre>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#list-containers","title":"List Containers","text":"<p>List only running containers: <pre><code>docker ps\n</code></pre></p> <p>You can use <code>--all</code> or <code>-a</code> option to show all containers, <code>Running</code> or <code>Stopped</code>: <pre><code>docker ps -a\n</code></pre></p> <p>You can use <code>--quiet</code> or <code>-q</code> option to display only the container IDs: <pre><code># List running container IDs\ndocker ps -q\n\n# List all container IDs\ndocker ps -a -q\n</code></pre></p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#stop-a-container","title":"Stop a Container","text":"<pre><code># Stop container using ID\ndocker stop &lt;container-id&gt;\n\n{OR}\n\n# Stop container using name\ndocker stop &lt;container-name&gt;\n</code></pre> <p>Note</p> <p>Stopping a container doesn't delete the container; it simply halts the container's execution. All the data and configuration associated with that container remain intact.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#start-a-stopped-container","title":"Start a Stopped Container","text":"<pre><code># Start container using ID\ndocker start &lt;container-id&gt;\n\n{OR}\n\n# Start container using name\ndocker start &lt;container-name&gt;\n</code></pre> <p>Note</p> <p>Deleting a container permanently removes it from your system. This action deletes all the data and configuration associated with that container.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#execute-a-command-in-the-running-container","title":"Execute a Command in the Running Container","text":"<p>The <code>docker exec</code> command runs a new command in a running container. The command runs in the default directory of the container.</p> <ol> <li> <p>Execute a command in a running container without starting a shell session:</p> <pre><code># List files and directories\ndocker exec -it &lt;container-name/container-id&gt; ls\n</code></pre> </li> <li> <p>Start a shell session in the container:</p> <pre><code>docker exec -it &lt;container-name/container-id&gt; sh\n{OR}\ndocker exec -it &lt;container-name/container-id&gt; bash\n</code></pre> <p>Difference between <code>sh</code> and <code>bash</code></p> <p><code>bash</code> and <code>sh</code> are two different shells in the Unix operating system. <code>bash</code> is an extended version of <code>sh</code> with additional features and improved syntax.</p> <p>Once the shell session is started, you can interact with the container just like you would with your own machine. You can run shell commands like <code>ls</code>, <code>pwd</code>, or any other desired commands.</p> <p>In our case since the container is running Nginx, we can request the default nginx page as follows:</p> <pre><code>curl localhost\n</code></pre> <p>You can also view the nginx configuration files and default Html page that is served:</p> <pre><code># View nginx configuration file\ncd /etc/nginx/conf.d\ncat default.conf\n\n# View the content of the default Html page that Nginx serves\ncat /usr/share/nginx/html/index.html\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#port-mapping","title":"Port Mapping","text":"<p>In the previous section we learned how to access the application from within the container. But, how do you access the application running inside the container from outside?</p> <p>You can use <code>Port Mapping</code> to make the processes inside the container available from the outside.</p> <pre><code>docker run -d --name nginx -p 81:80 nginx:latest\n</code></pre> <p>In the above command, we are creating the <code>nginx</code> container in detached mode and using the <code>-p</code> option to map port <code>80</code> of the container, where Nginx is running, to port <code>81</code> on the host.</p> <p>Once the container is up and running, you can open any browser on your local machine and access the Nginx application running in the container by visiting <code>localhost:81</code>.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#view-container-logs","title":"View Container Logs","text":"<pre><code>docker logs -f &lt;container-id/container-name&gt;\n</code></pre> <p>Note</p> <p>The <code>-f</code> flag in the <code>docker logs</code> command stands for \"follow\". It allows you to continuously stream the logs of the specified Docker container in real-time.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#delete-a-container","title":"Delete a Container","text":"<pre><code># Delete container using ID\ndocker rm &lt;container-id&gt;\n\n{OR}\n\n# Delete container using name\ndocker rm &lt;container-name&gt;\n</code></pre> <p>Note</p> <p>A container needs to be stopped before it can be deleted. You can't delete a running container.</p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#delete-all-stopped-containers","title":"Delete All Stopped Containers","text":"<p>What if you have multiple stopped containers and you want to delete all of them in a single command?</p> <p>You can use the following command to delete all stopped containers: <pre><code>docker container prune\n</code></pre></p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#delete-an-image","title":"Delete an Image","text":"<p>You can use the following commands to delete Docker images: <pre><code># Delete image using image ID\ndocker rmi &lt;image-id&gt;\n\n{OR}\n\n# Delete image using image name and tag\ndocker rmi &lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre></p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#delete-all-unused-images","title":"Delete All Unused Images","text":"<p>You can use the following command to remove all images on your Docker host that are not associated with a container: <pre><code>docker image prune -a\n</code></pre></p>"},{"location":"kubernetes-on-eks/docker-fundamentals/run-container-from-public-image/#clean-up","title":"Clean Up","text":"<p>You can use the following command to perform a comprehensive cleanup of your Docker system. It removes not only unused containers and images but also associated volumes: <pre><code>docker system prune -a --volumes\n</code></pre></p> <p>Warning</p> <p>This command is an effective way to reclaim disk space and clean up your Docker environment, but it should be used with caution because it permanently deletes containers, images, networks, and volumes that are not actively in use. Make sure you understand the potential consequences before running it in a production environment.</p>"},{"location":"kubernetes-on-eks/eks-with-amazon-ecr/eks-with-amazon-ecr/","title":"EKS With Amazon ECR","text":"<p>You can use Amazon ECR with EKS provided the worker nodes have the required permissions.</p> <p>At minimum, the IAM role must have the following permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:GetAuthorizationToken\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>In our case, the worker nodes already have these permissions. Remember the <code>imageBuilder</code> IAM add-on policies that we used when we created the EKS cluster using <code>eksctl</code>.</p> <p>The <code>imageBuilder</code> policy allows for full ECR (Elastic Container Registry) access.</p> <p>Now, let's see how you can use images from ECR in your EKS Deployments.</p>"},{"location":"kubernetes-on-eks/eks-with-amazon-ecr/eks-with-amazon-ecr/#step-1-create-kubernetes-deployment","title":"Step 1: Create Kubernetes Deployment","text":"<p>Let's create a kubernetes deployment that uses the image from ECR.</p> <code>deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: &lt;account-id&gt;.dkr.ecr.&lt;aws-region&gt;.amazonaws.com/my-nginx-repository:latest\n        resources:\n          requests:\n            cpu: \"1\"\n</code></pre> <p>Ensure to update the value of <code>image</code> field in <code>.spec.containers</code> with the image URI linked to your account and region.</p> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/eks-with-amazon-ecr/eks-with-amazon-ecr/#step-2-view-the-deployment-and-pods","title":"Step 2: View the Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/eks-with-amazon-ecr/eks-with-amazon-ecr/#step-3-view-the-html-file-served-by-nginx","title":"Step 3: View the HTML File Served by Nginx","text":"<pre><code># Start a shell session inside the nginx container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Hit the endpoint\ncurl localhost\n</code></pre> <p>You can also use <code>kubectl port-forward</code> to access the endpoint from your local host machine:</p> <pre><code># forward port 80 of the container to port 3000 on local host machine\nkubectl port-forward deploy/my-deployment 3000:80\n</code></pre> <p>Open any browser on your local host machine and hit localhost:3000.</p>"},{"location":"kubernetes-on-eks/eks-with-amazon-ecr/eks-with-amazon-ecr/#clean-up","title":"Clean Up","text":"<p>Let's delete the Deployment we created:</p> <pre><code>kubectl delete -f deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/","title":"ExternalDNS Demo With Multiple Hosts","text":"<p>Simply add the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation to either the kubernetes Ingress or Service, and ExternalDNS will use this information to create corresponding Route 53 records.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/reactapp:v1</li> <li>reyanshkharga/nodeapp:v1</li> </ul> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul> <p>reyanshkharga/reactapp:v1 is a frontend app that runs on port <code>3000</code>.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#objective","title":"Objective","text":"<p>In this example we will have 2 microservices:</p> <ol> <li><code>backend</code>: uses docker image <code>reyanshkharga/nodeapp:v1</code></li> <li><code>frontend</code>: uses docker image <code>reyanshkharga/reactapp:v1</code></li> </ol> <p>We'll do the following:</p> <ol> <li>Create a deployment and service for <code>backend</code> microservice.</li> <li>Create a deployment and service for <code>frontend</code> microservice.</li> <li>Create a ingress that sends traffic to one of the microservices based on the host.</li> <li>We'll also provide separate ExternalDNS configuration for each microservice using <code>external-dns.alpha.kubernetes.io/hostname</code> annotation in the service definition of each microservice.</li> </ol>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#step-1-create-kubernetes-objects","title":"Step 1: Create Kubernetes Objects","text":"<p>Let's create the kubernetes objects as discussed above:</p> <code>backend.yml</code> <code>frontend.yml</code> <code>ingress.yml</code> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend-container\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    external-dns.alpha.kubernetes.io/hostname: api.example.com # Optional\nspec:\n  type: NodePort\n  selector:\n    app: backend\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend-container\n        image: reyanshkharga/reactapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    external-dns.alpha.kubernetes.io/hostname: app.example.com # Optional\nspec:\n  type: NodePort\n  selector:\n    app: frontend\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\nspec:\n  ingressClassName: alb\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-nodeport-service\n            port:\n              number: 5000\n  - host: app.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-nodeport-service\n            port:\n              number: 3000\n</code></pre> <p>Observe that we have provided the ExternalDNS configuration for each of the microservices using the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- backend.yml\n\u2502   |-- frontend.yml\n\u2502   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>This will create the following resources:</p> <ul> <li>Deployment and service for <code>backend</code> microservice.</li> <li>Deployment and service for <code>frontend</code> microservice.</li> <li>Ingress with two rules.</li> </ul>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#step-2-verify-kubernetes-objects","title":"Step 2: Verify Kubernetes Objects","text":"<pre><code># List pods\nkubectl get pods\n\n# List deployments\nkubectl get deployments\n\n# List services\nkubectl get svc\n\n# List ingress\nkubectl get ingress\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#step-3-verify-aws-resources-in-aws-console","title":"Step 3: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Also, go to AWS Route 53 and verify the records (<code>api.example.com</code> and <code>app.example.com</code>) that were added by ExternalDNS.</p> <p>You can also check the events that external-dns pod performs:</p> <pre><code>kubectl logs -f &lt;external-dns-pod&gt; -n external-dns\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#step-4-access-app-using-route-53-dns","title":"Step 4: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomains you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following hosts:</p> <pre><code># Backend\nhttp://api.example.com\n\n# Frontend\nhttp://app.example.com\n</code></pre> <p>Note</p> <p>For this demo, we have not enabled SSL to maintain the focus on the ExternalDNS annotation. However, you can add SSL-specific annotations to enable SSL if needed.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-multiple-hosts/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- backend.yml\n\u2502   |-- frontend.yml\n\u2502   |-- ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>The Route 53 records will also be deleted when the ingress or service is deleted.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/","title":"ExternalDNS Demo With One Host","text":"<p>Simply add the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation to either the kubernetes Ingress or Service, and ExternalDNS will use this information to create corresponding Route 53 records.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with ExternalDNS annotation:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    alb.ingress.kubernetes.io/target-type: instance # Optional\n    # external-dns specific configuration for creating route53 record-set\n    external-dns.alpha.kubernetes.io/hostname: api.example.com # give your domain name here (Optional)\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Be sure to replace the value of <code>external-dns.alpha.kubernetes.io/hostname</code> with your domain.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Also, go to AWS Route 53 and verify the record (<code>api.example.com</code>) that was added by ExternalDNS.</p> <p>You can also check the events that external-dns pod performs:</p> <pre><code>kubectl logs -f &lt;external-dns-pod&gt; -n external-dns\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#step-5-access-app-using-route-53-dns","title":"Step 5: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttp://api.example.com/\n\n# Health path\nhttp://api.example.com/health\n\n# Random generator path\nhttp://api.example.com/random\n</code></pre> <p>Note</p> <p>For this demo, we have not enabled SSL to maintain the focus on the ExternalDNS annotation. However, you can add SSL-specific annotations to enable SSL if needed.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-one-host/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>The Route 53 record will also be deleted when the ingress or service is deleted.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/","title":"ExternalDNS Demo With Private Hosted Zone","text":"<p>In Amazon Route 53, a private hosted zone is a DNS (Domain Name System) zone that is used for private internal DNS resolution within a Virtual Private Cloud (VPC) or a set of interconnected VPCs. It is separate from a public hosted zone, which is used for DNS resolution on the public internet.</p> <p>Let's see how ExternalDNS works with private hosted zones.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll need to create a private hosted zone in the VPC where EKS cluster was created. For example, you can create a private hosted zone called <code>example.internal</code>.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with ExternalDNS annotation:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internal # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    alb.ingress.kubernetes.io/target-type: instance # Optional\n    # external-dns specific configuration for creating route53 record-set\n    external-dns.alpha.kubernetes.io/hostname: api.example.internal # give your domain name here (Optional)\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Observe that we have set <code>alb.ingress.kubernetes.io/scheme</code> to <code>internal</code> and therefore the AWS Load Balancer Controller will create an internal load balancer. Also, ExternalDNS will create a Route 53 record record <code>api.example.internal</code> in the private hosted zone we created because we have set <code>external-dns.alpha.kubernetes.io/hostname</code> to <code>api.example.internal</code>.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Also, go to AWS Route 53 and verify the record (<code>api.example.internal</code>) that was added by ExternalDNS.</p> <p>You can also check the events that external-dns pod performs:</p> <pre><code>kubectl logs -f &lt;external-dns-pod&gt; -n external-dns\n</code></pre>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#step-5-access-app-using-internal-load-balancer-dns","title":"Step 5: Access App Using Internal Load Balancer DNS","text":"<p>Because the load balancer is internal, access to our app from outside the VPC is restricted. To overcome this, let's create a pod that we can use to access the load balancer and, in turn, our app. Since the pod will reside within the same VPC, we will be able to access our app.</p> <p>First, let's create a pod as follows:</p> <code>nginx-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>Apply the manifest to create the pod:</p> <pre><code>kubectl apply -f nginx-pod.yml\n</code></pre> <p>Now, let's start a shell session inside the nginx container and hit the private Route 53 DNS:</p> <pre><code># Start a shell session inside the nginx container\nkubectl exec -it nginx -- bash\n\n# Hit the url using CURL\ncurl api.example.internal\n</code></pre> <p>You'll see the response from the app.</p>"},{"location":"kubernetes-on-eks/external-dns/external-dns-demo-with-private-hosted-zone/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n\u2502   |-- nginx-pod.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>The Route 53 record will also be deleted when the ingress or service is deleted.</p>"},{"location":"kubernetes-on-eks/external-dns/install-external-dns/","title":"Install ExternalDNS","text":"<p>Let's see how you can install <code>ExternalDNS</code> in your EKS cluster.</p>"},{"location":"kubernetes-on-eks/external-dns/install-external-dns/#step-1-get-the-required-iam-policy","title":"Step 1: Get the Required IAM Policy","text":"<p>First, get the IAM Policy from official git repository. It should look something like this:</p> <code>external-dns-iam-policy.json</code> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>The above IAM policy allows <code>ExternalDNS</code> to update Route 53 Resource Record Sets and Hosted Zones. If you prefer, you may fine-tune the policy to permit updates only to explicit Hosted Zone IDs.</p>"},{"location":"kubernetes-on-eks/external-dns/install-external-dns/#step-2-create-iam-policy","title":"Step 2: Create IAM Policy","text":"<p>We need to create a policy in IAM first. We will name the policy <code>ExternalDNSIAMPolicy</code>. But you can name it anything that you prefer.</p> <pre><code>aws iam create-policy \\\n    --policy-name ExternalDNSIAMPolicy \\\n    --policy-document file://external-dns-iam-policy.json\n</code></pre> <p>Note down the <code>ARN</code> of the policy. We'll need it in the next section.</p>"},{"location":"kubernetes-on-eks/external-dns/install-external-dns/#step-3-create-iam-role-and-service-account","title":"Step 3: Create IAM Role and Service Account","text":"<p>We'll use IAM Roles for Service Accounts (IRSA) to grant <code>ExternalDNS</code> permission to AWS resources. So, let's create IRSA as follows:</p> <pre><code>eksctl create iamserviceaccount \\\n  --cluster my-cluster \\\n  --name external-dns \\\n  --namespace external-dns \\\n  --attach-policy-arn &lt;policy-arn&gt; \\\n  --approve\n</code></pre> <p>Please note that we have specified the namespace as <code>external-dns</code>, and as a result, the service account will be created within this namespace.</p> <p>Verify the service account:</p> <pre><code># List service accounts\nkubectl get sa -n external-dns\n\n# View the service account definition in yaml format\nkubectl get sa external-dns -n external-dns -o yaml\n\n# Describe the service account\nkubectl describe sa external-dns -n external-dns\n</code></pre> <p>Also, go to AWS console and verify the IAM role that was created. You can get the role name from the annotation in the service account that was created.</p>"},{"location":"kubernetes-on-eks/external-dns/install-external-dns/#step-4-install-externaldns","title":"Step 4: Install ExternalDNS","text":"<p>With the service account ready, we can now move forward with the installation of <code>ExternalDNS</code>.</p> <ol> <li> <p>Download the YAML manifest for ExternalDNS:</p> <pre><code># Download external-dns manifest\nwget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/external-dns.yaml\n</code></pre> </li> <li> <p>Update the YAML manifest:</p> <p>Now, before we proceed with the installation of this manifest, we need to make some modifications to it.</p> <p>We'll deploy all the resources in <code>external-dns</code> namespace. So, we need to make the following modifications to ensure that resources are created in the <code>external-dns</code> namespace:</p> <ul> <li>In <code>ClusterRoleBinding</code> object replace <code>namespace: default</code> with <code>namespace: external-dns</code> since we have created the service account in <code>external-dns</code> namespace.</li> <li>In <code>Deployment</code> object add <code>namespace: external-dns</code> so that the resources are deployed in <code>external-dns</code> namespace</li> <li><code>ClusterRole</code> and <code>ClusterRoleBinding</code> are not namespaced objects so we don't have to specify the namespace</li> </ul> <p>We'll also omit the <code>--domain-filter</code>, <code>--policy</code>, and <code>--aws-zone-type</code> because we want ExternalDNS to manage all the public and private hosted zones and enable full synchronization.</p> <p>The modified manifest should look something like this:</p> <code>external-dns.yml</code> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\", \"pods\", \"nodes\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\", \"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\n  labels:\n    app.kubernetes.io/name: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n    spec:\n      serviceAccountName: external-dns\n      securityContext:\n        fsGroup: 65534\n      containers:\n      - name: external-dns\n        image: bitnami/external-dns:0.13.1\n        # must specify env AWS_REGION in AWS china regions\n        # env:\n        # - name: AWS_REGION\n        #   value: cn-north-1\n        args:\n        - --source=service\n        - --source=ingress\n        # - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        # - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre> </li> <li> <p>Apply the manifest to install ExternalDNS:</p> <pre><code># Install external-dns\nkubectl apply -f external-dns.yaml\n</code></pre> </li> <li> <p>Verify ExternalDNS pods and view logs:</p> <pre><code># List external-dns pods\nkubectl get pods -n external-dns\n\n# View external-dns logs\nkubectl logs -f &lt;pod-name&gt; -n external-dns\n</code></pre> </li> </ol> <p>References:</p> <ul> <li>Required IAM Policy for External-DNS</li> <li>ExternalDNS Manifest</li> </ul>"},{"location":"kubernetes-on-eks/external-dns/introduction-to-external-dns/","title":"Introduction to ExternalDNS","text":"<p>In the previous sections, we had to manualy map a subdomain to a load balancer. Kubernetes <code>ExternalDNS</code> automates the creation, updation, and deletion of the Route 53 records.</p> <p><code>ExternalDNS</code> is a kubernetes tool that draws inspiration from kubernetes DNS and enhances resource discoverability through public DNS servers.</p> <p>Unlike kubernetes' internal DNS server, KubeDNS, <code>ExternalDNS</code> does not act as a DNS server in itself. Instead, it leverages the kubernetes API to gather a comprehensive list of resources, such as Services and Ingresses, and then configures external DNS providers, like AWS Route 53 or Google Cloud DNS, to create the desired DNS records.</p> <p>This functionality allows kubernetes resources to become readily accessible via public DNS servers, offering greater flexibility and integration for managing domain names in kubernetes environments.</p> <p><code>ExternalDNS</code> creates DNS records based on the host information. <code>ExternalDNS</code> sets up and manages records in Route 53 that point to controller deployed ALBs.</p> <p>For ingress objects <code>ExternalDNS</code> will create a DNS record based on the <code>hosts</code> specified for the ingress object, as well as the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation.</p> <p>For services <code>ExternalDNS</code> will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the loadbalancer DNS to create Route 53 record.</p> <p>References:</p> <ul> <li>External DNS</li> </ul>"},{"location":"kubernetes-on-eks/helm/create-your-own-helm-chart/","title":"Create Your Own Helm Chart","text":"<p>We'll discuss about the Chart file structure in the next section. But you can get started quickly by using the <code>helm create</code> command.</p> <pre><code># Command template\nhelm create &lt;chart-name&gt;\n\n# Example\nhelm create my-chart\n</code></pre> <p>A folder with the name <code>my-chart</code> will be created. You can edit it and create your own templates. For now we are not going to make any changes to that.</p> <p>As you edit your chart, you can validate that it is well-formed by running <code>helm lint</code> command as follows:</p> <pre><code># Command template\nhelm lint &lt;chart-name&gt;\n\n# Example\nhelm lint my-chart\n</code></pre>"},{"location":"kubernetes-on-eks/helm/create-your-own-helm-chart/#package-the-chart-for-distribution","title":"Package the Chart for Distribution","text":"<p>You can use the <code>helm package</code> command to package the chart for distribution:</p> <pre><code># Command template\nhelm package &lt;chart-name&gt;\n\n# Example\nhelm package my-chart\n</code></pre> <p>It will create a <code>.tgz</code> file that can be distributed.</p>"},{"location":"kubernetes-on-eks/helm/create-your-own-helm-chart/#install-the-chart","title":"Install the Chart","text":"<p>Now that you have the Chart in the <code>.tgz</code> format, you can use helm install command to install it.</p> <pre><code># Command template\nhelm install &lt;name&gt; &lt;path-to-tgz-file&gt;\n\n# Example\nhelm install my-chart ./my-chart-0.1.0.tgz\n</code></pre> <p>You can use <code>--namespace</code> or <code>-n</code> argument to deploy the resources in the desired namespace. Also, you can use <code>--set</code> argument to override the default values or you can also use the <code>--values</code> or <code>-f</code> flag, followed by the path to the YAML file.</p> <pre><code>helm install &lt;name&gt; &lt;path-to-tgz-file&gt; --set some.key=some.value --set some.other.key=some.other.value\n\n{OR}\n\nhelm install &lt;name&gt; &lt;path-to-tgz-file&gt; -f myvalues.yml\n\n{OR}\n\nhelm install &lt;name&gt; &lt;path-to-tgz-file&gt; --values myvalues.yml\n</code></pre> <p>This will create kubernetes objects defined in the chart. In our case a nginx deployment with 1 replica will be created. (Take a look at values.yaml file to find out why!!)</p> <p>You can see the values that you can override using the helm show values command:</p> <pre><code># Command template\nhelm show values &lt;chart&gt;\n\n# Example\nhelm show values ./my-chart-0.1.0.tgz\n</code></pre>"},{"location":"kubernetes-on-eks/helm/create-your-own-helm-chart/#upgrade-the-chart","title":"Upgrade the Chart","text":"<p>Let's upgrade the chart by changing the image tag to latest:</p> <pre><code>helm upgrade my-chart my-chart-0.1.0.tgz --set image.tag=latest\n</code></pre> <p>Start a shell session inside the container and verify that nginx is running:</p>"},{"location":"kubernetes-on-eks/helm/create-your-own-helm-chart/#uninstall-the-chart","title":"Uninstall the Chart","text":"<p>You can use <code>helm uninstall</code> command to uninstall a chart:</p> <pre><code>helm uninstall my-chart\n</code></pre> <p>Uninstalling a chart deletes all the kubernetes objects that were created as part of <code>helm install</code> command.</p>"},{"location":"kubernetes-on-eks/helm/helm-upgrade-and-rollback/","title":"Helm Upgrade and Rollback","text":"<p>Let's see how you can upgrade or rollback a helm chart release.</p>"},{"location":"kubernetes-on-eks/helm/helm-upgrade-and-rollback/#helm-upgrade","title":"Helm Upgrade","text":"<p>The <code>helm upgrade</code> command is used to upgrade an existing release to a new version.</p> <p>It takes the name of the release, the name of the chart to upgrade to, and any values or options to apply during the upgrade process.</p> <p>For example, assume you have a release named <code>myapp</code> and you want to upgrade it to a new version of the <code>myapp</code> chart. You can use the following command to upgrade the release:</p> <pre><code>helm upgrade myapp myapp-chart --set some.key=value\n</code></pre> <p>You can use <code>--set</code> multiple times to set multiple values.</p> <p>You can also use a YAML file instead of <code>--set</code> argument as discussed earlier.</p>"},{"location":"kubernetes-on-eks/helm/helm-upgrade-and-rollback/#helm-upgrade-flags","title":"Helm Upgrade Flags","text":"<p>During the helm upgrade, you have the option to reset values using the <code>--reset-values</code> flag or retain them with the <code>--reuse-values</code> flag.</p>"},{"location":"kubernetes-on-eks/helm/helm-upgrade-and-rollback/#reset-values","title":"Reset Values","text":"<p>The <code>--reset-values</code> flag resets the values to default, i.e, the values provided in the <code>values.yaml</code> file of the chart.</p> <p>For example, the following command will upgrade the chart by setting all values to default:</p> <pre><code>helm upgrade &lt;name&gt; &lt;chart&gt; --reset-values\n</code></pre>"},{"location":"kubernetes-on-eks/helm/helm-upgrade-and-rollback/#reuse-values","title":"Reuse Values","text":"<p>The <code>--reuse-values</code> allows you to reuse the previously set values.</p> <p>For example, let's say you want to upgrade the chart by setting <code>foo=bar</code>.</p> <pre><code>helm upgrade &lt;name&gt; &lt;chart&gt; --set foo=bar\n</code></pre> <p>The above command will upgrade the chart by setting all values to default and <code>foo=bar</code>. The previously set values will be lost.</p> <p>Instead, you can use <code>--reuse-values</code> flag to reuse the previously set values and set <code>foo=bar</code>.</p> <pre><code>helm upgrade &lt;name&gt; &lt;chart&gt; --set foo=bar --resue-values\n</code></pre>"},{"location":"kubernetes-on-eks/helm/helm-upgrade-and-rollback/#helm-rollback","title":"Helm Rollback","text":"<p>The <code>helm rollback</code> command allows you to roll back to a previous version of a release.</p> <p>For example, assume that you have a helm release named <code>myapp</code> and you want to roll back to the previous version.</p> <p>First, list the release history to see the available revisions:</p> <pre><code>helm history myapp\n</code></pre> <p>The above command will show a list of all revisions for the <code>myapp</code> release, along with the corresponding revision number.</p> <p>Next, choose the revision you want to roll back to and use the <code>helm rollback</code> command to perform the rollback:</p> <pre><code>helm rollback myapp &lt;revision-number&gt;\n</code></pre> <p>For example, if you want to roll back to revision 2 of the \"my-release\" release, you would use the following command:</p> <pre><code>helm rollback myapp 3\n</code></pre> <p>The above command will roll back the release to the state it was in at revision 3. Any changes made in subsequent revisions will be undone, and the release will be returned to the state it was in at revision 3.</p> <p>You can also use the <code>--wait</code> flag with the <code>helm rollback</code> command to wait for the rollout to be complete before exiting, and the <code>--force</code> flag to force resource updates through a <code>delete/recreate</code> process rather than the default rolling upgrade process.</p> <p>Note that you also need to provide <code>-n</code> or <code>--namespace</code> argument if <code>myapp</code> is not in the <code>default</code> namespace.</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/","title":"Introduction to Helm","text":"<p>Helm is the package manager for Kubernetes.</p> <p>This means you can package your kubernetes YAML manifest files and distribute them through public or private helm repositories.</p> <p>You know that:</p> <ul> <li><code>apt</code> is a package manager for Debian, and Debian-based Linux distributions, yum</li> <li><code>homebrew</code> is a package manager for Apple's operating system, macOS</li> </ul> <p>Helm does the similar job but for Kubernetes packages.</p> <p>A <code>Chart</code> is a Helm package. Think of it like the Kubernetes equivalent of a homebrew formula, an apt dpkg, or a yum RPM file.</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#example-use-case-of-helm-chart","title":"Example Use Case of Helm Chart","text":"<p>Consider you need to deploy prometheus in your EKS cluster. Prometheus is a stateful application and has a lot of components.</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#method-1-write-yaml-manifest-files-for-all-the-componets","title":"Method 1: Write YAML manifest files for all the componets","text":"<p>This is going to be a cumbersome task since you need to look for the manifest files for all components on the internet. You may need to edit all the manifest to meet your needs. For example you might want to change namespace in all the manifest.</p> <p>What about updates? What if something changed in prometheus architecture? Incorporating those changes in your manifest is going to be a nightmare.</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#method-2-use-helm-chart","title":"Method 2: Use Helm Chart","text":"<p>What if someone has already packaged all the required YAML manifests together and published it as a package (chart in helm) in some repository?</p> <p>This will make your life easier. You just need to install the chart and you are good to go.</p> <p>You can also replace values easily. For example, if you want to deploy the prometheus resources in a particular namesapce, you can do that just by setting the required placeholder with the value you desire. (Remember set parameter?)</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#install-helm-cli","title":"Install Helm CLI","text":"<p>See the installation guide to instlal <code>Helm CLI</code> on your operating system.</p> <p>Verify if helm is installed:</p> <pre><code>helm version\n</code></pre> <p>The output will be similar to the below:</p> <pre><code>version.BuildInfo{Version:\"v3.11.1\", GitCommit:\"293b50c65d4d56187cd4e2f390f0ada46b4c4737\", GitTreeState:\"clean\", GoVersion:\"go1.18.10\"}\n</code></pre> <p>Now, let's see how you can work with Helm CLI to install charts in your kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#prerequisites","title":"Prerequisites","text":"<p>You must have the following in place before you can start working with Helm:</p> <ul> <li>A Kubernetes Cluster</li> <li>A local configured copy of <code>kubectl</code></li> </ul>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#initialize-a-helm-chart-repository","title":"Initialize a Helm Chart Repository","text":"<p>Once you have <code>Helm CLI</code> installed, you can add a chart repository. A chart repository is a server that houses packaged charts.</p> <p>For example, <code>Helm hub</code> is the official public repository for Helm Charts.</p> <p>Let's add <code>bitnami</code> helm chart repository as follows:</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>You can see which repositories are configured using the following command:</p> <pre><code>helm repo list\n</code></pre> <p>Now that you have added the <code>bitnami</code> repo, you can search the charts that are published in that repository using <code>helm search</code> command as follows:</p> <pre><code># Command template\nhelm search repo &lt;repo-name&gt;\n\n# Example\nhelm search repo bitnami\n</code></pre> <p>It will output a list of Helm charts available in the <code>bitnami</code> Helm chart repository, along with details such as chart name, version, description, and other relevant information.</p>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#install-a-chart","title":"Install a Chart","text":"<p>Now that we have added the repository. We can install charts from that repository.</p> <ol> <li> <p>Update the repo to make sure we get the latest list of charts:</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Install the chart:</p> <pre><code># Command template to install a chart\nhelm install &lt;name&gt; &lt;chart&gt;\n\n# Example\nhelm install mysql bitnami/mysql \n</code></pre> <p>The charts will be installed in the <code>default</code> namespace. You can use the <code>--namespace</code> or <code>-n</code> to install it in a particular namespace.</p> <pre><code># Create namespace mysql\nkubectl create namespace mysql\n\n# Install mysql in mysql namespace\nhelm install mysql bitnami/mysql -n mysql\n</code></pre> <p>You can get a basic idea of the features of this MySQL chart by running the following command:</p> <pre><code>helm show chart bitnami/mysql -n &lt;namespace&gt;\n</code></pre> </li> </ol> <p>Helm does not wait until all of the resources are running before it exits. Many charts require Docker images that are over 600M in size, and may take a long time to install into the cluster.</p> <p>To keep track of a release's state, or to re-read configuration information, you can use helm status command as follows:</p> <pre><code>helm status mysql -n &lt;namespace&gt;\n</code></pre> <p>When installing Helm charts with the <code>helm install</code> command, the <code>--set</code> argument allows you to define values used in a chart\u2019s templates.</p> <p>For example, when installing prometheus using helm you can set the values as per your need as follows:</p> <pre><code># Helm install with set argument\nhelm install prometheus prometheus-community/prometheus --namespace prometheus --set server.persistentVolume.size=20Gi --set server.retention=15d\n</code></pre> <p>Alternatively, you can also set values using a custom YAML file:</p> <pre><code>helm install prometheus prometheus-community/prometheus --namespace prometheus --values my-values.yml\n</code></pre> <p>Your<code>my-values.yml</code> may look like the below:</p> <pre><code>server:\n    persistentVolume:\n        size: 20Gi\n    retention: 15d\n</code></pre>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#show-default-valuesyml-for-a-given-chart","title":"Show Default values.yml for a Given Chart","text":"<p>You can use the following commmand to view the default values for a chart:</p> <pre><code># Command template\nhelm show values &lt;chart&gt;\n\n# Example\nhelm show values prometheus-community/prometheus\n</code></pre>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#helm-releases","title":"Helm Releases","text":"<p>The <code>helm list</code> (or <code>helm ls</code>) command will show you a list of all deployed releases:</p> <pre><code># Command template\nhelm list -n &lt;namespace&gt;\n\n# List all deployed releases in default namespace\nhelm list\n\n# List all deployed releases in prometheus namespace\nhelm list -n prometheus\n</code></pre>"},{"location":"kubernetes-on-eks/helm/introduction-to-helm/#uninstall-a-release","title":"Uninstall a Release","text":"<p>Use the helm uninstall command to uninstall a release:</p> <pre><code># Command template\nhelm uninstall &lt;name&gt; -n namespace\n\n# Example\nhelm uninstall mysql\n</code></pre>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/","title":"Using Helm for Templating","text":"<p>Helm is a package manager for kubernetes that can also be used for templating kubernetes manifests.</p> <p>Helm templating provides a way to create more maintainable, scalable, and repeatable kubernetes manifests. By using templates, you can reduce the complexity of managing your kubernetes resources and make it easier to customize and reuse them.</p>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#benefits-of-helm-templating","title":"Benefits of Helm Templating","text":"<p>Reusability: Templating allows you to define reusable templates for kubernetes manifests, making it easier to create new resources that follow a certain pattern. By using variables and templates, you can create a single template that can be used to create multiple resources with different values.</p> <p>Customization: With templating, you can easily customize your kubernetes manifests for different environments or use cases. For example, you might have different values for development, staging, and production environments. By using templates, you can easily swap out values for different environments.</p> <p>Simplification: Kubernetes manifests can be complex and difficult to manage, especially as your applications grow. Templating allows you to abstract away some of the complexity and manage your resources at a higher level.</p> <p>Versioning: With Helm, you can version your templates and apply them to your kubernetes cluster. This provides an audit trail of changes made to your cluster and allows you to roll back to a previous version of your templates if needed.</p> <p>Now, let's see how you can write templates for your kubernetes manifests using helm.</p>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#step-1-use-helm-create-command-to-initialize-the-chart","title":"Step 1: Use Helm Create Command to Initialize the Chart","text":"<p>Initialize the chart:</p> <pre><code>helm create helm-chart\n</code></pre> <p>We have named our chart <code>helm-chart</code>.</p>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#step-2-delete-unnecessary-files","title":"Step 2: Delete Unnecessary Files","text":"<p>Delete <code>charts/</code> folder. Delete all files in <code>templates/</code> folder. (We'll write our own templates) Delete all files in the root folder except <code>Chart.yaml</code>.</p> <p>The <code>Chart.yaml</code> file contains metadata about the chart, such as its name, version, and description.</p> <p>Here's how <code>Chart.yaml</code> file looks like:</p> <code>Chart.yaml</code> <pre><code>apiVersion: v2\nname: helm-chart\ndescription: A Helm chart for Kubernetes\n\n# A chart can be either an 'application' or a 'library' chart.\n#\n# Application charts are a collection of templates that can be packaged into versioned archives\n# to be deployed.\n#\n# Library charts provide useful utilities or functions for the chart developer. They're included as\n# a dependency of application charts to inject those utilities and functions into the rendering\n# pipeline. Library charts do not define any templates and therefore cannot be deployed.\ntype: application\n\n# This is the chart version. This version number should be incremented each time you make changes\n# to the chart and its templates, including the app version.\n# Versions are expected to follow Semantic Versioning (https://semver.org/)\nversion: 0.1.0\n\n# This is the version number of the application being deployed. This version number should be\n# incremented each time you make changes to the application. Versions are not expected to\n# follow Semantic Versioning. They should reflect the version the application is using.\n# It is recommended to use it with quotes.\nappVersion: \"1.16.0\"\n</code></pre> <p>The <code>charts/</code> folder within the created chart directory is used to store charts that the current chart depends on. In our case we don't have any dependency on other charts and that's why we deleted it.</p>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#step-3-add-templates","title":"Step 3: Add Templates","text":"<p>Let's create templates for kubernetes deployment and service in the <code>templates/</code> folder.</p> <code>deployment.yaml</code> <code>service.yaml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.appName }}-deployment\nspec:\n  replicas: {{ .Values.replicas }}\n  selector:\n    matchLabels:\n      app: {{ .Values.appName }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.appName }}\n    spec:\n      containers:\n      - name: {{ .Values.appName }}\n        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}\n        ports:\n        - containerPort: {{ .Values.port }}\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.appName }}-service\nspec:\n  selector:\n    app: {{ .Values.appName }}\n  ports:\n  - name: http\n    port: {{ .Values.port }}\n    targetPort: {{ .Values.port }}\n  type: {{ .Values.serviceType }}\n</code></pre>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#step-4-create-valuesyaml-file","title":"Step 4: Create values.yaml File","text":"<p>Create <code>values.yaml</code> file for default values. Additionally, create <code>values.prod.yaml</code> for prod environment and <code>values.dev.yaml</code> for dev environment.</p> <code>values.yaml</code> <code>values.prod.yaml</code> <code>values.dev.yaml</code> <pre><code>replicas: 1\n\nappName: myapp\n\nport: 80\n\nimage:\n  repository: reyanshkharga/nodeapp\n  tag: v2\n\nserviceType: ClusterIP\n</code></pre> <pre><code>replicas: 2\n\nappName: myapp\n\nport: 80\n\nimage:\n  repository: reyanshkharga/nodeapp\n  tag: v1\n\nserviceType: ClusterIP\n</code></pre> <pre><code>replicas: 1\n\nappName: myapp\n\nport: 80\n\nimage:\n  repository: reyanshkharga/nodeapp\n  tag: v2\n\nserviceType: ClusterIP\n</code></pre> <p>At this point your folder structure should look like following:</p> <pre><code>|-- helm-chart\n\u2502   |-- templates\n\u2502   |   |-- deployment.yaml\n\u2502   |   |-- service.yaml\n\u2502   |-- templates\n\u2502   |-- Chart.yaml\n\u2502   |-- values.dev.yaml\n\u2502   |-- values.prod.yaml\n\u2502   |-- values.yaml\n</code></pre>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#step-5-install-chart","title":"Step 5: Install Chart","text":"<p>Now that we have the templates/chart ready, we can install it.</p> <pre><code># Install in default namespace with default values\nhelm install mychart helm-chart/\n\n# Install in dev namespace\nkubectl create ns dev\nhelm install mychart helm-chart/ --values helm-chart/values.dev.yaml --namespace dev\n\n# Install in prod namespace\nkubectl create ns prod\nhelm install mychart helm-chart/ --values helm-chart/values.prod.yaml --namespace prod\n</code></pre>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#step-6-verify-resources","title":"Step 6: Verify Resources","text":"<p>Verify the kubernetes objects that were created as part of the helm install process.</p> <p>First, let's verify the charts installed:</p> <pre><code># Verify chart in default namespace\nhelm list\n\n# Verify chart in dev namespace\nhelm list -n dev\n\n# Verify chart in prod namespace\nhelm list -n prod\n</code></pre> <p>Next, let's list the kubernetes resources in each namespace:</p> <pre><code># List kubernetes resources in default namespace\nkubectl get all\n\n# List kubernetes resources in dev namespace\nkubectl get all -n dev\n\n# List kubernetes resources in prod namespace\nkubectl get all -n prod\n</code></pre>"},{"location":"kubernetes-on-eks/helm/using-helm-for-templating/#clean-up","title":"Clean Up","text":"<p>Let's uninstall the charts:</p> <pre><code># List charts\nhelm list -n &lt;namespace-name&gt;\n\n# Uninstall chart\nhelm uninstall &lt;chart&gt; -n &lt;namespace-name&gt;\n\n# Example\nhelm uninstall mychart -n dev\n</code></pre> <p>Also, delete the namespaces we created:</p> <pre><code># Delete namespace dev\nkubectl delete ns dev\n\n# Delete namespace prod\nkubectl delete ns prod\n</code></pre>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/","title":"Create and Manage IAM Roles for Service Accounts","text":"<p>Let's see how we can create and manage IAM Roles for Service Accounts (IRSA).</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#step-1-create-a-service-account","title":"Step 1: Create a Service Account","text":"<p>Let's create a ordinary service account in the default namespace.</p> <code>my-ordinary-service-account.yml</code> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-ordinary-service-account\n</code></pre> <p>Apply the manifest to create the service account:</p> <pre><code>kubectl apply -f my-ordinary-service-account.yml\n</code></pre> <p>Verify the service account:</p> <pre><code>kubectl get sa\n</code></pre>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#step-2-create-pod-with-service-account","title":"Step 2: Create Pod With Service Account","text":"<p>Let's create pods that uses the ordinary service account we created. We'll use deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aws-cli\n  template:\n    metadata:\n      labels:\n        app: aws-cli\n    spec:\n      serviceAccountName: my-ordinary-service-account\n      containers:\n      - name: aws-cli\n        image: amazon/aws-cli\n        command: [\"sh\", \"-c\",  \"sleep 3600\"]\n</code></pre> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment-with-ordinary-sa.yml\n</code></pre> <p>Verify Deployment and Pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#step-3-access-aws-resources-from-within-pod","title":"Step 3: Access AWS Resources From Within Pod","text":"<p>Let's try to list S3 buckets from within a pod in the deployment:</p> <pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Verify if the aws-cli is installed\naws --version\n\n# List S3 buckets\naws s3 ls\n</code></pre> <p>You'll receive the following error:</p> <pre><code>An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied\n</code></pre> <p>This is because the pod doesn't have permission to access S3 buckets.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#step-4-create-iam-role-for-service-account","title":"Step 4: Create IAM Role for Service Account","text":"<p>We will use <code>eksctl</code> to create an IAM Role for the service account.</p> <p>You have the flexibility to either provide all the parameters directly in the command line or use the <code>--config-file</code> option to supply the parameters in the <code>eksctl</code> command.</p> <ul> <li> <p>Create IRSA without config file</p> <pre><code>eksctl create iamserviceaccount \\\n    --name service-account-for-s3-access \\\n    --namespace default \\\n    --cluster my-cluster \\\n    --role-name iam-role-for-service-account-with-s3-access \\\n    --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess \\\n    --override-existing-serviceaccounts \\\n    --approve\n</code></pre> </li> <li> <p>Create IRSA using config file</p> <p>First we need to create the config file as follows:</p> <code>irsa.yml</code> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: my-cluster\n  region: ap-south-1\n\niam:\n  withOIDC: true # Must be enabled explicitly for iam.serviceAccounts to be created\n  serviceAccounts:\n  - metadata:\n      name: service-account-for-s3-access\n      namespace: default\n    roleName: iam-role-for-service-account-with-s3-access\n    attachPolicyARNs:\n    - arn:aws:iam::aws:policy/AmazonS3FullAccess\n</code></pre> <p>Now, we can create the service account using eksctl as follows:</p> <pre><code>eksctl create iamserviceaccount --config-file=irsa.yml --override-existing-serviceaccounts --approve\n</code></pre> </li> </ul> <p>This will do the following for you:</p> <ol> <li>Create an IAM Role in AWS</li> <li>Create a service account in your Kubernetes cluster</li> <li>Annotate the service account with the IAM Role ARN</li> </ol> <p>Visit the AWS console and verify the IAM Role. Pay close attention to the trust policy associated with the IAM Role.</p> <p>The trust policy enables the IAM OIDC provider to assume a role with web identity, but only for the specified service account.</p> <p>Also, verify the service account created:</p> <pre><code># List service accounts\nkubectl get sa\n\n# Describe the service account\nkubectl describe sa &lt;service-account-name&gt;\n</code></pre> <p>You'll observe the service account we created has <code>eks.amazonaws.com/role-arn</code> annotation.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#step-5-update-the-service-account-name-in-deployment","title":"Step 5: Update the Service Account Name in Deployment","text":"<p>Let's update the deployment to use the newly created service account that is associated with an IAM role granting S3 permissions.</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aws-cli\n  template:\n    metadata:\n      labels:\n        app: aws-cli\n    spec:\n      serviceAccountName: service-account-for-s3-access\n      containers:\n      - name: aws-cli\n        image: amazon/aws-cli\n        command: [\"sh\", \"-c\",  \"sleep 3600\"]\n</code></pre> <p>Apply the manifest to update the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#step-6-retry-accessing-aws-resources-from-within-a-pod","title":"Step 6: Retry Accessing AWS Resources from Within a Pod","text":"<pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Verify if the aws-cli is installed\naws --version\n\n# List S3 buckets\naws s3 ls\n</code></pre> <p>This time, you will notice that you are able to list the S3 buckets because the service account associated with the pod is connected to an IAM Role that provides the necessary access to S3 buckets.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/create-iam-roles-for-service-accounts/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-ordinary-service-account.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, delete IAM Role for Service Account (IRSA) that we created:</p> <pre><code># Without config file\neksctl delete iamserviceaccount --name service-account-for-s3-access --cluster my-cluster\n\n{OR}\n\n# Using config file\neksctl delete iamserviceaccount --config-file=irsa.yml --approve\n</code></pre> <p>This will delete both the IAM Role and the service account that were created.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/","title":"How Does IRSA Work?","text":"<p>Before diving into how IRSA works, it's crucial to have a basic understanding of concepts like <code>OAuth 2.0</code>, <code>OIDC</code>, and identity providers.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#what-is-oauth-20","title":"What is OAuth 2.0?","text":"<p><code>OAuth 2.0</code> (short for \"Open Authorization\u201d) is an open standard protocol for access delegation.</p> <p>It allows third-party applications to access resources, such as user data, from a server or service without requiring the user's credentials.</p> <p>In simple terms, <code>OAuth 2.0</code> allows users to grant access to their data to third-party applications without giving away their usernames and passwords.</p> <p> </p> <p><code>OAuth 2.0</code> is widely used by many popular web services, such as Facebook, Google, and Twitter, to allow third-party applications to access their users' data.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#oauth-20-flow","title":"OAuth 2.0 Flow","text":"<p>Here's a visual representation of how OAuth 2.0 works:</p> <p> </p> <ol> <li> <p>The end-user (resource owner) expresses their intent to access protected resources (e.g., emails from Gmail) through the client application (e.g., CRED).</p> </li> <li> <p>The client redirects the user to the authorization server's authentication endpoint, which corresponds to the resource they want to access (e.g., Gmail for emails). At the authorization server, the user may be prompted to log in, and if consent is granted, the authorization server redirects the user back to the client with an authorization code.</p> </li> <li> <p>The client application exchanges the received authorization code and its credentials with the authorization server to obtain an access token.</p> </li> <li> <p>The client utilizes the access token to request protected resources on the user's behalf.</p> </li> <li> <p>Finally, the end-user (resource owner) is provided with their protected resources, such as a credit card statement from Gmail, which are made available through the client application.</p> </li> </ol>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#oauth-20-example","title":"OAuth 2.0 Example","text":"<p>Here's an example of OAuth 2.0 in which a client application requests access to emails from your Gmail account.</p> <p> </p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#what-is-openid-connect-oidc","title":"What is OpenID Connect (OIDC)?","text":"<p>OAuth 2.0 is designed only for authorization. OAuth is like giving a third-party application a key. The key is useful but it doesn\u2019t tell the third-party application who you are or anything about you.</p> <p>OpenID Connect (OIDC) is an identity layer built on top of the OAuth 2.0 framework. </p> <p>It adds an authentication layer to the OAuth 2.0 authorization flow, allowing users to authenticate themselves to a third-party application using an identity provider, such as Google, Facebook, or Microsoft. </p> <p>It allows third-party applications to verify the identity of the end-user and to obtain basic user profile information.</p> <p>Instead of a key, OIDC is like giving the third-party application a badge. The badge not only gives the third-party application permissions to access the authorized data but also provides some basic information about who you are, i.e. your identity.</p> <p>When an authorization server supports OIDC, it is called an identity provider. Some popular OIDC identity providers are Google, Microsoft, Okta, and Auth0.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#openid-connect-oidc-flow","title":"OpenID Connect (OIDC) Flow","text":"<p>Here's a visual representation of how OpenID Connect (OIDC) works:</p> <p> </p> <ol> <li> <p>The end-user (resource owner) wants to log in to a third-party shopping website (e.g., Amazon) using their Google account.</p> </li> <li> <p>The shopping website redirects the user to Google's OpenID Connect Provider (OP) for authentication. After successful authentication, Google's OP issues an ID Token and an Access Token.</p> </li> <li> <p>The user's browser receives the ID Token and Access Token.</p> </li> <li> <p>The third-party website uses the Access Token to access user's Google profile data from Google's resource server.</p> </li> <li> <p>Google's resource server provides user's Google profile data to the shopping website, enabling the user to log in and personalize his shopping experience.</p> </li> </ol> <p>Note</p> <p>In OIDC, the primary focus is on providing a secure and standardized way for users to log in and obtain identity-related information in addition to OAuth 2.0 features for resource access.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#openid-connect-oidc-example","title":"OpenID Connect (OIDC) Example","text":"<p>Here's an example of OIDC where you log in to a client application (such as Upwork) using your Google or Apple account.</p> <p> </p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#what-is-iam-oidc-provider","title":"What is IAM OIDC Provider","text":"<p>IAM OIDC provider is an identity provider that supports OIDC.</p> <p>It enables kubernetes clusters to authenticate AWS IAM users and roles using OIDC (OpenID Connect) tokens.</p> <p>The IAM OIDC provider establishes a trust relationship between kubernetes and IAM by acting as a bridge between the kubernetes API server and IAM. It allows kubernetes to authenticate with AWS and obtain temporary security credentials that can be used to access AWS resources.</p> <p>When a kubernetes service account needs to access AWS resources, such as an Amazon S3 bucket or a DynamoDB table, it requires an AWS Identity and Access Management (IAM) role that grants the necessary permissions. However, kubernetes service accounts do not have AWS credentials by default, and cannot assume IAM roles directly.</p> <p>To solve this problem, EKS allows you to associate a kubernetes service account with an IAM role using an IAM OIDC provider. The IAM OIDC provider enables kubernetes service accounts to assume IAM roles, and access AWS resources securely and with appropriate permissions.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/how-does-irsa-work/#how-does-irsa-work_1","title":"How Does IRSA Work?","text":"<p>Here's a visual representation of how IRSA works:</p> <p> </p> <ol> <li> <p>The AWS SDK in the Pod requests the AWS Security Token Service (AWS STS) service for a temproray credentials. The Pod provides the JWT Token and the IAM Role ARN to STS for validation.</p> </li> <li> <p>The AWS STS service sends a request to the IAM service to validate whether it can issue temporary credentials to the Pod. The STS includes the JWT token and IAM Role ARN in the request to the IAM service for validation.</p> </li> <li> <p>The AWS IAM service, which trusts the IAM OIDC provider, retrieves the public keys from the JWKS (JSON Web Key Set) URL by accessing the /.well-known/OpenId-configuration endpoint. It then uses these keys to verify the authenticity and integrity of the received JWT token.</p> </li> <li> <p>The AWS IAM service validates the request and notifies the AWS STS service, indicating that the request is valid and authorized.</p> </li> <li> <p>The AWS STS service issues the temporary credentials to the POD. These temporary credentials grant the Pod access to the specified IAM role and associated AWS resources for a limited duration.</p> </li> <li> <p>The Pod can now access the desired AWS service, such as S3, within the boundaries set by the IAM policies associated with the IAM role.</p> </li> </ol> <p>References:</p> <ul> <li>How Does IRSA Work?</li> </ul>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/introduction-to-iam-roles-for-service-accounts/","title":"Introduction to IAM Roles for Service Accounts (IRSA)","text":"<p>In the world of AWS and Kubernetes, applications in container pods need to access AWS services securely. This is typically done by using AWS SDK or CLI with IAM permissions. To simplify this process, IAM roles for service accounts (IRSA) come into play.</p> <p>Let's dive in!</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/introduction-to-iam-roles-for-service-accounts/#approaches-to-grant-pods-access-to-aws-resources","title":"Approaches to Grant Pods Access to AWS Resources","text":"<p>There are two ways to grant a pod the required AWS permissions so that it can access AWS resources:</p> <ol> <li> <p>Through the IAM Role attached to the worker node the pod is running on:</p> <p>Pods inherit the IAM permissions from the worker node it is running on. For example, if the role attached to worker node has S3 permissions, pods running on that node will also also have the S3 permissions and can access S3 buckets.</p> <p>You can test this by adding S3 permissions to the role attached to the worker node where the pod is running.</p> <p>This method is not recommended since all the pods will inherit the same permissions even if the pods don't need it and it might pose security risks.</p> </li> <li> <p>Through a Service Account that is mapped to an AWS IAM Role:</p> <p>You can create an IAM role for service account and then annotate the kubernetes service account by adding <code>eks.amazonaws.com/role-arn</code> field in the <code>.metadata.annotations</code> section of the service account definition.</p> <p>When you create IAM role for service account using <code>eksctl</code> it does everything for you. It creates AWS IAM role, kubernetes service account and also adds the annotation in the service account.</p> </li> </ol>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/introduction-to-iam-roles-for-service-accounts/#what-is-an-iam-role-for-service-accounts-irsa","title":"What is an IAM Role for Service Accounts (IRSA)?","text":"<p>Amazon EKS supports IAM Roles for Service Accounts (IRSA) that allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts.</p> <p>The IAM role determines the permissions that the pods associated with the service account will have when interacting with AWS services.</p> <p>This provides fine-grained permission management for apps that run on EKS and use other AWS services such as S3, RDS, SQS, DynamoDB, and Kubernetes components like AWS Load Balancer controller or ExternalDNS.</p> <p>In summary, IRSA enables you to manage permissions for your pods using familiar AWS IAM roles and policies, providing fine-grained control over accessing AWS resources from within your EKS cluster.</p>"},{"location":"kubernetes-on-eks/iam-roles-for-service-accounts/introduction-to-iam-roles-for-service-accounts/#how-do-pods-use-irsa-to-access-aws-resources","title":"How Do Pods Use IRSA to Access AWS Resources?","text":"<p>To use IRSA, you create an IAM role in AWS with the necessary permissions, and then annotate the kubernetes service account with the IAM role's ARN to allow pods to securely access AWS resources without explicit credentials.</p> <p> </p> <p>References:</p> <ul> <li>IAM Roles for Service Accounts</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/","title":"Create Ingress With Default Backend","text":"<p>A <code>defaultBackend</code> is often configured in an Ingress controller to service any requests that do not match a path in the <code>spec</code>.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the Deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#step-2-create-a-nodeport-service","title":"Step 2: Create a NodePort Service","text":"<p>Let's create a NodePort service as follows:</p> <code>my-nodeport-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the NodePort service:</p> <pre><code>kubectl apply -f my-nodeport-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\nspec:\n  ingressClassName: alb\n  defaultBackend:\n    service:\n      name: my-nodeport-service\n      port:\n        number: 5000\n  rules:\n  - http:\n      paths:\n      - path: /yXnKHUoJGt\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Observe the following:</p> <ol> <li>We have used annotations to specify load balancer and target group attributes</li> <li>We have one rule that matches <code>/yXnKHUoJGt</code> path and then routes traffic to default backend which in this case is <code>my-nodeport-service</code>.</li> </ol> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the listener rules. You will observe a default listener rule that was set using <code>defaultBackend</code>.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#troubleshooting","title":"Troubleshooting","text":"<p>If you don't see the load balancer in the AWS console, this means the ingress has some issue. To identify the underlying issue, you can examine the logs of the controller as follows:</p> <pre><code># Describe the ingress\nkubectl describe ing my-ingress\n\n# View aws load balancer controller logs\nkubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-default-backend/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-nodeport-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>Default Backend in Ingress</li> <li>Path Types in Ingress</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/","title":"Create Ingress With Health Check","text":"<p>Health check on target groups can be controlled with following annotations:</p> Annotation Function <code>alb.ingress.kubernetes.io/healthcheck-protocol</code> specifies the protocol used when performing health check on targets. <code>alb.ingress.kubernetes.io/healthcheck-port</code> specifies the port used when performing health check on targets. When using <code>target-type: instance</code> with a service of type <code>NodePort</code>, the healthcheck port can be set to <code>traffic-port</code> to automatically point to the correct port. <code>alb.ingress.kubernetes.io/healthcheck-path</code> specifies the HTTP path when performing health check on targets. <code>alb.ingress.kubernetes.io/healthcheck-interval-seconds</code> specifies the interval(in seconds) between health check of an individual target. <code>alb.ingress.kubernetes.io/healthcheck-timeout-seconds</code> specifies the timeout(in seconds) during which no response from a target means a failed health check. <code>alb.ingress.kubernetes.io/success-codes</code> specifies the HTTP or gRPC status code that should be expected when doing health checks against the specified health check path. <code>alb.ingress.kubernetes.io/healthy-threshold-count</code> specifies the consecutive health checks successes required before considering an unhealthy target healthy. <code>alb.ingress.kubernetes.io/unhealthy-threshold-count</code> specifies the consecutive health check failures required before considering a target unhealthy."},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#annotation-format","title":"Annotation Format","text":"<p>Annotation keys and values can only be strings. Advanced format should be encoded as below:</p> <pre><code>boolean: 'true' # Must be quoted\ninteger: '42' # Must be quoted\nstringList: s1,s2,s3\nstringMap: k1=v1,k2=v2\njson: 'jsonContent' # Must be quoted\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#step-2-create-a-nodeport-service","title":"Step 2: Create a NodePort Service","text":"<p>Let's create a <code>NodePort</code> service as follows:</p> <code>my-nodeport-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the NodePort service:</p> <pre><code>kubectl apply -f my-nodeport-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>If you don't explicitly provide a <code>nodePort</code>, you'll observe that the service is automatically assigned one. However, if desired, you can specify a specific <code>nodePort</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with health check:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Observe the following:</p> <ol> <li>We have used annotations to specify load balancer and target group attributes</li> <li>We have one rule that matches <code>/</code> path and then routes traffic to <code>my-nodeport-service</code></li> <li>We have specified health check parameters for the target group</li> </ol> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the health check configuration of the target group that ingress created.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#step-5-access-app-via-load-balancer-dns","title":"Step 5: Access App Via Load Balancer DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the load balancer DNS and verify if everything is working properly.</p> <p>Access the load balancer DNS by entering it in your browser. You can get the load balancer DNS either from the AWS console or the Ingress configuration.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\n&lt;load-balancer-dns&gt;/\n\n# Health path\n&lt;load-balancer-dns&gt;/health\n\n# Random generator path\n&lt;load-balancer-dns&gt;/random\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#troubleshooting","title":"Troubleshooting","text":"<p>If you don't see the load balancer in the AWS console, this means the ingress has some issue. To identify the underlying issue, you can examine the logs of the controller as follows:</p> <pre><code># Describe the ingress\nkubectl describe ing my-ingress\n\n# View aws load balancer controller logs\nkubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-health-check/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-nodeport-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>Health Check Annotations</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/","title":"Create Ingress With IngressGroup","text":"<p><code>IngressGroup</code> feature enables you to group multiple Ingress resources together. The controller will automatically merge Ingress rules for all Ingresses within <code>IngressGroup</code> and support them with a single ALB.</p> <p>The <code>alb.ingress.kubernetes.io/group.name</code> annotation specifies the group name that this Ingress belongs to.</p> <p>The <code>alb.ingress.kubernetes.io/group.order</code> annotation specifies the order across all Ingresses within <code>IngressGroup</code>.</p> <p>By default, Ingresses don't belong to any <code>IngressGroup</code>, and we treat it as a \"implicit IngressGroup\" consisting of the Ingress itself.</p> <p>Ingresses with same <code>group.name</code> annotation will form an \"explicit IngressGroup\".</p> <p>Rules with the same order are sorted lexicographically by the Ingress\u2019s namespace/name.</p> <p>Warning</p> <p>If you turn your Ingress to belong a \"explicit IngressGroup\" by adding <code>group.name</code> annotation, other kubernetes users may create/modify their Ingresses to belong to the same <code>IngressGroup</code>, and can thus add more rules or overwrite existing rules with higher priority to the ALB for your Ingress.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/reactapp:v1</li> <li>reyanshkharga/nodeapp:v1</li> </ul> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul> <p>reyanshkharga/reactapp:v1 is a frontend app that runs on port <code>3000</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#objective","title":"Objective","text":"<p>In this example we will have 2 microservices:</p> <ol> <li><code>backend</code>: uses docker image <code>reyanshkharga/nodeapp:v1</code></li> <li><code>frontend</code>: uses docker image <code>reyanshkharga/reactapp:v1</code></li> </ol> <p>We'll do the following:</p> <ol> <li>Create a deployment and service for <code>backend</code> microservice.</li> <li>Create a deployment and service for <code>frontend</code> microservice.</li> <li>Create a ingress with ingress group and order for <code>backend</code> microservice.</li> <li>Create a ingress with ingress group and order for <code>frontend</code> microservice.</li> </ol>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#step-1-create-kubernetes-objects","title":"Step 1: Create Kubernetes Objects","text":"<p>Let's create the kubernetes objects as discussed above:</p> <code>backend.yml</code> <code>frontend.yml</code> <code>backend-ingress.yml</code> <code>frontend-ingress.yml</code> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend-container\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /health\nspec:\n  type: NodePort\n  selector:\n    app: backend\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend-container\n        image: reyanshkharga/reactapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /\nspec:\n  type: NodePort\n  selector:\n    app: frontend\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: backend-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\n    alb.ingress.kubernetes.io/group.order: '1'\nspec:\n  ingressClassName: alb\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-nodeport-service\n            port:\n              number: 5000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\n    alb.ingress.kubernetes.io/group.order: '0'\nspec:\n  ingressClassName: alb\n  rules:\n  - host: app.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-nodeport-service\n            port:\n              number: 3000\n</code></pre> <p>Notice that we have defined the <code>group.name</code> and <code>group.order</code> annotations for both the ingress. Having same <code>group.name</code> will ensure that a single load balancer is created and shared by both the ingress.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- backend.yml\n\u2502   |-- frontend.yml\n\u2502   |-- backend-ingress.yml\n\u2502   |-- frontend-ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>This will create the following resources:</p> <ul> <li>Deployment and service for <code>backend</code> microservice.</li> <li>Deployment and service for <code>frontend</code> microservice.</li> <li>Ingress for <code>backend</code> and <code>frontend</code> microservices.</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#step-2-verify-kubernetes-objects","title":"Step 2: Verify Kubernetes Objects","text":"<pre><code># List pods\nkubectl get pods\n\n# List deployments\nkubectl get deployments\n\n# List services\nkubectl get svc\n\n# List ingress\nkubectl get ingress\n</code></pre> <p>Go to the AWS Console and verify the resources created by the AWS Load Balancer Controller, including the load balancer, target groups, listener rules, etc.</p> <p>You will observe that only one load balancer was created with two rules, following the ordering defined by the <code>group.order</code> annotation in the ingress.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#step-3-add-records-in-route-53","title":"Step 3: Add Records in Route 53","text":"<p>Go to AWS Route 53 and add two <code>A</code> records (<code>api.example.com</code> and <code>app.example.com</code>) that points to the load balancer that was created. You can use alias to point the subdomain to the load balancer.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#step-4-access-app-using-route-53-dns","title":"Step 4: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomains you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following hosts:</p> <pre><code># Backend\nhttps://api.example.com\n\n# Frontend\nhttps://app.example.com\n</code></pre> <p>Also, verify that <code>HTTP</code> is redirected to <code>HTTPS</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ingressgroup/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- backend.yml\n\u2502   |-- frontend.yml\n\u2502   |-- backend-ingress.yml\n\u2502   |-- frontend-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, go to Route 53 and delete the <code>A</code> records that you created.</p> <p>References:</p> <ul> <li>IngressGroup</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/","title":"Create Ingress With Instance Mode","text":"<p>You can use <code>alb.ingress.kubernetes.io/target-type</code> annotation in the Ingress object to specify how to route traffic to pods. You can choose between <code>instance</code> and <code>ip</code>.</p> <p>The kubernetes service must be of type <code>NodePort</code> to use <code>instance</code> mode. This is because worker nodes (EC2 instances) are registered as targets in the target group that will be created by the AWS Load Balancer Controller.</p> <p>The default value for <code>alb.ingress.kubernetes.io/target-type</code> is <code>instance</code>. So you don't have to define this explicitly unless you want to use <code>ip</code> mode.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#step-2-create-a-nodeport-service","title":"Step 2: Create a NodePort Service","text":"<p>The kubernetes service must be of type <code>NodePort</code> to use <code>instance</code> mode. So, let's create a <code>NodePort</code> service as follows:</p> <code>my-nodeport-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the NodePort service:</p> <pre><code>kubectl apply -f my-nodeport-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>If you don't explicitly provide a <code>nodePort</code>, you'll observe that the service is automatically assigned one. However, if desired, you can specify a specific <code>nodePort</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Project=eks-masterclass,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    alb.ingress.kubernetes.io/target-type: instance # Default is instance\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 80\n</code></pre> <p>Observe the following:</p> <ol> <li>We have used annotations to specify load balancer and target group attributes</li> <li>We have one rule that matches <code>/</code> path and then routes traffic to <code>my-nodeport-service</code></li> </ol> <p>Note</p> <p>Before the <code>IngressClass</code> resource and <code>ingressClassName</code> field were added in Kubernetes 1.18, Ingress classes were specified with a <code>kubernetes.io/ingress.class</code> annotation on the Ingress. This annotation was never formally defined, but was widely supported by Ingress controllers.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre> <p>Here's what happens when you create an ingress:</p> <ol> <li>An ALB (ELBv2) is created in AWS for the new ingress resource.</li> <li>Target Groups are created in AWS for each unique kubernetes service described in the ingress resource.</li> <li>Listeners are created for every port detailed the ingress resource annotations.</li> <li>Listener rules are created for each path specified in the ingress resource. This ensures traffic to a specific path is routed to the correct kubernetes service.</li> </ol> <p>This is all done by the <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre> <p>You can see the events such as <code>creating securityGroup</code>, <code>created securityGroup</code>, <code>creating loadBalancer</code>, <code>created loadBalancer</code>, <code>created listener</code>, <code>created listener rule</code>, <code>creating targetGroupBinding</code>, <code>created targetGroupBinding</code>, <code>successfully deployed model</code>, etc in the logs.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the <code>Listeners</code>, <code>Rules</code> and <code>TargetGroups</code>.</p> <p>You will observe that in the Target Group, instances are registered as targets because we chose <code>instance</code> as target type.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#step-5-access-app-via-load-balancer-dns","title":"Step 5: Access App Via Load Balancer DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the load balancer DNS and verify if everything is working properly.</p> <p>Access the load balancer DNS by entering it in your browser. You can get the load balancer DNS either from the AWS console or the Ingress configuration.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\n&lt;load-balancer-dns&gt;/\n\n# Health path\n&lt;load-balancer-dns&gt;/health\n\n# Random generator path\n&lt;load-balancer-dns&gt;/random\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#troubleshooting","title":"Troubleshooting","text":"<p>If you don't see the load balancer in the AWS console, this means the ingress has some issue. To identify the underlying issue, you can examine the logs of the controller as follows:</p> <pre><code># Describe the ingress\nkubectl describe ing my-ingress\n\n# View aws load balancer controller logs\nkubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-instance-mode/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-nodeport-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/","title":"Create Ingress With Internal Load Balancer","text":"<p>You can create an internal load balancer to distribute traffic to your EC2 instances from clients with access to the VPC for the load balancer.</p> <p>An internal load balancer routes requests to targets using private IP addresses.</p> <p>You can set <code>alb.ingress.kubernetes.io/scheme</code> to <code>internal</code> to instruct AWS Load Balancer Controller to create an internal application load balancer.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object that creates an internal load balancer:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internal # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    alb.ingress.kubernetes.io/target-type: instance # Default is instance\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Note that we have set the value of <code>alb.ingress.kubernetes.io/scheme</code> to <code>internal</code> so that the Load Balancer Controller creates an internal load balancer.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the type of load balancer. It should be <code>internal</code>.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#step-5-access-app-using-internal-load-balancer-dns","title":"Step 5: Access App Using Internal Load Balancer DNS","text":"<p>Because the load balancer is internal, access to our app from outside the VPC is restricted. To overcome this, let's create a pod that we can use to access the load balancer and, in turn, our app. Since the pod will reside within the same VPC, we will be able to access our app.</p> <p>First, let's create a pod as follows:</p> <code>nginx-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>Apply the manifest to create the pod:</p> <pre><code>kubectl apply -f nginx-pod.yml\n</code></pre> <p>Now, let's start a shell session inside the nginx container and hit the internal load balancer url:</p> <pre><code># Start a shell session inside the nginx container\nkubectl exec -it nginx -- bash\n\n# Hit the load balancer url using CURL\ncurl &lt;internal-alb-dns&gt;\n</code></pre> <p>You'll see the response from the app.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-internal-load-balancer/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n\u2502   |-- nginx-pod.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/","title":"Create Ingress With IP Mode","text":"<p>You can use <code>alb.ingress.kubernetes.io/target-type</code> annotation in the Ingress object to specify how to route traffic to pods. You can choose between <code>instance</code> and <code>ip</code>.</p> <p>The default value for <code>alb.ingress.kubernetes.io/target-type</code> is <code>instance</code>. So, you must define this explicitly if you want to use <code>ip</code> mode.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>The kubernetes service can be <code>NodePort</code> or <code>ClusterIP</code> to use <code>ip</code> mode. So, let's create a <code>ClusterIP</code> service since it is more secure:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: ClusterIP\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Project=eks-masterclass,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    alb.ingress.kubernetes.io/target-type: ip # Default is instance\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-service\n            port:\n              number: 80\n</code></pre> <p>Observe the following:</p> <ol> <li>We have used annotations to specify load balancer and target group attributes</li> <li>We have one rule that matches <code>/</code> path and then routes traffic to <code>my-service</code></li> </ol> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the <code>Listeners</code>, <code>Rules</code> and <code>TargetGroups</code>.</p> <p>You will observe that in the Target Group, IPs are registered as targets because we chose <code>ip</code> as target type. These IPs are associated with pods that <code>my-service</code> is configured to serve the traffic from.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#step-5-access-app-via-load-balancer-dns","title":"Step 5: Access App Via Load Balancer DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the load balancer DNS and verify if everything is working properly.</p> <p>Access the load balancer DNS by entering it in your browser. You can obtain the load balancer DNS either from the AWS console or the Ingress configuration.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\n&lt;load-balancer-dns&gt;/\n\n# Health path\n&lt;load-balancer-dns&gt;/health\n\n# Random generator path\n&lt;load-balancer-dns&gt;/random\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#troubleshooting","title":"Troubleshooting","text":"<p>If you don't see the load balancer in the AWS console, this means the ingress has some issue. To identify the underlying issue, you can examine the logs of the controller as follows:</p> <pre><code># Describe the ingress\nkubectl describe ing my-ingress\n\n# View aws load balancer controller logs\nkubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ip-mode/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/","title":"Create Ingress With Multiple Hosts","text":"<p>You can use the <code>host</code> field to match the host in the rules. A listener rule will be created for each of the hosts defined in ingress rules.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/reactapp:v1</li> <li>reyanshkharga/nodeapp:v1</li> </ul> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul> <p>reyanshkharga/reactapp:v1 is a frontend app that runs on port <code>3000</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#objective","title":"Objective","text":"<p>In this example we will have 2 microservices:</p> <ol> <li><code>backend</code>: uses docker image <code>reyanshkharga/nodeapp:v1</code></li> <li><code>frontend</code>: uses docker image <code>reyanshkharga/reactapp:v1</code></li> </ol> <p>We'll do the following:</p> <ol> <li>Create a deployment and service for <code>backend</code> microservice.</li> <li>Create a deployment and service for <code>frontend</code> microservice.</li> <li>Create a ingress that sends traffic to one of the microservices based on the host.</li> <li>We'll also provide separate health check path for each microservice using <code>alb.ingress.kubernetes.io/healthcheck-path</code> annotation in the service definition of each microservice.</li> </ol>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#step-1-create-kubernetes-objects","title":"Step 1: Create Kubernetes Objects","text":"<p>Let's create the kubernetes objects as discussed above:</p> <code>backend.yml</code> <code>frontend.yml</code> <code>ingress.yml</code> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend-container\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /health\nspec:\n  type: NodePort\n  selector:\n    app: backend\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: frontend-container\n        image: reyanshkharga/reactapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /\nspec:\n  type: NodePort\n  selector:\n    app: frontend\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\nspec:\n  ingressClassName: alb\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-nodeport-service\n            port:\n              number: 5000\n  - host: app.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-nodeport-service\n            port:\n              number: 3000\n</code></pre> <p>Observe the following:</p> <ol> <li>We've provided the health check paths for each of the microservices using the <code>alb.ingress.kubernetes.io/healthcheck-path</code> annotation.</li> <li>In the ingress, we have used <code>host</code> in the rules to route traffic to a particular microservice based on matching host.</li> <li>We haven't provided <code>certificate-arn</code> because SSL discovery would work via host.</li> </ol> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- backend.yml\n\u2502   |-- frontend.yml\n\u2502   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>This will create the following resources:</p> <ul> <li>Deployment and service for <code>backend</code> microservice.</li> <li>Deployment and service for <code>frontend</code> microservice.</li> <li>Ingress with two rules.</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#step-2-verify-kubernetes-objects","title":"Step 2: Verify Kubernetes Objects","text":"<pre><code># List pods\nkubectl get pods\n\n# List deployments\nkubectl get deployments\n\n# List services\nkubectl get svc\n\n# List ingress\nkubectl get ingress\n</code></pre> <p>Also, go to the AWS Console and verify the resources created by the AWS Load Balancer Controller, including the load balancer, target groups, listener rules, etc.</p> <p>Pay close attention to the listener rules that were created. You will notice that, based on the host header, traffic is directed to a particular microservice.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#step-3-add-records-in-route-53","title":"Step 3: Add Records in Route 53","text":"<p>Go to AWS Route 53 and add two <code>A</code> records (<code>api.example.com</code> and <code>app.example.com</code>) that points to the load balancer that was created. You can use alias to point the subdomain to the load balancer.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#step-4-access-app-using-route-53-dns","title":"Step 4: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomains you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following hosts:</p> <pre><code># Backend\nhttps://api.example.com\n\n# Frontend\nhttps://app.example.com\n</code></pre> <p>Also, verify that <code>HTTP</code> is redirected to <code>HTTPS</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-multiple-hosts/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- backend.yml\n\u2502   |-- frontend.yml\n\u2502   |-- ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, go to Route 53 and delete the <code>A</code> records that you created.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/","title":"Create Ingress With Path Based Routing","text":"<p>Path-based routing is one of the features offered by Application Load Balancer.</p> <p>The Application Load Balancer forwards the requests to the specific targets based on the rules configured in the load balancer.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#overriding-health-check-configuration","title":"Overriding Health Check Configuration","text":"<p>Since each target in this case might have different health check requirements, you can override the ingress health check configurations by adding the same health check annotations in kubernetes service definition.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/reactapp:profile</li> <li>reyanshkharga/reactapp:notifications</li> <li>reyanshkharga/reactapp:feed</li> </ul> <p>Note</p> <p>All microservices (<code>profile</code>, <code>notifications</code>, and <code>feed</code>) run on port <code>3000</code>.</p> <ul> <li>reyanshkharga/reactapp:profile is a react app that serves request on <code>/profile</code> path.</li> <li>reyanshkharga/reactapp:notifications is a react app that serves request on <code>/notifications</code> path.</li> <li>reyanshkharga/reactapp:feed is a react app that serves request on <code>/feed</code> path.</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#objective","title":"Objective","text":"<p>In this example we will have 3 microservices:</p> <ol> <li><code>profile</code>: uses docker image <code>reyanshkharga/reactapp:profile</code></li> <li><code>notifications</code>: uses docker image <code>reyanshkharga/reactapp:notifications</code></li> <li><code>feed</code>: uses docker image <code>reyanshkharga/reactapp:feed</code></li> </ol> <p>We'll do the following:</p> <ol> <li>Create a deployment and service for <code>profile</code> microservice.</li> <li>Create a deployment and service for <code>notifications</code> microservice.</li> <li>Create a deployment and service for <code>feed</code> microservice.</li> <li>Create a ingress that sends traffic to one of the microservices based on the path.</li> <li>We'll also provide separate health check path for each microservice using <code>alb.ingress.kubernetes.io/healthcheck-path</code> annotation in the service definition of each microservice.</li> </ol>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#step-1-create-kubernetes-objects","title":"Step 1: Create Kubernetes Objects","text":"<p>Let's create the kubernetes objects as discussed above:</p> <code>profile.yml</code> <code>notifications.yml</code> <code>feed.yml</code> <code>ingress.yml</code> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: profile-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: profile\n  template:\n    metadata:\n      labels:\n        app: profile\n    spec:\n      containers:\n      - name: profile-container\n        image: reyanshkharga/reactapp:profile\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: profile-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /profile\nspec:\n  type: NodePort\n  selector:\n    app: profile\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: notifications-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: notifications\n  template:\n    metadata:\n      labels:\n        app: notifications\n    spec:\n      containers:\n      - name: notifications-container\n        image: reyanshkharga/reactapp:notifications\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: notifications-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /notifications\nspec:\n  type: NodePort\n  selector:\n    app: notifications\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: feed-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: feed\n  template:\n    metadata:\n      labels:\n        app: feed\n    spec:\n      containers:\n      - name: feed-container\n        image: reyanshkharga/reactapp:feed\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n# Service\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: feed-nodeport-service\n  annotations:\n    alb.ingress.kubernetes.io/healthcheck-path: /feed\nspec:\n  type: NodePort\n  selector:\n    app: feed\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /profile\n        pathType: Prefix\n        backend:\n          service:\n            name: profile-nodeport-service\n            port:\n              number: 3000\n  - http:\n      paths:\n      - path: /notifications\n        pathType: Prefix\n        backend:\n          service:\n            name: notifications-nodeport-service\n            port:\n              number: 3000\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: feed-nodeport-service\n            port:\n              number: 3000\n</code></pre> <p>Observe the following:</p> <ol> <li>We've provided the health check paths for each of the microservices using the <code>alb.ingress.kubernetes.io/healthcheck-path</code> annotation.</li> <li>In the ingress, we have used path-based rules to route traffic to a particular microservice based on matching path.</li> </ol> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- profile.yml\n\u2502   |-- notifications.yml\n\u2502   |-- feed.yml\n\u2502   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>This will create the following resources:</p> <ul> <li>Deployment and service for <code>profile</code> microservice.</li> <li>Deployment and service for <code>notifications</code> microservice.</li> <li>Deployment and service for <code>feed</code> microservice.</li> <li>Ingress with three rules.</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#step-2-verify-kubernetes-objects","title":"Step 2: Verify Kubernetes Objects","text":"<pre><code># List pods\nkubectl get pods\n\n# List deployments\nkubectl get deployments\n\n# List services\nkubectl get svc\n\n# List ingress\nkubectl get ingress\n</code></pre> <p>Also, go to the AWS Console and verify the resources created by the AWS Load Balancer Controller, including the load balancer, target groups, listener rules, etc.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#step-3-access-app-via-load-balancer-dns","title":"Step 3: Access App Via Load Balancer DNS","text":"<p>Copy the load balancer DNS from the ingress or AWS console and verify the following paths:</p> <pre><code>&lt;alb-dns&gt;/profile\n&lt;alb-dns&gt;/notifications\n&lt;alb-dns&gt;/feed\n</code></pre> <p>If everything is fine, the <code>&lt;alb-dns&gt;/profile</code>, <code>&lt;alb-dns&gt;/notifications</code>, and <code>&lt;alb-dns&gt;/feed</code> paths should serve the <code>profile</code>, <code>notifications</code>, and <code>feed</code> microservices, respectively.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#ordering-of-load-balancer-listener-rules","title":"Ordering of Load Balancer Listener Rules","text":"<p>Rules are evaluated in priority order, from the lowest value to the highest value. The default rule is evaluated last. You can change the priority of a non-default rule at any time but you cannot change the priority of the default rule.</p> <p>In later sections, we'll explore how to use the <code>IngressGroup</code> feature of the AWS Load Balancer Controller to specify the desired ordering of listener rules.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-path-based-routing/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- profile.yml\n\u2502   |-- notifications.yml\n\u2502   |-- feed.yml\n\u2502   |-- ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>Path-Based Routing on an Application Load Balancer</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/","title":"Create Ingress With SSL Redirect","text":"<p>The <code>alb.ingress.kubernetes.io/ssl-redirect</code> annotation enables SSLRedirect and specifies the SSL port that redirects to.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with SSL redirect:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:170476043077:certificate/2d88e035-cde7-472a-9cd3-6b6ce6ece961\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Be sure to replace the value of <code>alb.ingress.kubernetes.io/certificate-arn</code> with the <code>ARN</code> of the SSL certificate you created.</p> <p>Observe that we have added the <code>alb.ingress.kubernetes.io/ssl-redirect</code> annotation with value <code>443</code>. With this annotation in place, every <code>HTTP</code> listener will be configured with a default action which redirects to <code>HTTPS</code>, and other rules will be ignored.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the default listener rule and certificate attached to the <code>HTTPS (443)</code> listener in the load balancer.</p> <p>You'll notice that the <code>HTTP</code> is being redirected to <code>HTTPS</code>.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#step-5-add-record-in-route-53","title":"Step 5: Add Record in Route 53","text":"<p>Go to AWS Route 53 and add an <code>A</code> record (e.g <code>api.example.com</code>) for your domain that points to the Load Balancer. You can use alias to point the subdomain to the load balancer that was created.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#step-6-access-app-using-route-53-dns","title":"Step 6: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttps://api.example.com/\n\n# Health path\nhttps://api.example.com/health\n\n# Random generator path\nhttps://api.example.com/random\n</code></pre> <p>Also, verify that <code>HTTP</code> is redirected to <code>HTTPS</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-redirect/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, go to Route 53 and delete the <code>A</code> record that you created.</p> <p>References:</p> <ul> <li>SSL Redirect</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/","title":"Create Ingress With SSL","text":"<p>SSL support can be controlled with the following annotations:</p> Annotation Function <code>alb.ingress.kubernetes.io/certificate-arn</code> specifies the <code>ARN</code> of one or more certificate managed by AWS Certificate Manager. The first certificate in the list will be added as default certificate. And remaining certificate will be added to the optional certificate list. <code>alb.ingress.kubernetes.io/ssl-policy</code> specifies the Security Policy that should be assigned to the ALB, allowing you to control the protocol and ciphers. This is optional and defaults to <code>ELBSecurityPolicy-2016-08</code>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with SSL:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:170476043077:certificate/2d88e035-cde7-472a-9cd3-6b6ce6ece961\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Be sure to replace the value of <code>alb.ingress.kubernetes.io/certificate-arn</code> with the <code>ARN</code> of the SSL certificate you created.</p> <p>Note</p> <p><code>alb.ingress.kubernetes.io/listen-ports</code> defaults to <code>'[{\"HTTP\": 80}]'</code> or <code>'[{\"HTTPS\": 443}]'</code> depending on whether <code>certificate-arn</code> is specified.</p> <p>If you want to serve both <code>HTTP</code> and <code>HTTPS</code> traffic, you must set <code>alb.ingress.kubernetes.io/listen-ports</code> to <code>'[{\"HTTP\": 80}, {\"HTTPS\": 443}]'</code>.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the certificate attached to the <code>HTTPS (443)</code> listener in the load balancer.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#step-5-add-record-in-route-53","title":"Step 5: Add Record in Route 53","text":"<p>Go to AWS Route 53 and add an <code>A</code> record (e.g <code>api.example.com</code>) for your domain that points to the Load Balancer. You can use alias to point the subdomain to the load balancer that was created.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#step-6-access-app-using-route-53-dns","title":"Step 6: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttps://api.example.com/\n\n# Health path\nhttps://api.example.com/health\n\n# Random generator path\nhttps://api.example.com/random\n</code></pre> <p>Verify that both <code>HTTP</code> and <code>HTTPS</code> works.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, go to Route 53 and delete the <code>A</code> record that you created.</p> <p>References:</p> <ul> <li>Registering a New Domain in Route 53</li> <li>SSL Annotations</li> <li>Security Policies</li> </ul>"},{"location":"kubernetes-on-eks/ingress/introduction-to-ingress/","title":"Introduction to Ingress in Kubernetes","text":"<p><code>Ingress</code> in Kubernetes is like a traffic controller for incoming network traffic to your applications. It acts as a gateway that manages external access to services within your kubernetes cluster.</p> <p>An <code>Ingress controller</code> (e.g. AWS Load Balancer Controller) is responsible for fulfilling the <code>Ingress</code>, usually with a load balancer.</p> <p>To understand this, let's imagine a scenario. You have several applications running inside your kubernetes cluster, each with its own service. These services are accessible within the cluster, but you want to make them available to users outside the cluster, like your website or API.</p> <p>This is where <code>Ingress</code> comes in. Instead of exposing each service individually and managing multiple external IP addresses, <code>Ingress</code> provides a single entry point for incoming traffic. It acts as a smart router that directs requests to the appropriate services based on certain rules.</p>"},{"location":"kubernetes-on-eks/ingress/introduction-to-ingress/#analogy-to-understand-ingress","title":"Analogy to Understand Ingress","text":"<p>Think of <code>Ingress</code> as a receptionist at the entrance of an office building. Visitors come to the building and tell the receptionist which department they want to visit. The receptionist then guides them to the correct floor and office.</p> <p>Similarly, <code>Ingress</code> examines the incoming requests and uses rules you define to determine where the traffic should go. For example, you can configure it to direct traffic based on the domain name or the path of the URL. It then forwards the requests to the appropriate services or microservices running inside your cluster.</p> <p><code>Ingress</code> also provides additional features like load balancing, SSL termination (for secure connections), and traffic routing based on specific rules. It helps simplify the management of external access to your applications, making it easier to handle and control the flow of network traffic.</p> <p>In summary, <code>Ingress</code> in kubernetes is like a smart traffic controller or receptionist that manages incoming requests from outside your cluster and routes them to the correct services based on defined rules.</p>"},{"location":"kubernetes-on-eks/ingress/introduction-to-ingress/#traffic-routing-with-ingress","title":"Traffic Routing With Ingress","text":"<p>Here's how ingress controls network traffic in kubernetes:</p> <p> </p> <ol> <li>Client hits the ingress-managed load balancer</li> <li>Ingress sends the traffic to a kubernetes service that mathes the routing rule</li> <li>The kubernetes service serves the traffic from pods</li> </ol>"},{"location":"kubernetes-on-eks/ingress/introduction-to-ingress/#what-is-an-ingress-controller","title":"What is an Ingress Controller?","text":"<p>In order for the <code>Ingress</code> resource to work, the cluster must have an <code>Ingress controller</code> running.</p> <p>An <code>Ingress controller</code> (e.g. AWS Load Balancer Controller, NGINX Ingress Controller) is responsible for fulfilling the <code>Ingress</code>, usually with a load balancer.</p> <p>While the <code>Ingress</code> resource defines the rules and configuration for traffic routing, the Ingress controller is responsible for interpreting those rules and making them effective. It typically runs as a separate pod or deployment within the kubernetes cluster.</p> <p>The <code>Ingress controller</code> continuously monitors the <code>Ingress</code> resources and listens for changes or updates. When a new <code>Ingress</code> resource is created or modified, the controller reads the rules and configures itself accordingly to handle the specified traffic routing.</p> <p>In summary, an <code>Ingress controller</code> is a component in kubernetes that works alongside the <code>Ingress</code> resource to manage and control the routing of incoming network traffic.</p> <p>References:</p> <ul> <li>Ingress</li> </ul>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/","title":"Install AWS Load Balancer Controller","text":"<p>As discussed earlier, for the Ingress resource to work correctly, it is necessary to have an Ingress controller running in the cluster.</p> <p>So, let's go ahead and install AWS Load Balancer Controller which is an ingress controller.</p>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#step-1-verify-or-create-the-iam-oidc-provider","title":"Step 1: Verify or Create the IAM OIDC provider","text":"<p>The Amazon <code>AWS Load Balancer Controller</code> requires the use of the IAM OpenID Connect (OIDC) provider for authentication and authorization.</p> <p>This means that before installing the controller, we need to ensure that the IAM OIDC provider is created for your EKS cluster.</p> <p>We have the IAM OIDC provider already created when we created the cluster using <code>eksctl</code>.</p> <p>Let's verify if we have the IAM OIDC provider created for the cluster.</p> <ol> <li> <p>Retrieve your cluster's OIDC provider ID and store it in a variable:</p> <pre><code># Command template\noidc_id=$(aws eks describe-cluster --name &lt;cluster-name&gt; --region &lt;region-name&gt; --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\n\n# Actual command\noidc_id=$(aws eks describe-cluster --name my-cluster --region ap-south-1 --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\n</code></pre> <p>If the AWS profile being used already has the region configured, there is no requirement for you to provide the <code>--region</code> option.</p> </li> <li> <p>Determine whether an IAM OIDC provider with your cluster's ID is already in your account:</p> <pre><code>aws iam list-open-id-connect-providers | grep $oidc_id\n</code></pre> <p>If output is returned from the above command, then you already have a provider for your cluster and you can skip the next step.</p> </li> <li> <p>Create IAM OIDC provider for your cluster:</p> <p>If no output is returned in the previous command, then you must create an IAM OIDC provider for your cluster. </p> <pre><code># Command template\neksctl utils associate-iam-oidc-provider --cluster &lt;cluster-name&gt; --region &lt;region-name&gt; --approve\n\n# Actual command\neksctl utils associate-iam-oidc-provider --cluster my-cluster --region ap-south-1 --approve\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#step-2-add-the-eks-repository-to-helm","title":"Step 2: Add the EKS repository to Helm","text":"<p>We'll be using Helm to install the AWS Load Balancer Controller. Therefore, please ensure that you have Helm installed on your local system.</p> <p>Let's add the eks repository to helm:</p> <pre><code>helm repo add eks https://aws.github.io/eks-charts\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#step-3-install-aws-load-balancer-controller","title":"Step 3: Install AWS Load Balancer Controller","text":"<p>Now, that we have added the eks repository to helm, we can go ahead and install the AWS Load Balancer Controller using helm.</p> <p>The Load Balancer Controller requires to have certain AWS IAM permissions since it will create load balancer and related resources in AWS account at a later stage when we create Ingress Kubernetes object.</p> <p>The permissions can be granted to the AWS Load Balancer Controller either using a Service Account or using the IAM Role attached to worker nodes.</p>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#case-1-if-not-using-iam-roles-for-service-account","title":"Case 1: If not using IAM roles for service account","text":"<p>In this case, the AWS Load Balancer Controller inherits the required permissions from the role attached to worker nodes.</p> <p>Note</p> <p>Since granting permissions via worker nodes is not recommended we won't use this method. We'll use the method discussed in Case 2.</p> <ol> <li> <p>Make sure the IAM role attached to worker nodes has the following IAM permissions at minimum:</p> <pre><code>https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/main/docs/install/iam_policy.json\n</code></pre> <p>You can also download the IAM policy from the official github repository as follows:</p> <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\n</code></pre> </li> <li> <p>Install the AWS Load Balancer Controller using the following command:</p> <pre><code># Command template to install AWS Load Balancer Controller\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=&lt;cluster-name&gt; -n aws-load-balancer-controller\n\n# Actual command\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=my-cluster -n aws-load-balancer-controller\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#case-2-if-using-iam-roles-for-service-account","title":"Case 2: If using IAM roles for service account","text":"<p>In this case, the AWS Load Balancer Controller gains the required permissions from the service account.</p> <ol> <li> <p>Download IAM policy required for the AWS Load Balancer Controller:</p> <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\n</code></pre> </li> <li> <p>Create an IAM policy:</p> <pre><code>aws iam create-policy \\\n    --policy-name AWSLoadBalancerControllerIAMPolicy \\\n    --policy-document file://iam-policy.json\n</code></pre> </li> <li> <p>Create an IAM role and ServiceAccount for the Load Balancer controller:</p> <p>Let's create an IAM role and service account. Use the policy <code>ARN</code> from the previous step. If you prefer a predetermined role name you can specify <code>--role-name</code> parameter. (We'll let <code>eksctl</code> name the role)</p> <pre><code># Create IAM role and service account\neksctl create iamserviceaccount \\\n--cluster=&lt;cluster-name&gt; \\\n--namespace=aws-load-balancer-controller \\\n--name=aws-load-balancer-controller \\\n--attach-policy-arn=arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:policy/AWSLoadBalancerControllerIAMPolicy \\\n--approve\n</code></pre> <p>This will also annotate the service account with the IAM role it creates.</p> </li> <li> <p>Verify the IAM role and service account:</p> <pre><code># List service accounts in aws-load-balancer-controller namespace\nkubectl get sa -n aws-load-balancer-controller\n\n# View the service account definition in yaml format\nkubectl get sa aws-load-balancer-controller -n aws-load-balancer-controller -o yaml\n</code></pre> <p>Note down the role name displayed in the <code>.metadata.annotations</code> section of the service account definition. Go to the AWS console and verify if the role exists.</p> </li> <li> <p>Install the Load Balancer Controller:</p> <p>If you are using IAM Roles for service account you need to specify both of the chart values <code>serviceAccount.create=false</code> and <code>serviceAccount.name=aws-load-balancer-controller</code>.</p> <pre><code># Command template to install AWS Load Balancer Controller\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=&lt;cluster-name&gt; -n aws-load-balancer-controller --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\n\n# Actual Command\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=my-cluster -n aws-load-balancer-controller --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\n</code></pre> <p>If the command executes successfully you will see an ouput that looks something like this:</p> <pre><code>NAME: aws-load-balancer-controller\nLAST DEPLOYED: Wed Jun 28 00:17:46 2023\nNAMESPACE: aws-load-balancer-controller\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nAWS Load Balancer controller installed!\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#step-4-verify-resources-created-by-aws-load-balancer-controller","title":"Step 4: Verify Resources Created by AWS Load Balancer Controller","text":"<pre><code># List pods\nkubectl get po -n aws-load-balancer-controller\n\n# List ingress class\nkubectl get ingressclass -n aws-load-balancer-controller\n</code></pre> <p>In kubernetes, an <code>Ingress Class</code> is a way to differentiate and configure multiple Ingress controllers within a cluster. It allows you to specify different behaviors and settings for different Ingress controllers based on their class.</p> <p>For example, you can have both the NGINX Ingress Controller and the AWS Load Balancer Controller installed in the same Kubernetes cluster to meet different requirements.</p> <p>When creating an Ingress resource, you can specify the desired Ingress class using the <code>kubernetes.io/ingress.class</code> annotation.</p> <p>Check AWS Load Balancer Controller logs:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/install-aws-load-balancer-controller/#uninstall-aws-load-balancer-controller","title":"Uninstall AWS Load Balancer Controller","text":"<p>You need to execute the following commands only when you no longer need the AWS Load Balancer Controller installed in your kubernetes cluster:</p> <pre><code># Uninstall AWS Load Balancer Controller\nhelm uninstall aws-load-balancer-controller -n aws-load-balancer-controller\n\n# Delete the service account\neksctl delete iamserviceaccount --name aws-load-balancer-controller --cluster &lt;cluster-name&gt;\n</code></pre> <p>References:</p> <ul> <li>AWS Load Balancer Controller Concepts</li> <li>AWS Load Balancer Controller Installation Guide</li> <li>AWS Load Balancer Controller - Git Repo</li> </ul>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/introduction-to-aws-load-balancer-controller/","title":"Introduction to AWS Load Balancer Controller","text":"<p>AWS Load Balancer Controller (LBC) is an Ingress controller to help manage AWS Elastic Load Balancers for a kubernetes cluster.</p> <p>The <code>AWS Load Balancer Controller</code> automates the creation and configuration of the load balancers, ensuring that they match the desired state specified in kubernetes resources such as Services and Ingresses.</p> <p>Note</p> <p><code>AWS Load Balancer Controller</code> was formerly known as <code>AWS ALB Ingress Controller</code>.</p>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/introduction-to-aws-load-balancer-controller/#how-does-aws-load-balancer-controller-work","title":"How Does AWS Load Balancer Controller Work?","text":"<p>The following diagram details the AWS components the AWS Load Balancer Controller creates. It also demonstrates the route ingress traffic takes from the ALB to the kubernetes cluster.</p> <p> </p> <p>This section describes each step (circle) above. This example demonstrates satisfying 1 ingress resource.</p> <ol> <li> <p>The AWS Load Balancer Controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources.</p> </li> <li> <p>An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it's created in using annotations.</p> </li> <li> <p>Target Groups are created in AWS for each unique kubernetes service described in the ingress resource.</p> </li> <li> <p>Listeners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults (80 or 443) are used. Certificates may also be attached via annotations.</p> </li> <li> <p>Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct kubernetes Service.</p> </li> </ol> <p>Along with the above, the controller also:</p> <ul> <li>Deletes AWS components when ingress resources are removed from k8s.</li> <li>Modifies AWS components when ingress resources change in k8s.</li> <li>Assembles a list of existing ingress-related AWS components on start-up, allowing you to recover if the controller were to be restarted.</li> </ul>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/introduction-to-aws-load-balancer-controller/#traffic-modes-in-aws-load-balancer-controller","title":"Traffic Modes in AWS Load Balancer Controller","text":"<p>AWS Load Balancer controller supports two traffic modes:</p> <ul> <li> <p>Instance mode: Ingress traffic starts at the ALB and reaches the kubernetes nodes through each service's NodePort. This means that services referenced from ingress resources must be exposed by <code>type:NodePort</code> in order to be reached by the ALB.</p> </li> <li> <p>IP mode: Ingress traffic starts at the ALB and reaches the kubernetes pods directly.</p> </li> </ul> <p>By default, Instance mode is used. Users can explicitly select the mode via <code>alb.ingress.kubernetes.io/target-type</code> annotation.</p>"},{"location":"kubernetes-on-eks/ingress/aws-load-balancer-controller/introduction-to-aws-load-balancer-controller/#aws-load-balancer-controller-annotations","title":"AWS Load Balancer Controller Annotations","text":"<p>You can add annotations to kubernetes Ingress and Service objects to customize their behaviour.</p> <p>Annotations in the AWS Load Balancer Controller are additional metadata or configuration settings that can be added to kubernetes resources (such as Services and Ingresses) to provide specific instructions or customizations for the load balancer configuration.</p> <p>By adding annotations to kubernetes resources, you can configure advanced features like SSL/TLS termination, sticky sessions, health checks, target group attributes, cross-zone load balancing, access logs, security policies, and more.</p> <p>References:</p> <ul> <li>AWS Load Balancer Controller</li> <li>Ingress Annotations - AWS Load Balancer Controller</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/","title":"Create Ingress With SSL Discovery Via Host","text":"<p>TLS certificates for ALB Listeners can be automatically discovered with hostnames from Ingress resources if the <code>alb.ingress.kubernetes.io/certificate-arn</code> annotation is not specified.</p> <p>The controller will attempt to discover TLS certificates from the <code>tls</code> field in Ingress and <code>host</code> field in Ingress rules.</p> <p>Note</p> <p>You need to explicitly specify to use <code>HTTPS</code> listener with <code>alb.ingress.kubernetes.io/listen-ports</code> annotation.</p> <p>The Certificate Discovery can be either via Ingress rule <code>host</code> or Ingress <code>tls</code>. In this tutorial, we'll explore certificate discovery using the <code>host</code> field in Ingress rules.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with SSL discovery via <code>host</code> field in Ingress rules:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\nspec:\n  ingressClassName: alb\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Observe that we have added the <code>host</code> field to the Ingress rule. SSL certificate discovery will be based on this field. The AWS Load Balancer Controller will examine the <code>host</code> and search for an available certificate that can be used.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>You'll notice that the certificate is attached to the load balancer created by the ingress. We didn't specify the certificate <code>ARN</code> in the ingress manifest, yet it was attached to the load balancer due to SSL discovery via the <code>host</code> field in the ingress rules.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#step-5-add-record-in-route-53","title":"Step 5: Add Record in Route 53","text":"<p>Go to AWS Route 53 and add an <code>A</code> record (e.g <code>api.example.com</code>) for your domain that points to the Load Balancer. You can use alias to point the subdomain to the load balancer that was created.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#step-6-access-app-using-route-53-dns","title":"Step 6: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttps://api.example.com/\n\n# Health path\nhttps://api.example.com/health\n\n# Random generator path\nhttps://api.example.com/random\n</code></pre> <p>Also, verify that <code>HTTP</code> is redirected to <code>HTTPS</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-host/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, go to Route 53 and delete the <code>A</code> record that you created.</p> <p>References:</p> <ul> <li>Certificate Discovery Via Host</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/","title":"Create Ingress With SSL Discovery Via Host","text":"<p>TLS certificates for ALB Listeners can be automatically discovered with hostnames from Ingress resources if the <code>alb.ingress.kubernetes.io/certificate-arn</code> annotation is not specified.</p> <p>The controller will attempt to discover TLS certificates from the <code>tls</code> field in Ingress and <code>host</code> field in Ingress rules.</p> <p>Note</p> <p>You need to explicitly specify to use <code>HTTPS</code> listener with <code>alb.ingress.kubernetes.io/listen-ports</code> annotation.</p> <p>The Certificate Discovery can be either via Ingress rule <code>host</code> or Ingress <code>tls</code>. In this tutorial, we'll explore certificate discovery using the <code>tls</code> field in Ingress.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Next, let's create a service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: demo\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#step-3-create-ingress","title":"Step 3: Create Ingress","text":"<p>Now that we have the service ready, let's create an Ingress object with SSL discovery via <code>tls</code> field in Ingress:</p> <code>my-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing # Default value is internal\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=DevOps # Optional\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer # Optional\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # SSL Annotations\n    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08 # Optional\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\nspec:\n  ingressClassName: alb\n  tls:\n  - hosts:\n    - api.example.com\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-nodeport-service\n            port:\n              number: 5000\n</code></pre> <p>Observe that we have added the <code>tls</code> field in the Ingress spec. SSL certificate discovery will be based on this field. The AWS Load Balancer Controller will examine the <code>tls</code> field and search for available certificates that can be used for the specified hosts.</p> <p>Apply the manifest to create ingress:</p> <pre><code>kubectl apply -f my-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress\n{OR}\nkubectl get ing\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#step-4-verify-aws-resources-in-aws-console","title":"Step 4: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>You'll notice that the certificate is attached to the load balancer created by the ingress. We didn't specify the certificate <code>ARN</code> in the ingress manifest, yet it was attached to the load balancer due to SSL discovery via the <code>tls</code> field in the ingress.</p> <p>Also, verify that the ALB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#step-5-add-record-in-route-53","title":"Step 5: Add Record in Route 53","text":"<p>Go to AWS Route 53 and add an <code>A</code> record (e.g <code>api.example.com</code>) for your domain that points to the Load Balancer. You can use alias to point the subdomain to the load balancer that was created.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#step-6-access-app-using-route-53-dns","title":"Step 6: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttps://api.example.com/\n\n# Health path\nhttps://api.example.com/health\n\n# Random generator path\nhttps://api.example.com/random\n</code></pre> <p>Also, verify that <code>HTTP</code> is redirected to <code>HTTPS</code>.</p>"},{"location":"kubernetes-on-eks/ingress/ingress-with-ssl-discovery/ingress-with-ssl-discovery-via-tls/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- my-ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, go to Route 53 and delete the <code>A</code> record that you created.</p> <p>References:</p> <ul> <li>Certificate Discovery Via TLS</li> </ul>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/","title":"Install and Configure AWS CLI","text":"<p>The <code>AWS CLI</code> is a command-line tool that enables users to manage AWS services directly from the command line, offering a flexible and efficient way to create, configure, and interact with AWS resources.</p> <p>Let's see how you can install the <code>AWS CLI</code> on your operating system and configure it to access resources in AWS.</p> <p>Warning</p> <p>Given the ever-changing nature of the installation process, it is advisable to rely on the official documentation when installing <code>AWS CLI</code>.</p> <p>You can visit the official documentation and follow the instructions to install or update the <code>AWS CLI</code> on your operating system.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#step-1-install-aws-cli","title":"Step 1: Install AWS CLI","text":""},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#install-aws-cli-on-mac","title":"Install AWS CLI on Mac","text":"<ol> <li> <p>Download the installation file using the <code>curl</code> command:</p> <pre><code>curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\n</code></pre> </li> <li> <p>Run the standard macOS installer program, specifying the downloaded <code>.pkg</code> file as the source:</p> <pre><code>sudo installer -pkg AWSCLIV2.pkg -target /\n</code></pre> </li> <li> <p>Verify the installation:</p> <pre><code># Verify that the shell can find and run the aws command in your $PATH\nwhich aws\n\n# Check AWS CLI version\naws --version\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#install-aws-cli-on-windows","title":"Install AWS CLI on Windows","text":"<ol> <li> <p>Download and run AWS CLI MSI insatller</p> <p>To install AWS CLI on Windows you can just download and run the AWS CLI MSI installer for Windows (64-bit).</p> </li> <li> <p>Verify the installation:</p> <pre><code># Check AWS CLI version\naws --version\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#install-aws-cli-on-linux-x86-64-bit","title":"Install AWS CLI on Linux x86 (64-bit)","text":"<ol> <li> <p>Download the installation file:</p> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n</code></pre> </li> <li> <p>Unzip the installer:</p> <pre><code>unzip awscliv2.zip\n</code></pre> </li> <li> <p>Run the installation program:</p> <pre><code>sudo ./aws/install\n</code></pre> </li> <li> <p>Verify the installation:</p> <pre><code># Verify that the shell can find and run the aws command in your $PATH\nwhich aws\n\n# Check AWS CLI version\naws --version\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#install-aws-cli-on-linux-arm","title":"Install AWS CLI on Linux ARM","text":"<ol> <li> <p>Download the installation file:</p> <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"\n</code></pre> </li> <li> <p>Unzip the installer:</p> <pre><code>unzip awscliv2.zip\n</code></pre> </li> <li> <p>Run the installation program:</p> <pre><code>sudo ./aws/install\n</code></pre> </li> <li> <p>Verify the installation:</p> <pre><code># Verify that the shell can find and run the aws command in your $PATH\nwhich aws\n\n# Check AWS CLI version\naws --version\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#step-2-configure-aws-cli","title":"Step 2: Configure AWS CLI","text":"<p>Just installing the <code>AWS CLI</code> won't grant you access to AWS resources. To interact with AWS services, you must configure it by providing your AWS credentials.</p> <p>Below are the steps you can follow to configure the AWS CLI:</p> <ol> <li> <p>Create an AWS IAM user with programmatic access and download the access credentials (access key ID and secret access key).</p> </li> <li> <p>Give the IAM user the necessary permissions. To keep it simple, we'll grant <code>Administrator</code> access to the user.</p> <p>Warning</p> <p>It's important to note that while we've simplified the process by granting <code>Administrator</code> access for convenience, it's generally recommended to provide more granular and specific permissions to IAM users based on their actual needs. This helps reduce potential security risks and ensures a more controlled and secure environment.</p> </li> <li> <p>Configure the AWS CLI:</p> <pre><code># Command template\naws configure --profile &lt;aws-profile-name&gt;\n\n# Actual command\naws configure --profile my-aws-account\n</code></pre> <p>You'll be prompted to enter <code>AWS Access Key ID</code> and <code>AWS Secret Access Key</code> and the Default <code>region</code> name this profile will use.</p> <p>Warning</p> <p>If you don't specify the <code>--profile</code> parameter, the <code>AWS CLI</code> will use the <code>default</code> profile. To enhance configuration and security, it's advisable to use a named profile when configuring the <code>AWS CLI</code>.</p> <p>Using a named profile also enables you to configure multiple AWS accounts with different settings.</p> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-and-configure-aws-cli/#step-3-use-aws-cli-to-access-resources-in-aws","title":"Step 3: Use AWS CLI to Access Resources in AWS","text":"<p>Now that we have configured the <code>AWS CLI</code>, let's explore how you can use it to access resources in your AWS account. </p> <p>For instance, you can list <code>S3</code> buckets using the following AWS CLI command:</p> <pre><code>aws s3 ls --profile &lt;aws-profile-name&gt;\n</code></pre> <p>You can also export the <code>AWS_PROFILE</code> environment variable and then call <code>aws</code> command without passing <code>--profile</code> parameter.</p> <ol> <li> <p>Export the <code>AWS_PROFILE</code> environment variable:</p> <pre><code>export AWS_PROFILE=my-aws-account\n</code></pre> </li> <li> <p>Run AWS CLI command:</p> <pre><code>aws s3 ls\n</code></pre> </li> </ol> <p>References:</p> <ul> <li>Install or update the latest version of the AWS CLI</li> <li>Configure the AWS CLI</li> <li>Named Profile in AWS CLI</li> </ul>"},{"location":"kubernetes-on-eks/install-cli-tools/install-eksctl/","title":"Install eksctl","text":"<p><code>eksctl</code> is a simple command line tool for creating and managing Kubernetes clusters on Amazon EKS.</p> <p><code>eksctl</code> provides the fastest and easiest way to create a new cluster with nodes for Amazon EKS.</p> <p>Let's see how you can install <code>eksctl</code> on your operating system.</p> <p>Warning</p> <p>Given the ever-changing nature of the installation process, it is advisable to rely on the official documentation when installing <code>eksctl</code>.</p> <p>You can visit the official documentation and follow the instructions to install <code>eksctl</code> on your operating system.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/install-eksctl/#step-1-install-eksctl","title":"Step 1: Install eksctl","text":""},{"location":"kubernetes-on-eks/install-cli-tools/install-eksctl/#install-eksctl-on-macos","title":"Install eksctl on MacOS","text":"<p>You can install or upgrade <code>eksctl</code> on MacOS using the following command:</p> <pre><code>brew upgrade eksctl &amp;&amp; { brew link --overwrite eksctl; } || { brew tap weaveworks/tap; brew install weaveworks/tap/eksctl; }\n</code></pre>"},{"location":"kubernetes-on-eks/install-cli-tools/install-eksctl/#install-eksctl-on-windows","title":"Install eksctl on Windows","text":"<p>You can use Chocolatey to install <code>eksctl</code> on Windows. If you do not already have Chocolatey installed on your Windows system, see Installing Chocolatey.</p> <p>Install <code>eksctl</code>:</p> <pre><code>choco install -y eksctl \n</code></pre> <p>If you already have <code>eksctl</code> installed, you can upgrade it using the followign command:</p> <pre><code>choco upgrade -y eksctl \n</code></pre>"},{"location":"kubernetes-on-eks/install-cli-tools/install-eksctl/#install-eksctl-on-unix","title":"Install eksctl on Unix","text":"<ol> <li> <p>Download and extract the latest release of <code>eksctl</code> with the following command:</p> <pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n</code></pre> </li> <li> <p>Move the extracted binary to <code>/usr/local/bin</code>:</p> <pre><code>sudo mv /tmp/eksctl /usr/local/bin\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-eksctl/#step-2-verify-eksctl-installation","title":"Step 2: Verify eksctl Installation","text":"<pre><code>eksctl version\n</code></pre> <p>The above command should print the version of the eksctl installed on your system.</p> <p>References:</p> <ul> <li>Install eksctl on Unix</li> <li>Install eksctl on Windows</li> <li>Install eksctl on MacOs</li> </ul>"},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/","title":"Install Helm CLI","text":""},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/#install-helm-cli","title":"Install Helm CLI","text":"<p><code>Helm CLI</code> is a command-line tool that is used to interact with Helm, the package manager for Kubernetes.</p> <p>It is used to install, manage, and upgrade applications and services on Kubernetes clusters using Helm charts.</p> <p>Let's see how you can install <code>Helm CLI</code> on your operating system.</p> <p>Warning</p> <p>Given the ever-changing nature of the installation process, it is advisable to rely on the official documentation when installing <code>Helm CLI</code>.</p> <p>You can visit the official documentation and follow the instructions to install <code>Helm CLI</code> on your operating system.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/#step-1-install-helm-cli","title":"Step 1: Install Helm CLI","text":""},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/#install-helm-cli-on-macos","title":"Install Helm CLI on MacOS","text":"<p>You can install <code>Helm CLI</code> with Homebrew using the following command:</p> <pre><code>brew install helm\n</code></pre>"},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/#install-helm-cli-on-windows","title":"Install Helm CLI on Windows","text":"<p>You can use Chocolatey to install <code>Helm CLI</code> on Windows. If you do not already have Chocolatey installed on your Windows system, see Installing Chocolatey.</p> <p>Install <code>Helm CLI</code>:</p> <pre><code>choco install kubernetes-helm\n</code></pre>"},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/#install-helm-cli-on-debianubuntu","title":"Install Helm CLI on Debian/Ubuntu","text":"<p>You can install <code>Helm CLI</code> from Apt as follows:</p> <pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre>"},{"location":"kubernetes-on-eks/install-cli-tools/install-helm-cli/#step-2-verify-helm-installation","title":"Step 2: Verify Helm Installation","text":"<pre><code>helm version\n</code></pre> <p>If <code>Helm CLI</code> is installed on your system, it should produce an output similar to the below:</p> <pre><code>version.BuildInfo{Version:\"v3.12.3\", GitCommit:\"3a31588ad33fe3b89af5a2a54ee1d25bfe6eaa5e\", GitTreeState:\"clean\", GoVersion:\"go1.20.7\"}\n</code></pre> <p>References:</p> <ul> <li>Install Helm CLI on MacOS</li> <li>Install Helm CLI on Windows</li> <li>Install Helm CLI on Debina/Ubuntu</li> </ul>"},{"location":"kubernetes-on-eks/install-cli-tools/install-kubectl/","title":"Install kubectl","text":"<p>The Kubernetes command-line tool, <code>kubectl</code>, allows you to run commands against Kubernetes clusters.</p> <p>With <code>kubectl</code>, users can create, modify, and delete Kubernetes resources such as pods, deployments, services, and namespaces.</p> <p>Let's see how you can install <code>kubectl</code> on your operating system.</p> <p>Warning</p> <p>Given the ever-changing nature of the installation process, it is advisable to rely on the official documentation when installing <code>kubectl</code>.</p> <p>You can visit the official documentation and follow the instructions to install <code>kubectl</code> on your operating system.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/install-kubectl/#step-1-install-kubectl","title":"Step 1: Install Kubectl","text":""},{"location":"kubernetes-on-eks/install-cli-tools/install-kubectl/#install-kubectl-on-macos","title":"Install kubectl on MacOS","text":"<p>You can install kubectl with homebrew as follows:</p> <pre><code>brew install kubectl\n</code></pre>"},{"location":"kubernetes-on-eks/install-cli-tools/install-kubectl/#install-kubectl-on-windows","title":"Install kubectl on Windows","text":"<p>Follow the official documentation on how to Install and Set Up kubectl on Windows. This will ensure you have the latest release installed on your system.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/install-kubectl/#install-kubectl-on-linux","title":"Install kubectl on Linux","text":"<ol> <li> <p>Download the latest release with the command:</p> <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n</code></pre> </li> <li> <p>Validate the binary (optional):</p> <pre><code># Download the kubectl checksum file\ncurl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\n\n# Validate the kubectl binary against the checksum file\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\n</code></pre> </li> <li> <p>Install kubectl:</p> <pre><code>sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/install-cli-tools/install-kubectl/#step-2-verify-kubectl-installation","title":"Step 2: Verify Kubectl Installation","text":"<p>Run the following command to verify if <code>kubectl</code> is installed:</p> <pre><code>kubectl version --client --output=yaml\n</code></pre> <p>The output should look similar to the below:</p> <pre><code>clientVersion:\n  buildDate: \"2022-06-27T22:22:16Z\"\n  compiler: gc\n  gitCommit: b77d9473a02fbfa834afa67d677fd12d690b195f\n  gitTreeState: clean\n  gitVersion: v1.23.7-eks-4721010\n  goVersion: go1.17.10\n  major: \"1\"\n  minor: 23+\n  platform: linux/amd64\n</code></pre> <p>References:</p> <ul> <li>Install and Set Up kubectl on Linux</li> <li>Install and Set Up kubectl on Windows</li> <li>Install and Set Up kubectl on macOS</li> </ul>"},{"location":"kubernetes-on-eks/install-cli-tools/introduction-to-cli-tools/","title":"Introduction to CLI Tools","text":"<p>To effectively operate within the Kubernetes and EKS environments, we require specific command-line interface (CLI) tools that facilitate the configuration and administration of resources in both EKS and Kubernetes. </p> <p>Now, let's look at the key CLI tools that will be using in this course.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/introduction-to-cli-tools/#aws-cli","title":"AWS CLI","text":"<p>The <code>AWS CLI</code> is a command-line tool that enables users to manage AWS services directly from the command line, offering a flexible and efficient way to create, configure, and interact with AWS resources.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/introduction-to-cli-tools/#eksctl","title":"Eksctl","text":"<p><code>Eksctl</code> is a command-line utility that streamlines the creation and management of Kubernetes clusters on Amazon EKS, making cluster administration more accessible and efficient.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/introduction-to-cli-tools/#kubectl","title":"Kubectl","text":"<p>Using <code>kubectl</code>, individuals have the ability to create, alter, and remove Kubernetes resources like pods, deployments, services, and namespaces.</p>"},{"location":"kubernetes-on-eks/install-cli-tools/introduction-to-cli-tools/#helm-cli","title":"Helm CLI","text":"<p><code>Helm CLI</code> is a command-line tool that is used to interact with Helm, the package manager for Kubernetes. It is used to install, manage, and upgrade applications and services on Kubernetes clusters using Helm charts.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/create-and-manage-configmaps/","title":"Create and Manage ConfigMaps","text":"<p>Let's see how we can create and manage <code>ConfigMaps</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/create-and-manage-configmaps/#step-1-create-a-configmap","title":"Step 1: Create a ConfigMap","text":"<code>my-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-configmap\ndata:\n  # property-like keys; each key maps to a simple value\n  foo1: bar1\n  foo2: bar2\n\n  # file-like keys; each key maps to a file content\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  default.conf: |\n    server {\n      listen       8080;\n      listen  [::]:8080;\n      server_name  localhost;\n\n      #access_log  /var/log/nginx/host.access.log  main;\n\n      location / {\n          root   /usr/share/nginx/html;\n          index  index.html index.htm;\n      }\n\n      #error_page  404              /404.html;\n\n      # redirect server error pages to the static page /50x.html\n      #\n      error_page   500 502 503 504  /50x.html;\n      location = /50x.html {\n          root   /usr/share/nginx/html;\n      }\n    }\n</code></pre> <p>Observe the following:</p> <ul> <li>The <code>data</code> field is where we define key-value pairs</li> <li>There are two property-like keys <code>foo1</code>, and <code>foo2</code></li> <li>There are two file-like keys <code>game.properties</code>, and <code>default.conf</code></li> </ul> <p>Apply the manifest to create the ConfigMap:</p> <pre><code>kubectl apply -f my-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/create-and-manage-configmaps/#step-2-list-configmaps","title":"Step 2: List ConfigMaps","text":"<pre><code>kubectl get configmaps\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get cm\nkubectl get configmap\nkubectl get configmaps\n</code></pre> <p>Note</p> <p><code>configmap</code> is abbreviated as <code>cm</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/create-and-manage-configmaps/#step-3-describe-a-configmap","title":"Step 3: Describe a ConfigMap","text":"<pre><code># Command template\nkubectl describe configmap &lt;configmap-name&gt;\n{OR}\nkubectl describe configmap/&lt;configmap-name&gt;\n\n# Actual command\nkubectl describe configmap my-configmap\n{OR}\nkubectl describe configmap/my-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/create-and-manage-configmaps/#step-4-delete-a-configmap","title":"Step 4: Delete a ConfigMap","text":"<pre><code># Command template\nkubectl delete configmap &lt;configmap-name&gt;\n{OR}\nkubectl delete configmap/&lt;configmap-name&gt;\n\n# Actual command\nkubectl delete configmap my-configmap\n{OR}\nkubectl delete configmap/my-configmap\n</code></pre> <p>You can also use the manifest to delete the resource as follows:</p> <pre><code>kubectl delete -f my-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/introduction-to-configmap/","title":"Introduction to ConfigMap","text":"<p>A <code>ConfigMap</code> is kubernetes is used to store non-confidential data in key-value pairs.</p> <p>Pods can consume <code>ConfigMaps</code> as environment variables, command-line arguments, or as configuration files in a volume.</p> <p>A <code>ConfigMap</code> allows for decoupling of configuration data from container images and provides flexibility in managing application configurations.</p> <p>A <code>ConfigMap</code> is not designed to hold large chunks of data. The data stored in a <code>ConfigMap</code> cannot exceed <code>1 MiB</code>. If you need to store settings that are larger than this limit, you may want to consider mounting a volume or use a separate database or file service.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/introduction-to-configmap/#use-cases-of-configmap","title":"Use Cases of ConfigMap","text":"<ol> <li> <p>Environment Variables: ConfigMaps can be used to set environment variables for containers running in a pod.</p> </li> <li> <p>Application Configuration: ConfigMaps are frequently used to store application configuration data, such as database connection strings and API endpoints.</p> </li> <li> <p>File Mounts: ConfigMaps can be used to mount configuration files as volumes in Kubernetes pods. This allows applications to read configuration files without having to include them in the container image.</p> </li> </ol> <p>References:</p> <ul> <li>ConfigMaps</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/","title":"Supply Multiple Nginx Configuration Using ConfigMap","text":"<p>In the previous lesson, we learned how to use a <code>ConfigMap</code> to supply nginx configuration file to nginx. More specifically, we used a <code>ConfigMap</code> to modify the default nginx port.</p> <p>In this example, we will explore how to use a <code>ConfigMap</code> to supply multiple nginx configuration files. Specifically, we will learn how to use a <code>ConfigMap</code> to modify the default nginx port and serve a custom HTML page.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/#step-1-create-a-configmap-that-stores-nginx-configuration","title":"Step 1: Create a ConfigMap That Stores Nginx Configuration","text":"<p>Let's create a <code>ConfigMap</code> that stores nginx configuration:</p> <code>nginx-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-configmap\ndata:\n  default.conf: |\n    server {\n      listen       8080;\n      listen  [::]:8080;\n      server_name  localhost;\n\n      #access_log  /var/log/nginx/host.access.log  main;\n\n      location / {\n          root   /usr/share/nginx/html;\n          index  index.html index.htm;\n      }\n\n      #error_page  404              /404.html;\n\n      # redirect server error pages to the static page /50x.html\n      #\n      error_page   500 502 503 504  /50x.html;\n      location = /50x.html {\n          root   /usr/share/nginx/html;\n      }\n    }\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n      &lt;html&gt;\n        &lt;head&gt;\n          &lt;title&gt;Welcome to My Website!&lt;/title&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n          &lt;h1&gt;Welcome to My Website!&lt;/h1&gt;\n        &lt;/body&gt;\n    &lt;/html&gt;\n</code></pre> <p>Observe the following:</p> <ul> <li>The nginx server listens on port <code>8080</code></li> <li>Nginx will serve the modified <code>index.html</code> page at <code>/usr/share/nginx/html</code></li> </ul> <p>Apply the manifest to create ConfigMap:</p> <pre><code>kubectl apply -f nginx-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/#step-2-verify-configmap","title":"Step 2: Verify ConfigMap","text":"<pre><code># List configmaps\nkubectl get cm\n\n# Describe the configmap\nkubectl describe cm nginx-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/#step-3-create-nginx-deployment-that-uses-configuration-from-configmap","title":"Step 3: Create Nginx Deployment That Uses Configuration from ConfigMap","text":"<p>Let's create nginx deployment that uses configuration files from the <code>ConfigMap</code> we created in the previous step:</p> <code>nginx-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: my-volume\n          mountPath: \"/etc/nginx/conf.d/default.conf\"\n          subPath: \"default.conf\"\n        - name: my-volume\n          mountPath: \"/usr/share/nginx/html/index.html\"\n          subPath: \"index.html\"\n      volumes:\n      - name: my-volume\n        configMap:\n          name: nginx-configmap\n</code></pre> <p>Observe the following:</p> <ul> <li>We have created a volume from <code>nginx-configmap</code> ConfigMap</li> <li>We have mounted <code>default.conf</code> file from <code>nginx-configmap</code> to the <code>nginx</code> container at <code>/etc/nginx/conf.d/default.conf</code></li> <li>We have mounted <code>index.html</code> file from <code>nginx-configmap</code> to the <code>nginx</code> container at <code>/usr/share/nginx/html/index.html</code></li> </ul> <p>Note</p> <p>By using <code>subPath</code> to specify the specific file within the <code>ConfigMap</code> volume, we can selectively mount only the necessary configuration files or data that a container needs.</p> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f nginx-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n\n# Describe the Pod\nkubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>Observe the <code>Mounts</code> field of the <code>nginx</code> container. You'll see 2 mounts from <code>my-volume</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/#step-5-view-nginx-configuration-files","title":"Step 5: View Nginx Configuration Files","text":"<pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# View the content of the default.conf file\ncat /etc/nginx/conf.d/default.conf\n\n# View the content of the default web pages\ncd /usr/share/nginx/html\nls\ncat index.html\n</code></pre> <p>Observe the following:</p> <ul> <li>The nginx server listens on port <code>8080</code></li> <li>Nginx serves the supplied <code>index.html</code> page mounted at <code>/usr/share/nginx/html</code></li> </ul> <p>Get response from nginx server:</p> <pre><code># Hit port 8080\ncurl localhost:8080\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-multiple-nginx-configuration-using-configmap/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- nginx-deployment.yml\n\u2502   |-- nginx-configmap.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/","title":"Supply Nginx Configuration Using ConfigMap","text":"<p>Now, let's see how you can use <code>ConfigMap</code> to supply configuration to applications. In this example, we will use <code>ConfigMap</code> to provide configuration to the nginx application.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#nginx-deployment-without-configmap","title":"Nginx Deployment Without ConfigMap","text":"<p>First, let's explore the default behaviour of <code>nginx</code> application.</p> <p>By default, Nginx serves responses from a virtual server that listens on port <code>80</code> and serves the default <code>index.html</code> file located at <code>/usr/share/nginx/html</code>.</p> <p>Let's test this out!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#1-create-nginx-deployment-manifest","title":"1. Create nginx deployment manifest","text":"<code>nginx-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#2-apply-the-manifest-to-create-the-nginx-deployment","title":"2. Apply the manifest to create the nginx deployment","text":"<pre><code>kubectl apply -f nginx-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#3-verify-deployment-and-pods","title":"3. Verify deployment and pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#4-view-default-nginx-configuration-files","title":"4. View default nginx configuration files","text":"<pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# View the content of the default.conf file\ncat /etc/nginx/conf.d/default.conf\n\n# View the content of the default web pages\ncd /usr/share/nginx/html\nls\ncat index.html\n</code></pre> <p>Observe the following:</p> <ul> <li>The nginx server listens on port <code>80</code> by default</li> <li>Nginx serves the default <code>index.html</code> page at <code>/usr/share/nginx/html</code></li> </ul> <p>Get response from nginx server:</p> <pre><code># Hit port 80\ncurl localhost\n{OR}\ncurl localhost:80\n</code></pre> <p>What if you want nginx to serve response from port <code>8080</code> instead of port <code>80</code>?</p> <p><code>ConfigMap</code> can help. Let's try it out!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#step-1-create-a-configmap-that-stores-nginx-configuration","title":"Step 1: Create a ConfigMap That Stores Nginx Configuration","text":"<p>First, let's create a <code>ConfigMap</code> that stores nginx configuration as follows:</p> <code>nginx-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-configmap\ndata:\n  default.conf: |\n    server {\n      listen       8080;\n      listen  [::]:8080;\n      server_name  localhost;\n\n      #access_log  /var/log/nginx/host.access.log  main;\n\n      location / {\n          root   /usr/share/nginx/html;\n          index  index.html index.htm;\n      }\n\n      #error_page  404              /404.html;\n\n      # redirect server error pages to the static page /50x.html\n      #\n      error_page   500 502 503 504  /50x.html;\n      location = /50x.html {\n          root   /usr/share/nginx/html;\n      }\n    }\n</code></pre> <p>Observe the following:</p> <ul> <li>The nginx server listens on port <code>8080</code></li> <li>Nginx will serve the default <code>index.html</code> page at <code>/usr/share/nginx/html</code></li> </ul> <p>Apply the manifest to create ConfigMap:</p> <pre><code>kubectl apply -f nginx-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#step-2-verify-configmap","title":"Step 2: Verify ConfigMap","text":"<pre><code># List configmaps\nkubectl get cm\n\n# Describe the configmap\nkubectl describe cm nginx-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#step-3-update-nginx-deployment-to-use-configuration-from-configmap","title":"Step 3: Update Nginx Deployment to Use Configuration From ConfigMap","text":"<p>Let's update the nginx deployment to use nginx configuration from the <code>ConfigMap</code> we created in the previous step.</p> <p>The updated deployment should look like the following:</p> <code>nginx-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: my-volume\n          mountPath: \"/etc/nginx/conf.d\"\n      volumes:\n      - name: my-volume\n        configMap:\n          name: nginx-configmap\n</code></pre> <p>Apply the manifest to update the deployment:</p> <pre><code>kubectl apply -f nginx-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n\n# Describe the Pod\nkubectl describe pod &lt;pod-name&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#step-5-view-nginx-configuration-files","title":"Step 5: View Nginx Configuration Files","text":"<pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# View the content of the default.conf file\ncat /etc/nginx/conf.d/default.conf\n\n# View the content of the default web pages\ncd /usr/share/nginx/html\nls\ncat index.html\n</code></pre> <p>Observe the following:</p> <ul> <li>The nginx server now listens on port <code>8080</code></li> <li>Nginx serves the default <code>index.html</code> page at <code>/usr/share/nginx/html</code></li> </ul> <p>Get response from nginx server:</p> <pre><code># Hit port 8080\ncurl localhost:8080\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/supply-nginx-configuration-using-configmap/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- nginx-deployment.yml\n\u2502   |-- nginx-configmap.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/","title":"Use ConfigMap as Volume","text":"<p>Let's see how we can use <code>ConfigMap</code> as a volume and mount it in a container.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/#step-1-create-a-configmap","title":"Step 1: Create a ConfigMap","text":"<code>my-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-configmap\ndata:\n  # property-like keys; each key maps to a simple value\n  foo1: bar1\n  foo2: bar2\n\n  # file-like keys; each key maps to a file content\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5    \n  default.conf: |\n    server {\n      listen       8080;\n      listen  [::]:8080;\n      server_name  localhost;\n\n      #access_log  /var/log/nginx/host.access.log  main;\n\n      location / {\n          root   /usr/share/nginx/html;\n          index  index.html index.htm;\n      }\n\n      #error_page  404              /404.html;\n\n      # redirect server error pages to the static page /50x.html\n      #\n      error_page   500 502 503 504  /50x.html;\n      location = /50x.html {\n          root   /usr/share/nginx/html;\n      }\n    }\n</code></pre> <p>Apply the manifest to create ConfigMap:</p> <pre><code>kubectl apply -f my-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/#step-2-verify-configmap","title":"Step 2: Verify ConfigMap","text":"<pre><code># List configmaps\nkubectl get cm\n\n# Describe the configmap\nkubectl describe cm my-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/#step-3-create-pods-that-uses-configmap-as-volume","title":"Step 3: Create Pods That Uses ConfigMap as Volume","text":"<p>Let's create pods that uses <code>ConfigMap</code> as volume and mounts it in a container. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: my-volume\n          mountPath: \"/config\"\n      volumes:\n      - name: my-volume\n        configMap:\n          name: my-configmap\n</code></pre> <p>Observe the following:</p> <ul> <li>The pod uses the ConfigMap <code>my-configmap</code> as volume</li> <li>The volume is mounted at <code>/config</code> directory in the <code>nginx</code> container</li> </ul> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/#step-5-verify-volume-mount-and-data","title":"Step 5: Verify Volume Mount and Data","text":"<ol> <li> <p>Open a shell session inside the <code>nginx</code> container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>View data:</p> <pre><code>cd /config\nls\n</code></pre> </li> </ol> <p>Please note that when a <code>ConfigMap</code> is mounted as a volume in a container, each key in the <code>ConfigMap</code> is stored as a file in the container's file system. This means that the container can read the contents of each file as if they were regular files in the container's file system.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/use-configmap-as-volume/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-configmap.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/conventional-approach-of-supplying-environment-variables/","title":"Conventional Approach of Supplying Environment Variables","text":"<p>Let's look at the conventional approach of supplying environment variables to containers running inside a pod.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/conventional-approach-of-supplying-environment-variables/#step-1-create-pods-that-uses-environment-variables","title":"Step 1: Create Pods That Uses Environment Variables","text":"<p>Let's create pods that uses environment variables. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        env:\n        - name: key1\n          value: value1\n        - name: key2\n          value: value2\n</code></pre> <p>Observe the following:</p> <ul> <li>We are using the keyword <code>env</code> to supply a list of environment variables to the <code>nginx</code> container</li> <li>In this example, we have supplied two environment variables; <code>key1</code>, and <code>key2</code>, with values <code>value1</code> and <code>value2</code>, respectively.</li> </ul> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/conventional-approach-of-supplying-environment-variables/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/conventional-approach-of-supplying-environment-variables/#step-3-verify-environment-variables","title":"Step 3: Verify Environment Variables","text":"<p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> <p>List environment variables available to the container:</p> <pre><code>env\n</code></pre> <p>You'll see a list of environment variables available to the container. This includes both <code>system-provided</code> as well as <code>user-provided</code> environment variables.</p> <p>Print values of the environment variables we set:</p> <pre><code># Print value of the environment variable key1\necho $key1\n\n# Print value of the environment variable key2\necho $key2\n</code></pre> <p>You'll notice the environment variables <code>key1</code> and <code>key2</code> are set to <code>value1</code> and <code>value2</code> respectively.</p> <p>This method of supplying environment variables is acceptable if you have only a few of them. But, what if you have a significant number of environment variables? Adding all of them in the manifest file will needlessly bloat the manifest file.</p> <p>In that case, you can use <code>ConfigMap</code> to supply a list of environment variables to containers in a pod. We'll explore this in the next section.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/conventional-approach-of-supplying-environment-variables/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/","title":"Supply Environment Variables From Multiple Sources","text":"<p>What if you want to use <code>ConfigMap</code> for the environemnt variables but also want to supply additional environment variables directly in the pod definition?</p> <p>Let's see how we can do that.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/#step-1-create-a-configmap","title":"Step 1: Create a ConfigMap","text":"<code>my-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-configmap\ndata:\n  foo1: bar1\n  foo2: bar2\n</code></pre> <p>Apply the manifest to create ConfigMap:</p> <pre><code>kubectl apply -f my-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/#step-2-verify-configmap","title":"Step 2: Verify ConfigMap","text":"<pre><code># List configmaps\nkubectl get cm\n\n# Describe the configmap\nkubectl describe cm my-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/#step-3-create-pods-that-uses-environment-variables","title":"Step 3: Create Pods That Uses Environment Variables","text":"<p>Let's create pods that uses <code>ConfigMap</code> as well as the conventional approach to set environment variables for the container. We'll use a deployment to create pods.</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        env:\n        - name: key1\n          value: value1\n        - name: key2\n          value: value2\n        envFrom:\n        - configMapRef:\n            name: my-configmap\n</code></pre> <p>Observe the following:</p> <ul> <li>We are using <code>env</code> keyword to supply a list of environment variables</li> <li>We are also using <code>envFrom</code> keyword to supply a list of environment variables from the ConfigMap <code>my-configmap</code></li> </ul> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/#step-5-verify-environment-variables","title":"Step 5: Verify Environment Variables","text":"<p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> <p>List environment variables available to the container:</p> <pre><code>env\n</code></pre> <p>You'll see a list of environment variables available to the container. This includes both <code>system-provided</code> as well as <code>user-provided</code> (using <code>env</code> and <code>envFrom</code> keyword) environment variables.</p> <p>Print values of the environment variables we set:</p> <pre><code># Print value of the environment variable key1\necho $key1\n\n# Print value of the environment variable key2\necho $key2\n\n# Print value of the environment variable foo1\necho $foo1\n\n# Print value of the environment variable foo2\necho $foo2\n</code></pre> <p>You'll notice the following:</p> <ul> <li>Environment variables <code>key1</code> and <code>key2</code> are set to <code>value1</code> and <code>value2</code> respectively.</li> <li>Environment variables <code>foo1</code> and <code>foo2</code> are set to <code>bar1</code> and <code>bar2</code> respectively.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-environment-variables-from-multiple-sources/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-configmap.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/","title":"Supply Only Specific Environment Variables From ConfigMap","text":"<p>Consider a case where you want to use only a specific items from <code>ConfigMap</code> as environment variables instead of supplying entire <code>ConfigMap</code> as environment variables.</p> <p>Let's see how we can do that.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/#step-1-create-a-configmap","title":"Step 1: Create a ConfigMap","text":"<code>my-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-configmap\ndata:\n  foo1: bar1\n  foo2: bar2\n</code></pre> <p>Apply the manifest to create ConfigMap:</p> <pre><code>kubectl apply -f my-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/#step-2-verify-configmap","title":"Step 2: Verify ConfigMap","text":"<pre><code># List configmaps\nkubectl get cm\n\n# Describe the configmap\nkubectl describe cm my-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/#step-3-create-pods-that-uses-environment-variables","title":"Step 3: Create Pods That Uses Environment Variables","text":"<p>Let's create pods that uses specific items from the <code>ConfigMap</code> as environment variables for the container. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        env:\n        - name: key1\n          value: value1\n        - name: key2\n          value: value2\n        - name: foo1\n          valueFrom:\n            configMapKeyRef:\n              name: my-configmap\n              key: foo1\n</code></pre> <p>Observe the following:</p> <ul> <li>We are using <code>env</code> keyword to supply a list of environment variables.</li> <li>We are also using <code>valueFrom</code> keyword to get the value of the key <code>foo1</code> in the ConfigMap <code>my-configmap</code> and then setting it as the value of an environment variable named <code>foo1</code>.</li> </ul> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/#step-5-verify-environment-variables","title":"Step 5: Verify Environment Variables","text":"<p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> <p>List environment variables available to the container:</p> <pre><code>env\n</code></pre> <p>You'll see a list of environment variables available to the container. This includes both <code>system-provided</code> as well as <code>user-provided</code> (using env and <code>valueFrom</code> keyword) environment variables.</p> <p>Print values of the environment variables we set:</p> <pre><code># Print value of the environment variable key1\necho $key1\n\n# Print value of the environment variable key2\necho $key2\n\n# Print value of the environment variable foo1\necho $foo1\n</code></pre> <p>You'll notice the following:</p> <ul> <li>Environment variables <code>key1</code> and <code>key2</code> are set to <code>value1</code> and <code>value2</code> respectively.</li> <li>Environment variables <code>foo1</code> is set to <code>bar1</code>.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/supply-only-specific-environment-variables-from-configmap/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-configmap.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/","title":"Use ConfigMap to Supply Environment Variables","text":"<p>Let's see how we can use ConfigMap to supply environment variables to containers running inside in a pod.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/#step-1-create-a-configmap","title":"Step 1: Create a ConfigMap","text":"<p>Let's create a <code>ConfigMap</code> with data that contains the required environment variables.</p> <code>my-configmap.yml</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-configmap\ndata:\n  foo1: bar1\n  foo2: bar2\n</code></pre> <p>Apply the manifest to create ConfigMap:</p> <pre><code>kubectl apply -f my-configmap.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/#step-2-verify-configmap","title":"Step 2: Verify ConfigMap","text":"<pre><code># List configmaps\nkubectl get cm\n\n# Describe the configmap\nkubectl describe cm my-configmap\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/#step-3-create-pods-that-uses-environment-variables","title":"Step 3: Create Pods That Uses Environment Variables","text":"<p>Let's create pods that uses <code>ConfigMap</code> to set environment variables for the container. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        envFrom:\n        - configMapRef:\n            name: my-configmap\n</code></pre> <p>Observe that we are using the keyword <code>envFrom</code> to supply a list of environment variables from the ConfigMap <code>my-configmap</code>.</p> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/#step-5-verify-environment-variables","title":"Step 5: Verify Environment Variables","text":"<p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> <p>List environment variables available to the container:</p> <pre><code>env\n</code></pre> <p>You'll see a list of environment variables available to the container. This includes both <code>system-provided</code> as well as <code>user-provided</code> environment variables.</p> <p>Print values of the environment variables we set:</p> <pre><code># Print value of the environment variable foo1\necho $foo1\n\n# Print value of the environment variable foo2\necho $foo2\n</code></pre> <p>You'll notice the environment variables <code>foo1</code> and <code>foo2</code> are set to <code>bar1</code> and <code>bar2</code> respectively.</p> <p>Now, what if you want to use <code>ConfigMap</code> for the environemnt variables but also want to supply additional environment variables directly in the pod definition?</p> <p>In that case, you can use both the <code>env</code> and <code>envFrom</code> field. We'll explore this in the next section.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/configmap/using-configmap-for-enironment-variables/use-configmap-to-supply-environment-variables/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-configmap.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/create-daemonset/","title":"Create and Manage Kubernetes DaemonSets","text":"<p>Let's see how you can create and manage kubernetes Daemonsets. For this demonstration, let's assume there is a need to run an nginx server on each node. In the real world, your requirements might differ depending on your use case.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/create-daemonset/#step-1-create-daemonset-manifest","title":"Step 1: Create DaemonSet Manifest","text":"<p>First, we need to write the DaemonSet manifest as follows:</p> <code>my-daemonset.yml</code> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Required fields:</p> <ul> <li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object.</li> <li><code>kind</code> - What kind of object you want to create.</li> <li><code>metadata</code> - Data that helps uniquely identify the object, including a name string, UID , and - optional namespace.</li> <li><code>spec</code> - What state you desire for the object.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/create-daemonset/#step-2-create-daemonset","title":"Step 2: Create DaemonSet","text":"<p>Let's use <code>kubectl apply</code> to apply the manifest and create the DaemonSet:</p> <pre><code># Create daemonset\nkubectl apply -f my-daemonset.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/create-daemonset/#step-3-list-daemonsets-and-pods","title":"Step 3: List DaemonSets and Pods","text":"<pre><code>kubectl get daemonsets\n{OR}\nkubectl get daemonset\n{OR}\nkubectl get ds\n</code></pre> <p>Note</p> <p><code>daemonset</code> is abbreviated as <code>ds</code>.</p> <p>Also, list pods and verify if they are running on all the nodes in the cluster:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>You'll that a copy of the pod is running on all the nodes in the cluster.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/create-daemonset/#step-4-delete-daemonset","title":"Step 4: Delete DaemonSet","text":"<pre><code># Delete the daemonset\nkubectl delete -f my-daemonset.yml\n</code></pre> <p>This will delete the DaemonSet and pods managed by the DaemonSet.</p> <p>References:</p> <ul> <li>DaemonSet</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/introduction-to-daemonset/","title":"Introduction to Kubernetes DaemonSet","text":"<p>A <code>DaemonSet</code> ensures that all (or some - using node selectors) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected.</p> <p>Deleting a <code>DaemonSet</code> will clean up the pods it created.</p> <p>Essentially, <code>DaemonSets</code> ensure that specific pods are running on all or a subset of nodes within a kubernetes cluster. Unlike other controllers that ensure a specified number of replicas, <code>DaemonSets</code> run exactly one instance of a pod on each node in the cluster or a subset defined by node selectors.</p> <p> </p> <p><code>DaemonSets</code> are similar to Deployments in that they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers, storage servers).</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/daemonset/introduction-to-daemonset/#use-cases-of-daemonset","title":"Use Cases of DaemonSet","text":"<ol> <li> <p>Logging and Monitoring</p> <p><code>DaemonSets</code> are often used for deploying logging agents, monitoring tools, or other system-level applications that need to run on every node. For instance, deploying tools like Prometheus node exporters or log collection agents like Fluentd or Filebeat via DaemonSets ensures comprehensive visibility into cluster health and performance.</p> </li> <li> <p>Security Agents and Policies</p> <p>Security-related tools like intrusion detection systems, firewalls, or antivirus agents can be deployed using <code>DaemonSets</code>, ensuring each node within the cluster has the necessary security measures in place.</p> </li> <li> <p>Networking and Load Balancing</p> <p><code>DaemonSets</code> can be leveraged to deploy networking solutions such as load balancers, proxies, or service meshes that require a presence on each node to manage traffic or enforce network policies uniformly.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/","title":"Create and Manage Deployment Using Declarative Approach","text":"<p>Let's see how you can create and manage Kubernetes Deployments declaratively.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-1-create-deployment-manifest","title":"Step 1: Create Deployment Manifest","text":"<p>First, we need to write the Deployment manifest as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: backend\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 80\n</code></pre> <p>Required fields:</p> <ul> <li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object.</li> <li><code>kind</code> - What kind of object you want to create.</li> <li><code>metadata</code> - Data that helps uniquely identify the object, including a name string, UID , and optional namespace.</li> <li><code>spec</code> - What state you desire for the object.</li> </ul> <p>You'll notice that the YAML configuration for the <code>Deployment</code> looks very similar to the <code>ReplicaSet</code>, with the exception of the kind attribute.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-2-create-deployment","title":"Step 2: Create Deployment","text":"<p>Let's use <code>kubectl apply</code> to apply the manifest and create the Deployment:</p> <pre><code># Create deployment\nkubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-3-list-deployments","title":"Step 3: List Deployments","text":"<p>List Deployments and verify that the deployment we created is available:</p> <pre><code>kubectl get deployments\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-4-view-replicasets-created-by-the-deployment","title":"Step 4: View ReplicaSets Created by the Deployment","text":"<p>As discussion earlier, when you create a <code>Deployment</code>, kubernetes creates a <code>ReplicaSet</code> for you and manages the replicas of your application based on the Deployment's desired state.</p> <p>Let's list the <code>ReplicaSets</code> created by <code>my-deployment</code>:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre> <p>You'll see a <code>ReplicaSet</code> created and managed by the Deployment.</p> <p>The name of the ReplicaSets created by Deployments starts with the deployment name.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-5-view-pods-created-by-the-deployment","title":"Step 5: View Pods Created by the Deployment","text":"<p>List pods:</p> <pre><code>kubectl get pods\n</code></pre> <p>You'll see pods created and managed by the Deployment.</p> <p>The name of the pods created by Deployments starts with the deployment name.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-6-describe-the-deployment","title":"Step 6: Describe the Deployment","text":"<p>Describe the deployment:</p> <pre><code>kubectl describe deployment/my-deployment\n{OR}\nkubectl describe deployment my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-7-scale-the-deployment","title":"Step 7: Scale the Deployment","text":"<p>Let's scale replicas to 3:</p> <pre><code>kubectl scale deployment/my-deployment --replicas 3\n</code></pre> <p>You can also use <code>kubectl edit</code> command to scale the Deployment as follows:</p> <ol> <li>Set the editor you want to use (Default is vim):</li> </ol> <pre><code>export KUBE_EDITOR=nano\n</code></pre> <ol> <li>Edit the YAML manifest by changing the value of replicas to a desired value and then save file to scale the deployment:</li> </ol> <pre><code>kubectl edit deployment/my-deployment\n{OR}\nkubectl edit deployment my-deployment\n</code></pre> <p>Verify that the deployment was scaled:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List replicasets\nkubectl get rs | grep my-deployment\n\n# List pods\nkubectl get pods | grep my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-8-update-the-deployment","title":"Step 8: Update the Deployment","text":"<p>Let's update the deployment to use a new image <code>reyanshkharga/nginx:v2</code>.</p> <p>In the Deployment YAML manifest change the value of image to <code>reyanshkharga/nginx:v2</code>.</p> <p>Now, apply the manifest to update the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-9-verify-if-the-deployment-was-updated","title":"Step 9: Verify if the Deployment Was Updated","text":"<p>Verfify the Deployment, Pods, and ReplicaSets:</p> <pre><code># Describe deployment\nkubectl describe deploy/my-deployment\n\n# List pods created by the deployment\nkubectl get pods | grep my-deployment\n\n# List replicasets created by the deployment\nkubectl get rs | grep my-deployment\n</code></pre> <p>You'll notice the following:</p> <ol> <li>The Deployment is updated and uses the updated image.</li> <li>A new ReplicaSet is created</li> <li>New Pods come up and old Pods are terminated.</li> <li>Old ReplicaSets are not deleted (unless this was 11th revision)</li> </ol> <p>Here's a visual representation of the rolling update described above:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-10-update-the-deployment-few-more-times","title":"Step 10: Update the Deployment Few More Times","text":"<ol> <li>Change the image to <code>reyanshkharga/nginx:v3</code> and apply the configuration again.</li> <li>Change the image to <code>reyanshkharga/nginx:v4</code> and apply the configuration again.</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-11-list-deployment-revisions","title":"Step 11: List Deployment Revisions","text":"<p>As discussed earlier, Kubernetes Deployment controller stores up to 10 revisions of a Deployment.</p> <p>You can configure the maximum number of revisions to keep by setting the <code>.spec.revisionHistoryLimit</code> field in the Deployment's YAML file.</p> <p>List the Deployment revisions:</p> <pre><code>kubectl rollout history deployment/my-deployment\n</code></pre> <p>Get the details of a particular revision:</p> <pre><code>kubectl rollout history deployment/my-deployment --revision=2\n</code></pre> <p>List ReplicaSets managed by the Deployment:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-12-rollback-the-deployment","title":"Step 12: Rollback the Deployment","text":"<p>Rollback to the previous version:</p> <pre><code>kubectl rollout undo deployment/my-deployment\n</code></pre> <p>Verify rollback:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre> <p>You'll notice that the pods are now managed by the replicaset from previous revision.</p> <p>You can also rollback to a particular version as follows:</p> <pre><code>kubectl rollout undo deployment my-deployment --to-revision=1\n</code></pre> <p>Verify rollback:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre> <p>Here's a visual representation of the rollback flow described above:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-declarative-approach/#step-13-delete-the-deployment","title":"Step 13: Delete the Deployment","text":"<pre><code>kubectl delete deployment/my-deployment\n{OR}\nkubectl delete -f my-deployment.yml\n</code></pre> <p>References:</p> <ul> <li>Deployment v1 apps</li> <li>Workload Resources - Deployment</li> <li>ObjectMeta</li> <li>DeploymentSpec</li> <li>Deployment Concept</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/","title":"Create and Manage Deployment Using Imperative Commands","text":"<p>Let's look at the imperative commands that you can use to create and manage Kubernetes <code>Deployments</code>.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<pre><code># Command template\nkubectl create deployment &lt;deployment-name&gt; --image=&lt;image-name&gt; --replicas=&lt;replica-count&gt;\n{OR}\nkubectl create deployment/&lt;deployment-name&gt; --image=&lt;image-name&gt; --replicas=&lt;replica-count&gt;\n\n# Actual command\nkubectl create deployment my-deployment --image=reyanshkharga/nginx:v1 --replicas 2\n{OR}\nkubectl create deployment/my-deployment --image=reyanshkharga/nginx:v1 --replicas 2\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/#step-2-list-deployments","title":"Step 2: List Deployments","text":"<pre><code># List all deployments\nkubectl get deployments\n\n# List all deployments with expanded (aka \"wide\") output\nkubectl get deployments -o wide\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get deploy \nkubectl get deployment\nkubectl get deployments\n</code></pre> <p>Note</p> <p><code>deployment</code> is abbreviated as <code>deploy</code>.</p> <p>The <code>UP-TO-DATE</code> field shows how many replicas have been updated to the latest version, while the <code>AVAILABLE</code> field shows how many replicas are currently available and ready to serve traffic.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/#step-3-view-replicasets-created-by-the-deployment","title":"Step 3: View ReplicaSets Created by the Deployment","text":"<pre><code>kubectl get rs | grep my-deployment\n</code></pre> <p>You'll see a <code>ReplicaSet</code> created and managed by the Deployment.</p> <p>The name of the <code>ReplicaSets</code> created by <code>Deployments</code> starts with the deployment name.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/#step-4-view-pods-created-by-the-deployment","title":"Step 4: View Pods Created by the Deployment","text":"<pre><code>kubectl get pods\n</code></pre> <p>You'll see Pods created and managed by the <code>Deployment</code>.</p> <p>The name of the pods created by a <code>Deployment</code> starts with the deployment name.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/#step-5-describe-a-deployment","title":"Step 5: Describe a Deployment","text":"<pre><code># Command template\nkubectl describe deployment &lt;deployment-name&gt;\n{OR}\nkubectl describe deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl describe deployment my-deployment\n{OR}\nkubectl describe deployment/my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/deployment-using-imperative-approach/#step-6-delete-a-deployment","title":"Step 6: Delete a Deployment","text":"<pre><code># Command template\nkubectl delete deployment &lt;deployment-name&gt;\n{OR}\nkubectl delete deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl delete deployment my-deployment\n{OR}\nkubectl delete deployment/my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/introduction-to-deployment/","title":"Introduction to Kubernetes Deployment","text":"<p>A Kubernetes <code>Deployment</code> tells Kubernetes how to create or modify instances of the pods.</p> <p>You describe a desired state in a <code>Deployment</code>, and the <code>Deployment Controller</code> changes the actual state to the desired state at a controlled rate.</p> <p>Using <code>Deployments</code> you can easily scale the number of replica pods, enable the rollout of updated code in a controlled manner, or roll back to an earlier deployment version if necessary.</p> <p><code>Deployment</code> in Kubernetes is an abstraction of a <code>ReplicaSet</code>. When you create a <code>Deployment</code>, Kubernetes creates a <code>ReplicaSet</code> for you and manages the replicas of your application based on the Deployment's desired state.</p> <p><code>Deployments</code> provide additional features such as rolling updates, rollbacks, and scaling strategies, which are not available with <code>ReplicaSets</code> alone.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/introduction-to-deployment/#deployment-overview","title":"Deployment Overview","text":"<p><code>Deployment</code> doesn't interact with pods directly. It manages pods using <code>ReplicaSets</code>.</p> <p> </p> <p><code>Deployment</code> does the rolling update automatically without any human interaction and increases the abstraction by one level.</p> <p>It is recommended to use <code>Deployments</code> instead of directly using <code>ReplicaSets</code>, unless you require custom update orchestration or don't require updates at all.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/pause-and-resume-deployment/","title":"Pause and Resume a Deployment Rollout","text":"<p>Pausing and resuming a deployment rollout is a common practice in software development when deploying new changes to production environments.</p> <p>This allows you to monitor the deployment closely and address any issues that may arise before continuing with the rollout.</p> <p>Let's see how you can pause and resume a deployment rollout.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/pause-and-resume-deployment/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a Deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: backend\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/pause-and-resume-deployment/#step-2-pause-the-deployment-rollout","title":"Step 2: Pause the Deployment Rollout","text":"<p>To pause a deployment rollout, you can use the following command:</p> <pre><code># Command template\nkubectl rollout pause deployment &lt;deployment-name&gt;\n{OR}\nkubectl rollout pause deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl rollout pause deployment my-deployment\n{OR}\nkubectl rollout pause deployment/my-deployment\n</code></pre> <p>This will prevent any new replicas from being created and stop the update process. You can then investigate any issues that may have occurred and make any necessary changes.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/pause-and-resume-deployment/#step-3-update-the-deployment","title":"Step 3: Update the Deployment","text":"<p>Let's update the deployment to use a new image <code>reyanshkharga/nginx:v2</code>.</p> <p>In the Deployment YAML manifest change the value of image to <code>reyanshkharga/nginx:v2</code>.</p> <p>Now, apply the manifest again:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>List Pods to verify if rollout is paused:</p> <pre><code>kubectl get pods | grep my-deployment\n</code></pre> <p>You'll see that the rollout didn't take place because the deployment rollout is paused.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/pause-and-resume-deployment/#step-4-resume-the-deployment-rollout","title":"Step 4: Resume the Deployment Rollout","text":"<p>To resume a deployment rollout, you can use the following command:</p> <pre><code># Command template\nkubectl rollout resume deployment &lt;deployment-name&gt;\n{OR}\nkubectl rollout resume deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl rollout resume deployment my-deployment\n{OR}\nkubectl rollout resume deployment/my-deployment\n</code></pre> <p>This will continue the rollout from where it left off.</p> <p>List pods to verify if rollout has resumed:</p> <pre><code>kubectl get pods | grep my-deployment\n</code></pre> <p>You'll see that the rollout resumes from where it left off because the deployment rollout is resumed.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/pause-and-resume-deployment/#clean-up","title":"Clean Up","text":"<p>Delete the deployment:</p> <pre><code>kubectl delete -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/rollback-deployment/","title":"Rollback a Kubernetes Deployment","text":"<p>In Kubernetes, a rollback is the process of reverting a <code>Deployment</code> to a previous revision.</p> <p>When you update a <code>Deployment</code>, Kubernetes stores the previous revision(s) of the Deployment's configuration and the <code>ReplicaSets</code> it created.</p> <p>If there is an issue with the new revision, you can use the <code>kubectl rollout undo</code> command to rollback to a particular revision.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/rollback-deployment/#step-1-rollback-to-previous-revision","title":"Step 1: Rollback to Previous Revision","text":"<pre><code># Command template\nkubectl rollout undo deployment &lt;deployment-name&gt;\n{OR}\nkubectl rollout undo deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl rollout undo deployment my-deployment\n{OR}\nkubectl rollout undo deployment/my-deployment\n</code></pre> <p>Verify rollback:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/rollback-deployment/#step-2-rollback-to-a-particular-revision","title":"Step 2: Rollback to a Particular Revision","text":"<p>To rollback a Kubernetes Deployment to a particular version, You can use the <code>kubectl rollout undo</code> command with the <code>--to-revision</code> flag followed by the revision number:</p> <pre><code># Command template\nkubectl rollout undo deployment &lt;deployment-name&gt; --to-revision=&lt;revision-number&gt;\n{OR}\nkubectl rollout undo deployment/&lt;deployment-name&gt; --to-revision=&lt;revision-number&gt;\n\n# Actual command\nkubectl rollout undo deployment my-deployment --to-revision=1\n{OR}\nkubectl rollout undo deployment/my-deployment --to-revision=1\n</code></pre> <p>Verify rollback:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre> <p>Here's a visual representation of the rollback the <code>Deployment</code> performs:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/rollback-deployment/#clean-up","title":"Clean Up","text":"<p>Delete the Deployment:</p> <pre><code>kubectl delete deployment my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/scale-deployment/","title":"Scale a Kubernetes Deployment","text":"<p>You can use <code>kubectl scale</code> command to scale a Kubernetes <code>Deployment</code> to a desired value.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/scale-deployment/#step-1-scale-a-deployment","title":"Step 1: Scale a Deployment","text":"<pre><code># Command template\nkubectl scale deployment &lt;deployment-name&gt; --replicas &lt;desired-count&gt;\n{OR}\nkubectl scale deployment/&lt;deployment-name&gt; --replicas &lt;desired-count&gt;\n\n# Actual command\nkubectl scale deployment my-deployment --replicas 3\n{OR}\nkubectl scale deployment/my-deployment --replicas 3\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/scale-deployment/#step-2-verify-if-the-deployment-was-scaled","title":"Step 2: Verify if the Deployment Was Scaled","text":"<p>Verfify the Deployment and Pods:</p> <pre><code># Describe deployment\nkubectl describe deploy/my-deployment\n\n# List pods\nkubectl get pods | grep my-deployment\n</code></pre> <p>You'll notice the following:</p> <ul> <li>New pods come up if the updated value of replicas is greater than the previous value (Scale up)</li> <li>Some of the old pods are terminated if the updated value of replicas is less than the previous value (Scale down)</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/scale-deployment/#use-kubectl-edit-command-to-scale-the-deployment","title":"Use kubectl edit Command to Scale the Deployment","text":"<p>You can also use <code>kubectl edit</code> command to scale the Deployment as follows:</p> <ol> <li> <p>Set the editor you want to use (Default is <code>vim</code>):</p> <pre><code>export KUBE_EDITOR=nano\n</code></pre> </li> <li> <p>Edit and update the Deployment:</p> <pre><code># Command template\nkubectl edit deployment &lt;deployment-name&gt;\n{OR}\nkubectl edit deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl edit deployment my-deployment\n{OR}\nkubectl edit deployment/my-deployment\n</code></pre> <p>Change the replicas to a desired value (For example, 2) and save the file. The Deployment will be updated.</p> </li> <li> <p>Verfify the Deployment and Pods again:</p> <pre><code># Describe deployment\nkubectl describe deploy/my-deployment\n\n# List pods\nkubectl get pods | grep my-deployment\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/update-deployment/","title":"Update a Kubernetes Deployment","text":"<p>By default, Kubernetes performs rolling updates when you update a <code>Deployment</code>. It does so by creating a new <code>ReplicaSet</code>.</p> <p>The old <code>ReplicaSets</code> are not deleted. By default, Kubernetes <code>Deployment</code> controller stores up to 10 revisions of a <code>Deployment</code>. This means that when you update a <code>Deployment</code>, the controller will keep up to 10 previous versions of the Deployment's configuration and the <code>ReplicaSets</code> it created.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/update-deployment/#step-1-update-a-deployment","title":"Step 1: Update a Deployment","text":"<p>Let's update the <code>Deployment</code> to use a new image <code>reyanshkharga/nginx:v2</code>.</p> <p>You can use <code>kubectl set image</code> command to update the Deployment to use a new image.</p> <pre><code># Command template\nkubectl set image deployment &lt;deployment-name&gt; &lt;container-name&gt;=&lt;new-image&gt;\n{OR}\nkubectl set image deployment/&lt;deployment-name&gt; &lt;container-name&gt;=&lt;new-image&gt;\n\n# Actual command\nkubectl set image deployment my-deployment nginx=reyanshkharga/nginx:v2\n{OR}\nkubectl set image deployment/my-deployment nginx=reyanshkharga/nginx:v2\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/update-deployment/#step-2-verify-if-the-deployment-was-updated","title":"Step 2: Verify if the Deployment Was Updated","text":"<p>Verfify the Deployment, Pods, and ReplicaSets:</p> <pre><code># Describe deployment\nkubectl describe deploy/my-deployment\n\n# List pods\nkubectl get pods | grep my-deployment\n\n# List replicasets\nkubectl get rs | grep my-deployment\n</code></pre> <p>You'll observe the following:</p> <ul> <li>The <code>Deployment</code> is updated and uses the updated image.</li> <li>A new <code>ReplicaSet</code> is created</li> <li>New Pods come up and old Pods are terminated.</li> <li>Old <code>ReplicaSets</code> are not deleted (unless this was 11th revision)</li> </ul> <p>Here's a visual representation of the rolling update the <code>Deployment</code> performs:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/deployment/update-deployment/#step-3-list-deployment-revisions","title":"Step 3: List Deployment Revisions","text":"<p>As discussed earlier, Kubernetes <code>Deployment</code> controller stores up to 10 revisions of a <code>Deployment</code>.</p> <p>You can configure the maximum number of revisions to keep by setting the <code>.spec.revisionHistoryLimit</code> field in the Deployment's YAML file using <code>kubectl edit deployment</code> command.</p> <p>List the Deployment revisions:</p> <pre><code># Command template\nkubectl rollout history deployment &lt;deployment-name&gt;\n{OR}\nkubectl rollout history deployment/&lt;deployment-name&gt;\n\n# Actual command\nkubectl rollout history deployment my-deployment\n{OR}\nkubectl rollout history deployment/my-deployment\n</code></pre> <p>Get the details of a particular revision:</p> <pre><code># Command template\nkubectl rollout history deployment &lt;deployment-name&gt; --revision=&lt;revision-number&gt;\n{OR}\nkubectl rollout history deployment/&lt;deployment-name&gt; --revision=&lt;revision-number&gt;\n\n# Actual command\nkubectl rollout history deployment my-deployment --revision=2\n{OR}\nkubectl rollout history deployment/my-deployment --revision=2\n</code></pre> <p>List ReplicaSets managed by the Deployment:</p> <pre><code>kubectl get rs | grep my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/init-containers-demo/","title":"Init Containers Demo","text":"<p>Now, let's see init containers in action and observe how these specialized containers, as discussed in the previous section, perform their crucial tasks within the kubernetes deployment.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/init-containers-demo/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/nodeapp</li> <li>reyanshkharga/alpine</li> </ul> <p>Note</p> <p>reyanshkharga/alpine is the modified version of famous <code>alpine</code> Docker image which includes <code>curl</code>, <code>ping</code>, and <code>nslookup</code> linux utilities.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/init-containers-demo/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>Let's use a deployment to create pods with init containers as follows: </p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n      initContainers:\n      - name: init-database\n        image: reyanshkharga/alpine\n        command: ['sh', '-c', \"until nslookup my-database-service; do echo waiting for database service to be available...; sleep 2; done\"]\n      - name: init-cache\n        image: reyanshkharga/alpine\n        command: ['sh', '-c', \"until nslookup my-cache-service; do echo waiting for cache service to be available...; sleep 2; done\"]\n</code></pre> <p>Note</p> <ul> <li> <p><code>until</code> command in Linux is used to execute a set of commands as long as the final command in the <code>until</code> commands has an exit status which is not zero.</p> </li> <li> <p><code>nslookup</code> is a command-line tool for querying DNS to retrieve information about domain names and their associated IP addresses. It is used for network troubleshooting and DNS resolution.</p> </li> </ul> <p>Here's what each init container does in the above deployment:</p> <ol> <li> <p><code>init-database</code> initializes by checking the availability of the <code>my-database-service</code> through DNS resolution, waiting until it's reachable before moving on.</p> </li> <li> <p><code>init-cache</code> performs a similar initialization, ensuring the <code>my-cache-service</code> is accessible through DNS resolution before proceeding with the main container.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/init-containers-demo/#step-2-verify-pods","title":"Step 2: Verify Pods","text":"<p>Tip</p> <p>List the pods in watch mode so that we can observe how the status changes once the init containers finish their tasks.</p> <pre><code># List pods\nkubectl get pods -w\n</code></pre> <p>You'll notice that the pods haven't finished their initialization and are currently in a non-running state. This is because the init containers are still pending completion, as the services <code>my-database-service</code> and <code>my-cache-service</code> haven't been created and are consequently unavailable.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/init-containers-demo/#step-3-create-services","title":"Step 3: Create Services","text":"<p>Let's start creating services that init containers are waiting for:</p> <code>my-database-service.yml</code> <code>my-cache-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-database-service\nspec:\n  selector:\n    name: my-database\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3306\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-cache-service\nspec:\n  selector:\n    name: my-cache\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 6379\n</code></pre> <p>Remember that <code>kubelet</code> runs each init container sequentially. So, even if you create the <code>my-cache-service</code> first, the <code>init-cache</code> init container won't run until the preceding init container <code>init-database</code> has completed successfully.</p> <ol> <li> <p>Create <code>my-database-service</code>: </p> <pre><code>kubectl apply -f my-database-service.yml\n</code></pre> <p>Once <code>my-database-service</code> is operational, you will observe that the <code>init-database</code> init container has successfully completed its tasks.</p> </li> <li> <p>Create <code>my-cache-service</code>:</p> <pre><code>kubectl apply -f my-cache-service.yml\n</code></pre> <p>Now that <code>my-cache-service</code> is also operations, <code>init-cache</code> init container would also complete its tasks successfully.</p> </li> </ol> <p>At this stage, both init containers have successfully finished their tasks. The pods in the deployment will now be initialized and the status will change to <code>PodInitializing</code>, and then eventually to <code>Running</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/init-containers-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-database-service.yml\n\u2502   |-- my-cache-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/introduction-to-init-containers/","title":"Introduction to Init Containers","text":"<p>The name \"Init\" comes from the term \"initialization\", which refers to the process of preparing something for use.</p> <p>A pod can have multiple containers running apps within it, but it can also have one or more <code>init containers</code>, which are run before the main containers are started.</p> <p>Init containers are specialized containers in kubernetes that are designed to run initialization tasks before the main containers in a pod start running.</p> <p>If you specify multiple init containers for a pod, <code>kubelet</code> runs each init container sequentially. Each init container must succeed before the next can run.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/introduction-to-init-containers/#kitchen-analogy-for-init-containers","title":"Kitchen Analogy for Init Containers","text":"<p>Let's imagine that you're the head chef preparing a meal in a busy restaurant kitchen. You have all the ingredients you need to make your dish, but some of them require special preparation before they can be used.</p> <p>This is where init containers come in. Just like how you might use a sous chef or assistant to prepare ingredients before you start cooking, init containers are used to perform any necessary setup tasks before your main container starts running.</p> <p>For example, imagine you need to use vegetables in your dish, but they need to be chopped first. You could have a sous chef (<code>init container</code>) who chops the vegetables and stores them in a separate bowl that your main container can access.</p> <p>Another example could be grinding spices, and you could have another assitant (<code>init container</code>) who grinds the spice for you and stores it in a seperate bowl that you can access.</p> <p>In this way, init containers help ensure that your main container has everything it needs to run properly and efficiently, just like how a well-prepared kitchen helps a chef create a delicious meal.</p> <p> </p> <p>Below are the kubernetes equivalent terms in the above analogy:</p> <ul> <li><code>Restaurant</code>: Kubernetes Cluster</li> <li><code>Kitchen</code>: Pod</li> <li><code>Sous chef or assistant</code>: Init Containers</li> <li><code>Head chef</code>: Main Containers</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/introduction-to-init-containers/#execution-flow-of-init-containers","title":"Execution Flow of Init Containers","text":"<p>Kubelet runs each init container sequentially. The main containers start in parallel once all init containers have completed successfully.</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/introduction-to-init-containers/#use-cases-of-init-containers","title":"Use Cases of Init Containers","text":"<p>Here are three common use cases for init containers in kubernetes:</p> <ol> <li> <p>Data Preparation: Init containers can be used to prepare data required by the main container. For example, an init container can download data from an external source or a shared volume and convert it to a format that can be consumed by the main container.</p> </li> <li> <p>Health Checks: Init containers can be used to perform pre-startup checks to ensure that the main container can start safely. For example, an init container can check if a database is available before starting the main container that depends on it.</p> </li> <li> <p>Configuration: Init containers can be used to set up configurations and secrets required by the main container. For example, an init container can populate environment variables with configuration values or mount secrets into the shared volume to be used by the main container.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/init-containers/introduction-to-init-containers/#benefits-of-init-containers","title":"Benefits of Init Containers","text":"<ol> <li> <p>Decoupling: One of the benefits of using init containers is that they can be used to perform tasks that require specialized tools or skills that are not available in the main container, without having to modify or bloat the main container image.</p> </li> <li> <p>Isolation: Init containers help ensure that the main application container only starts once all the necessary dependencies have been met. This means that you can ensure that your application container has a consistent and stable environment to run in, which can help prevent issues like dependency conflicts.</p> </li> <li> <p>Flexibility: By allowing you to set up your dependencies before your main application container starts, init containers help reduce resource waste. Instead of having your application container sit idle while it waits for dependencies to be installed, you can use an init container to ensure that everything is in place before the main container starts.</p> </li> </ol> <p>References:</p> <ul> <li>Init Containers</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/","title":"Create and Manage LimitRange","text":"<p>Let's see how we can create and manage Limit Ranges.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-1-create-a-namespace","title":"Step 1: Create a Namespace","text":"<p>Let's create a namespace first:</p> <code>dev-namespace.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    name: dev\n</code></pre> <p>Apply the manifest to create the namespace:</p> <pre><code>kubectl apply -f dev-namespace.yml\n</code></pre> <p>Verify the namespace:</p> <pre><code>kubectl get ns\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-2-create-a-limitrange-object-in-the-namespace","title":"Step 2: Create a LimitRange Object in the Namespace","text":"<p>Create a <code>LimitRange</code> object in the <code>dev</code> namespace:</p> <code>dev-limitrange.yml</code> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: dev-limitrange\n  namespace: dev\nspec:\n  limits:\n  - default: # this section defines default limits\n      cpu: 500m\n      memory: 512Mi\n    defaultRequest: # this section defines default requests\n      cpu: 100m\n      memory: 64Mi\n    max: # maximum resource limits\n      cpu: 1000m\n      memory: 1000Mi\n    min: # minumum resource limits\n      cpu: 100m\n      memory: 64Mi\n    type: Container\n</code></pre> <p>Here's a breakdown of the purpose of each field defined in <code>.spec.limits</code>:</p> <ul> <li> <p><code>default</code>: This field sets the default resource <code>limits</code> for containers. If a container does not specify its own resource <code>limits</code>, it will inherit these default values.</p> </li> <li> <p><code>defaultRequest</code>: This field sets the default resource <code>requests</code> for containers. Similar to defaults, if a container does not specify its own resource <code>requests</code>, it will use these default values.</p> </li> <li> <p><code>min</code>: The <code>min</code> field specifies the minimum resource <code>request</code> that a container must set. It enforces a lower boundary, ensuring that containers are allocated at least the specified amount of resources. This prevents resource starvation and ensures containers have the necessary resources to function properly.</p> </li> <li> <p><code>max</code>: The <code>max</code> field sets the maximum resource <code>limits</code> that a container can specify. It imposes an upper boundary, preventing containers from consuming excessive resources. This helps in maintaining fairness, preventing resource overutilization, and mitigating the impact of misbehaving or compromised containers.</p> </li> </ul> <p>The <code>type</code> field in the <code>LimitRange</code> object specifies the scope to which the resource <code>limits</code> apply. It can have two possible values:</p> <ol> <li> <p><code>Container</code>: This type indicates that the resource <code>limits</code> defined in the <code>LimitRange</code> object apply to individual containers within pods. It means that the specified limits are enforced for each container running in the namespace.</p> </li> <li> <p><code>Pod</code>: If the type is set to <code>Pod</code>, the resource limits defined in the <code>LimitRange</code> object apply at the pod level. This means that the limits specified will be shared by all containers within the same pod.</p> </li> </ol> <p>Apply the manifest to create the <code>LimitRange</code> object:</p> <pre><code>kubectl apply -f dev-limitrange.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-3-verify-the-limitrange","title":"Step 3: Verify the LimitRange","text":"<pre><code># List limitranges\nkubectl get limitrage -n dev\n\n# Describe the limitrange\nkubectl describe limitrange dev-limitrange -n dev\n</code></pre> <p>Note that <code>LimitRange</code> can also be used to enforce a ratio between <code>request</code> and <code>limit</code> for a resource in a namespace. And, that's why you would notice a field <code>Max Limit/Request Ratio</code> as well when you describe the limitrange.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-4-create-pod-in-the-dev-namespace","title":"Step 4: Create Pod in the Dev Namespace","text":"<p>Let's create pods in the <code>dev</code> namespace and observe how the default <code>request</code> and <code>limit</code> values are applied to the pods. We'll use deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre> <p>You can observe that we have not specified any <code>request</code> or <code>limit</code> for cpu or memory in the pod template.</p> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-5-verify-deployment-and-pods","title":"Step 5: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments -n dev\n\n# List pods\nkubectl get pods -n dev\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-6-describe-pod","title":"Step 6: Describe Pod","text":"<p>Let's describe one of the pods in the deployment and observe the <code>request</code> and <code>limit</code> values:</p> <pre><code>kubectl describe pod &lt;pod-name&gt; -n dev\n</code></pre> <p>You'll observe that the pod has inherited the default <code>request</code> and <code>limit</code> values defined in the <code>LimitRange</code> object in the <code>dev</code> namespace.</p> <p>Note that the default <code>request</code> and <code>limit</code> values for resources can be overriden if we explicitly specify it in the pod definition. Let's see that in the next step.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-7-override-the-default-values-explicitly","title":"Step 7: Override the Default Values Explicitly","text":"<p>Let's override the default <code>request</code> and <code>limit</code> values by explicitly providing the desired <code>request</code> and <code>limit</code> values in the pod template:</p> <p>Update the deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        # Override the default requests and limits inherited from limitrange\n        resources:\n          requests:\n            cpu: \"110m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"510m\"\n            memory: \"600Mi\"\n</code></pre> <p>Apply the manifest to update the deployment:</p> <pre><code>kubectl apply -f my-deployment-with-override.yml\n</code></pre> <p>Verify pods:</p> <pre><code>kubectl get pods -n dev\n</code></pre> <p>Describe one of the pods:</p> <pre><code>kubectl describe pod &lt;pod-name&gt; -n dev\n</code></pre> <p>You'll observe that the pod has the resource <code>request</code> and <code>limit</code> values set as specified in the pod template.</p> <p>Pleast note that when you override the <code>request</code> and <code>limit</code> values of a resource the following conditions must be satisfied:</p> <ol> <li>The <code>request</code> value of the resource can't be lesser than the <code>min</code> value specified in the limitrange.</li> <li>The <code>limit</code> value of the resource can't be greater than the <code>max</code> value specified in the limitrange.</li> </ol> <p>In our example, the <code>requests</code> value for the <code>cpu</code> resource can't be lesser than <code>100m</code> (the <code>min</code> value in limitrange definition) and the <code>limits</code> value of the <code>cpu</code> resource can't be greater than <code>1000m</code> (the <code>max</code> value in the limitrange definition).</p> <p>Similarly, the <code>requests</code> value for the <code>memory</code> resource can't be lesser than <code>64Mi</code> (the <code>min</code> value in limitrange definition) and the <code>limits</code> value of the <code>memory</code> resource can't be greater than <code>1000Mi</code> (the <code>max</code> value in the limitrange definition).</p> <p>Let's test this out in the next step.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#step-8-override-the-default-values-with-forbidden-values","title":"Step 8: Override the Default Values With Forbidden Values","text":"<p>Let's see what happens when we try to override the default values but do not adhere to the <code>min</code> and <code>max</code> limit values specified in the <code>LimitRange</code> object.</p> <p>Update the deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        # Override the default requests and limits inherited from limitrange\n        resources:\n          requests:\n            cpu: \"110m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"510m\"\n            memory: \"1200Mi\"\n</code></pre> <p>Notice that the cpu <code>request</code> and <code>limit</code> values are well within the specified <code>min</code> and <code>max</code> values defined in the <code>LimitRange</code> object (<code>100m &lt;= 110m &amp; 510m &lt;= 1000m</code>) but the memory <code>limit</code> value exceeds the ma<code>x value defined in the</code>LimitRange<code>object (</code>1024Mi &gt; 1000Mi`).</p> <p>Let's update the deployment and observe the output:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Describe the deployment:</p> <pre><code>kubectl get deploy/my-deployment -n dev\n</code></pre> <p>You'll notice that no new pods come up because the min/max constraint is not satisfied.</p> <p>You can verify the same by checking the events in the replicaset object as follows:</p> <pre><code># List replicasets in dev namespace\nkubectl get rs -n dev\n\n# Describe the replicaset to view the events\nkubectl describe rs &lt;replicaset-name&gt; -n dev\n</code></pre> <p>You'll observe an event similar to one below:</p> <pre><code>Error creating: pods \"my-deployment-5ddffd4d5d-5drmh\" is forbidden: maximum memory usage per Container is 1000Mi, but limit is 1200Mi\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/create-limitrange/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- dev-limitrange.yml\n\u2502   |-- dev-namespace.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/limitrange/introduction-to-limitrange/","title":"Introduction to LimitRange","text":"<p>In kubernetes, a <code>LimitRange</code> is an object that allows you to define <code>default</code> and <code>maximum</code> resource <code>limits</code> for pods and containers within a namespace.</p> <p>The <code>LimitRange</code> ensures that pods adhere to the specified resource constraints on CPU, memory, and storage resources, helping to manage and allocate resources effectively without manual intervention for every pod.</p> <p>When you create a <code>LimitRange</code> in a namespace, the defined <code>default</code> and <code>maximum</code> resource <code>limits</code> are automatically applied to pods within that namespace, even if you don't explicitly specify them.</p> <p>Remember that <code>LimitRange</code> is applied at the namespace level, so it affects all containers running within that namespace unless overridden by explicit pod resource requests or limits.</p> <p>it is recommended to avoid having multiple <code>LimitRange</code> objects in a namespace to prevent conflicts and confusion.</p> <p>References:</p> <ul> <li>LimitRange in Kubernetes</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/introduction-to-namespace/","title":"Introduction to Kubernetes Namespace","text":"<p>In kubernetes, a <code>namespace</code> is a way to logically divide a single physical cluster into multiple virtual clusters.</p> <p>It is used to group and isolate resources such as pods, services, and deployments based on their purpose, owner, or environment.</p> <p>Namespaces provide a way to organize and manage resources in a multi-tenant or multi-environment setup, and avoid naming conflicts.</p> <p>Each kubernetes object belongs to a namespace, and objects with the same name can coexist in different namespaces.</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/introduction-to-namespace/#initial-namespaces","title":"Initial Namespaces","text":"<p>Kubernetes starts with four initial namespaces:</p> <ol> <li> <p><code>default</code>: Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.</p> </li> <li> <p><code>kube-node-lease</code>: This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.</p> </li> <li> <p><code>kube-public</code>: This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster.</p> </li> <li> <p><code>kube-system</code>: The namespace for objects created by the kubernetes system.</p> </li> </ol> <p>References:</p> <ul> <li>Kubernetes Namespaces</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/","title":"Create and Manage Namespaces Using Declarative Approach","text":"<p>Let's see how you can create and manage kubernetes namespaces declaratively.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-1-create-a-namespace","title":"Step 1: Create a Namespace","text":"<p>Let's create two namespaces <code>dev</code> and <code>prod</code> as follows:</p> <code>dev-namespace.yml</code> <code>prod-namespace.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n</code></pre> <p>Apply the manifest files to create namespaces:</p> <pre><code># Create dev namespace\nkubectl apply -f dev-namespace.yml\n\n# Create prod namespace\nkubectl apply -f prod-namespace.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-2-list-namespaces","title":"Step 2: List Namespaces","text":"<p>List all the namespaces:</p> <pre><code>kubectl get namespaces\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-3-describe-a-namespace","title":"Step 3: Describe a Namespace","text":"<p>Describe <code>dev</code> namespace:</p> <pre><code>kubectl describe namespace dev\n</code></pre> <p>Describe <code>prod</code> namespace:</p> <pre><code>kubectl describe namespace prod\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-4-create-kubernetes-resources-in-a-namespace","title":"Step 4: Create Kubernetes Resources in a Namespace","text":"<p>Let's create two pods. One in <code>dev</code> namespace and another in <code>prod</code> namespace.</p> <code>dev-pod.yml</code> <code>prod-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dev-pod\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-pod\n  namespace: prod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n</code></pre> <p>Apply the manifest files to create pods:</p> <pre><code># Create dev-pod\nkubectl apply -f dev-pod.yml\n\n# Create prod-pod\nkubectl apply -f prod-pod.yml\n</code></pre> <p>Similarly you can create <code>Deployment</code>, <code>ReplicaSet</code>, <code>Service</code> etc. in a namespace. You just have to specify namespace in the metadata section of the resource definition.</p> <p>Resources will be configured in the <code>default</code> namespace if the <code>namespace</code> field is not set in the <code>metadata</code> section of the resource definition.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-5-list-resources-in-a-namespace","title":"Step 5: List Resources in a Namespace","text":"<p>List all the pods in the <code>dev</code> namespace:</p> <pre><code>kubectl get pods -n dev\n</code></pre> <p>List all the pods in the <code>prod</code> namespace:</p> <pre><code>kubectl get pods -n prod\n</code></pre> <p>Also, let's start a shell session inside the <code>nginx</code> container of both the pods to verify if the application is running as expected.</p> <ol> <li> <p>Verify nginx container of <code>dev-pod</code> in the <code>dev</code> namespace:</p> <pre><code># Start a shell session\nkubectl exec -it dev-pod -n dev -- bash\n\n# Get the default nginx page\ncurl localhost\n</code></pre> </li> <li> <p>Verify nginx container of <code>prod-pod</code> in the <code>prod</code> namespace:</p> <pre><code># Start a shell session\nkubectl exec -it prod-pod -n prod -- bash\n\n# Get the default nginx page\ncurl localhost\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-6-delete-a-resource-in-a-namespace","title":"Step 6: Delete a Resource in a Namespace","text":"<p>Delete <code>dev-pod</code> from <code>dev</code> namespace:</p> <pre><code>kubectl delete pod dev-pod -n dev\n{OR}\nkubectl delete -f dev-pod.yml\n</code></pre> <p>Delete <code>prod-pod</code> from <code>prod</code> namespace:</p> <pre><code>kubectl delete pod prod-pod -n prod\n{OR}\nkubectl delete -f prod-pod.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-declarative-approach/#step-7-delete-a-namespace","title":"Step 7: Delete a Namespace","text":"<p>Delete <code>dev</code> namespace:</p> <pre><code>kubectl delete ns dev\n{OR}\nkubectl delete -f dev-namespace.yml\n</code></pre> <p>Delete <code>prod</code> namespace:</p> <pre><code>kubectl delete ns prod\n{OR}\nkubectl delete -f prod-namespace.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/","title":"Create and Manage Namespaces Using Imperative Commands","text":"<p>Let's look at the imperative commands that you can use to create and manage kubernetes namespaces.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-1-create-a-namespace","title":"Step 1: Create a Namespace","text":"<pre><code># Command template\nkubectl create namespace &lt;namespace-name&gt;\n</code></pre> <p>Let's create a namespace named <code>dev</code> follows:</p> <pre><code># Create namespace dev\nkubectl create namespace dev\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-2-list-namespaces","title":"Step 2: List Namespaces","text":"<pre><code>kubectl get namespaces\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get ns \nkubectl get namespace\nkubectl get namespaces\n</code></pre> <p>Note</p> <p><code>namespace</code> is abbreviated as <code>ns</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-3-describe-a-namespace","title":"Step 3: Describe a Namespace","text":"<pre><code># Command template\nkubectl describe namespace &lt;namespace-name&gt;\n\n# Actual command\nkubectl describe namespace dev\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-4-create-kubernetes-resources-in-a-namespace","title":"Step 4: Create Kubernetes Resources in a Namespace","text":"<p>Let's create a pod in the <code>dev</code> namespace that we created:</p> <pre><code>kubectl run nginx --image=nginx -n dev\n</code></pre> <p>You can either use <code>-n</code> or <code>--namespace</code> to specify the namespace where the resource should be created.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-5-list-resources-in-a-namespace","title":"Step 5: List Resources in a Namespace","text":"<p>Let's list all the pods in the <code>dev</code> namespace:</p> <pre><code>kubectl get pods -n dev\n{OR}\nkubectl get pods --namespace dev\n</code></pre> <p>Let's start a shell session inside the nginx container to verify if the application in the pod is running as expected:</p> <pre><code>kubectl exec -it nginx -n dev -- bash\n</code></pre> <p>Get the default nginx page:</p> <pre><code>curl localhost\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-6-delete-a-resource-in-a-namespace","title":"Step 6: Delete a Resource in a Namespace","text":"<p>Let's delete the <code>nginx</code> pod in the <code>dev</code> namespace:</p> <pre><code>kubectl delete pod nginx -n dev\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/namespace/namespace-using-imperative-approach/#step-7-delete-a-namespace","title":"Step 7: Delete a Namespace","text":"<pre><code># Command template\nkubectl delete namespace &lt;namespace-name&gt;\n\n# Actual command\nkubectl delete namespace dev\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/busybox-pod/","title":"Introduction to Busybox Pod","text":"<p>Let's learn about a pod called <code>busybox</code> that we will use a lot in this course for various purposes.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/busybox-pod/#busybox-docker-image","title":"Busybox Docker Image","text":"<ul> <li>BusyBox image is a <code>Linux-based</code> image containing many handy utilities.</li> <li>The BusyBox container image is incredibly <code>lightweight</code>.</li> <li>The image is less than 3 MB in size.</li> <li>Busybox image doesn't have a <code>bash</code> shell. It has a shell at <code>/bin/sh</code>.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/busybox-pod/#create-busybox-pod","title":"Create Busybox Pod","text":"<code>busybox.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    args:\n      - /bin/sh\n      - -c\n      - sleep 3600\n  restartPolicy: Never\n</code></pre> <pre><code># Create busybox pod\nkubectl apply -f busybox.yml\n</code></pre> <ul> <li>The busybox container has an entrypoint <code>/bin/sh</code> and it has no running tasks. The process comes to an end.</li> <li>In order to keep the container alive we are making it sleep for 3600 seconds.</li> </ul> <p>You can also use the following imperative command to create a busybox pod:</p> <pre><code>kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c \"sleep 3600\"\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/busybox-pod/#clean-up","title":"Clean Up","text":"<p>Let's delete the busybox Pod.</p> <pre><code>kubectl delete -f busybox.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/imperative-vs-declarative-approach/","title":"Imperative vs Declarative Approach","text":"<p>Imperative approach describes <code>HOW</code> the program executes, while the declarative approach describes <code>WHAT</code> programs to be executed.</p> <p>The imperative approach is good for learning and interactive experimentation (e.g. Python Interactive Shell)</p> <p>The declarative approach is good for reproducible deployments and for production.</p> <p>There are two ways to deploy to Kubernetes:</p> <ol> <li>Imperatively, with the <code>kubectl</code> commands</li> <li>Declaratively, by writing manifests and using <code>kubectl apply</code></li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/introduction-to-pod/","title":"Introduction to Kubernetes Pod","text":"<p>A Kubernetes pod (as in a pod of whales or pea pod) is the smallest deployable unit in the Kubernetes system.</p> <p>It runs one or more closely related containers that share the same network and storage resources.</p> <p> </p> <p>Think of a pod like a small team of people working together on a project. Each person has a specific role to play, and they work together towards a common goal. In the same way, a Kubernetes pod consists of one or more containers that work together to provide a specific service or application.</p> <p>Pods can be easily managed and scaled up or down based on demand. For example, if your application requires more resources to handle a surge in traffic, you can simply add more pods to handle the increased load.</p> <p>Overall, Kubernetes pods provide a flexible and scalable way to deploy and manage containerized applications</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-declarative-approach/","title":"Create and Manage Kubernetes Pods Using Declarative Approach","text":"<p>Let's see how you can create and manage kubernetes Pods declaratively.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-declarative-approach/#step-1-create-pod-manifest","title":"Step 1: Create Pod Manifest","text":"<p>First, we need to write the pod manifest as follows:</p> <code>my-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: my-pod\nspec:\ncontainers:\n- name: my-container\n  image: reyanshkharga/nginx:v1\n  imagePullPolicy: Always\n  ports:\n  - containerPort: 80\n</code></pre> <p>Required fields:</p> <ul> <li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object.</li> <li><code>kind</code> - What kind of object you want to create.</li> <li><code>metadata</code> - Data that helps uniquely identify the object, including a name string, UID , and - optional namespace.</li> <li><code>spec</code> - What state you desire for the object.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-declarative-approach/#step-2-create-pod","title":"Step 2: Create Pod","text":"<p>Let's use <code>kubectl apply</code> to apply the manifest and create the pod:</p> <pre><code># Create pod\nkubectl apply -f my-pod.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-declarative-approach/#step-3-list-pods","title":"Step 3: List Pods","text":"<p>List pods and verify that the pod we created is up and running:</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-declarative-approach/#step-4-verify-application","title":"Step 4: Verify Application","text":"<p>Verify that the application in the container inside the pod is running properly:</p> <pre><code># Start shell session inside the container\nkubectl exec -it my-pod -- curl localhost\n\n# Access application\ncurl localhost\n</code></pre> <p>Also, let's use <code>kubectl port-forward</code> to access the application locally:</p> <pre><code>kubectl port-forward my-pod 5000:80\n</code></pre> <p>Open any browser and visit <code>localhost:5000</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-declarative-approach/#step-5-delete-pod","title":"Step 5: Delete Pod","text":"<pre><code># Delete the pod\nkubectl delete -f my-pod.yml\n</code></pre> <p>References:</p> <ul> <li>Pods Concept</li> <li>Pod v1 core</li> <li>Workload Resources - Pod</li> <li>ObjectMeta</li> <li>PodSpec</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/","title":"Create and Manage Kubernetes Pods Using Imperative Commands","text":"<p>Let's look at the imperative commands that you can use to create and manage Kubernetes Pods.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-1-create-a-pod","title":"Step 1: Create a Pod","text":"<pre><code># Command template\nkubectl run &lt;pod-name&gt; --image=&lt;image-name&gt;\n\n# Actual command\nkubectl run my-pod --image=reyanshkharga/nginx:v1\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-2-list-pods","title":"Step 2: List Pods","text":"<pre><code># List all pods\nkubectl get pods\n\n# List all pods with expanded (aka \"wide\") output\nkubectl get pods -o wide\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get po \nkubectl get pod\nkubectl get pods\n</code></pre> <p>Note</p> <p><code>pod</code> is abbreviated as <code>po</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-3-describe-a-pod","title":"Step 3: Describe a Pod","text":"<pre><code># Command template\nkubectl describe pod &lt;pod-name&gt;\n{OR}\nkubectl describe pod/&lt;pod-name&gt;\n\n# Actual command\nkubectl describe pod my-pod\n{OR}\nkubectl describe pod/my-pod\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-4-interact-with-a-pod","title":"Step 4: Interact With a Pod","text":"<p>Let's start a shell session inside the container:</p> <pre><code># Command template\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Actual command\nkubectl exec -it my-pod -- bash\n</code></pre> <p>Now that you are inside the container, you can run any linux commands.</p> <p>Try running the below commands:</p> <pre><code>ls\nenv\ncurl localhost\n</code></pre> <p>Note</p> <p>A pod can have multiple containers. In that case, you need to provide the container name you want to connect to as follows:</p> <pre><code>kubectl exec -it my-pod --container &lt;container-name&gt; -- bash\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-5-interact-with-pods-from-outside-the-container","title":"Step 5: Interact With Pods From Outside the Container","text":"<p>You can also run commands inside the container without starting a seperate shell session.</p> <pre><code># Interact with pod from outside the container\nkubectl exec -it my-pod -- ls /\nkubectl exec -it my-pod -- env\nkubectl exec -it my-pod -- curl localhost\n\n# If the pod has multiple containers\nkubectl exec -it my-pod --container &lt;container-name&gt; -- &lt;command&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-6-use-port-forwarding-to-access-application","title":"Step 6: Use Port Forwarding to Access Application","text":"<p><code>kubectl port forward</code> is a command that allows you to access an application or a service running on a Kubernetes cluster from your local machine.</p> <p>You can use port forwarding to investigate issues with kubernetes applications locally:</p> <pre><code># Command template\nkubectl port-forward &lt;pod-name&gt; &lt;host-port&gt;:&lt;container-port&gt;\n\n# Actual command\nkubectl port-forward my-pod 5000:80\n</code></pre> <p>Let's test it out!</p> <pre><code># Hit the port 5000 on your host machine\ncurl localhost:5000/\n</code></pre> <p>Or you can open any browser and visit <code>localhost:5000</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-7-view-logs-of-a-pod","title":"Step 7: View Logs of a Pod","text":"<pre><code># Command template\nkubectl logs &lt;pod-name&gt;\n\n# Actual command\nkubectl logs my-pod\n\n# Stream the log\nkubectl logs my-pod -f\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-using-imperative-approach/#step-8-delete-a-pod","title":"Step 8: Delete a Pod","text":"<pre><code># Command template\nkubectl delete pod &lt;pod-name&gt;\n\n# Actual command\nkubectl delete pod my-pod\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-with-labels/","title":"Kubernetes Pod With Labels","text":"<p>Labels are key/value pairs that are attached to Kubernetes objects, such as pods.</p> <p>Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-with-labels/#step-1-create-pods-with-labels","title":"Step 1: Create Pods With Labels","text":"<code>frontend.yml</code> <code>backend.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-frontend-pod\n  labels:\n    app: nginx-frontend\n    tier: frontend\n    environment: dev\nspec:\n  containers:\n  - name: nginx\n    image: reyanshkharga/nginx:v1\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-backend-pod\n  labels:\n    app: nginx-backend\n    tier: backend\n    environment: prod\nspec:\n  containers:\n  - name: nginx\n    image: reyanshkharga/nginx:v2\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n</code></pre> <p>Apply the manifest files to create pods:</p> <pre><code># Create frontend pod\nkubectl apply -f frontend.yml\n\n# Create backend pod\nkubectl apply -f backend.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-with-labels/#step-2-list-pods","title":"Step 2: List Pods","text":"<pre><code># List pods\nkubectl get pods\n\n# List pods and show labels\nkubectl get pods --show-labels\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-with-labels/#step-3-filter-pods-using-labels","title":"Step 3: Filter Pods Using Labels","text":"<ol> <li> <p>Equality based filtering:</p> <pre><code>kubectl get pods -l environment=prod\nkubectl get pods -l environment=dev\nkubectl get pods -l environment=qa\n\nkubectl get pods -l environment=prod,tier=backend\nkubectl get pods -l environment=prod,tier=frontend\nkubectl get pods -l environment=dev,tier=frontend\nkubectl get pods -l environment=qa,tier=backend\n</code></pre> </li> <li> <p>Set based filtering:</p> <pre><code>kubectl get pods -l 'environment in (prod, qa)'\nkubectl get pods -l 'environment in (dev, qa)'\nkubectl get pods -l 'environment in (prod, dev)'\nkubectl get pods -l 'environment in (prod, dev, qa)'\n\nkubectl get pods -l 'environment in (prod, qa),tier in (backend, frontend)'\nkubectl get pods -l 'environment in (dev, qa),tier in (backend, frontend)'\nkubectl get pods -l 'environment in (prod, dev),tier in (backend, frontend)'\nkubectl get pods -l 'environment in (prod, dev),tier in (database)'\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/pod/pod-with-labels/#clean-up","title":"Clean Up","text":"<pre><code># Delete frontend pod\nkubectl delete -f frontend.yml\n\n# Delete backend pod\nkubectl delete -f backend.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/introduction-to-probes/","title":"Introduction to Probes in Kubernetes","text":"<p>Let's say you have an e-commerce application running in a container, and you want to make sure it's always available for your customers to use.</p> <p>However, there's always a risk that the application could become unavailable due to a variety of reasons, such as a bug in the application, a connection error with the database, or some other issue.</p> <p>This is where kubernetes probes come in. By setting up probes for your application, kubernetes can monitor its health and automatically take action if any issues arise.</p> <p>For example, if a probe detects that the application is not responding, kubernetes can automatically restart the container or remove it from the pool of available instances, ensuring that the application is always available for your customers.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/introduction-to-probes/#what-is-a-probe-in-kubernetes","title":"What is a Probe in Kubernetes?","text":"<p>Kubernetes probes are health checks that are performed periodically by the kubelet on a container.</p> <p>To perform a health check, the kubelet either executes code within the container, or makes a network request.</p> <p>If a container fails a health check defined by a probe, kubernetes can automatically take corrective action, such as restarting the container or removing the pod from the pool of available pods.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/introduction-to-probes/#health-check-mechanisms","title":"Health Check Mechanisms","text":"<p>There are three main health check mechanisms used by probes:</p> <ol> <li> <p>exec: Executes a specified command inside the container. The diagnostic is considered successful if the command exits with a status code of <code>0</code>.</p> </li> <li> <p>httpGet: Performs an <code>HTTP GET</code> request against the pod's IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to <code>200</code> and less than <code>400</code>.</p> </li> <li> <p>tcpSocket: Performs a TCP check against the pod's IP address on a specified port. The diagnostic is considered successful if the port is open.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/","title":"Liveness Probe Demo","text":"<p>Keep in mind the following:</p> <ul> <li>Liveness probe indicates whether the container is still running and responding to requests.</li> <li>The probe is performed periodically until the container is terminated or the pod is deleted.</li> <li>If liveness probe fails, container is restarted.</li> </ul> <p> </p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#docker-image","title":"Docker Image","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p> <p>We'll be using the <code>liveness</code> tag of the image. reyanshkharga/nodeapp:liveness is a node.js application that has a 10% chance of failure.</p> <p>Note</p> <p>reyanshkharga/nodeapp:liveness is a node application with following endpoints:</p> <ul> <li><code>GET /</code> Returns a JSON object containing <code>Host</code> and <code>Version</code></li> <li><code>GET /health</code> Returns the health status of the application</li> </ul> <p>Here's the flowchart to illustrate the working of <code>reyanshkharga/nodeapp:liveness</code> app:</p> <pre><code>graph LR\n  A(Start) --&gt; B{Healthy?};\n  B --&gt;|True| C{{\"Generate random \n  number in [1,10]\"}};\n  B --&gt;|False| D(Error Response);\n  C --&gt;|Number is 9| E(\"Error response and \n  App becomes \n  unhealthy\");\n  C --&gt;|Number is not 9| F(Successful Response);</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#step-1-create-deployment-without-liveness-probe","title":"Step 1: Create Deployment Without Liveness Probe","text":"<p>First, let's create a deployment without any liveness probe and observe the behaviour of the app:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:liveness\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <ol> <li> <p>Create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> </li> <li> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#step-2-expose-application-using-a-service","title":"Step 2: Expose Application Using a Service","text":"<p>Let's create a LoadBalancer service to expose our application:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <ol> <li> <p>Create service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> </li> <li> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#step-3-access-application","title":"Step 3: Access Application","text":"<p>Open three seperate terminals to monitor the following:</p> <ol> <li> <p>Watch pods:</p> <pre><code>kubectl get pods -w\n</code></pre> </li> <li> <p>Stream logs:</p> <pre><code>kubectl logs -f &lt;pod-name&gt;\n</code></pre> </li> <li> <p>Access application:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n</code></pre> <p>Hit the root endpoint again and again until you get an error. Now, when you try to access the <code>health</code> endpoint you'll see that the app is unhealthy.</p> </li> </ol> <p>Observation</p> <p>Because the process continues to run, by default kubernetes thinks that everything is fine and continues to send requests to the pod even when it is unhealthy.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#step-4-update-the-deployment-by-adding-a-liveness-probe","title":"Step 4: Update the Deployment By Adding a Liveness Probe","text":"<p>Let's update the deployment by adding a liveness probe to the container.</p> <p>The updated deployment should look like the following:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:liveness\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 5000\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 1\n</code></pre> <p>Fields for liveness probes:</p> <ul> <li><code>initialDelaySeconds</code>: Number of seconds after the container has started before liveness probe is initiated. Defaults to 0 seconds. Minimum value is 0.</li> <li><code>periodSeconds</code>: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.</li> <li><code>timeoutSeconds</code>: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.</li> <li><code>successThreshold</code>: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup probes because the container is restarted after the probe is failed. Minimum value is 1.</li> <li><code>failureThreshold</code>: After a probe fails <code>failureThreshold</code> times in a row, kubernetes considers that the overall check has failed and the container is not ready, healthy, or live.</li> </ul> <pre><code># Update deployment\nkubectl apply -f my-deployment.yml\n</code></pre> <p>The deployment will be rolled out.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#step-5-access-application-again","title":"Step 5: Access Application Again","text":"<p>Try to access the application again:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n</code></pre> <p>Hit the root endpoint again and again until you get an error. At this point the app will become unhealthy.</p> <p>This time, we have a liveness probe set up. Kubernetes can detect when the application becomes unhealthy and will stop sending traffic to unhealthy pods. Kubernetes will also automatically restart the container.</p> <p>You can see the events by describing the pod:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>Observation</p> <p>Because the liveness probe fails, the container is restarted. (check <code>RESTARTS</code> field)</p> <p>Play with it multiple times to gain a good understanding of how liveness probe works.</p> <p>Note</p> <p>We currently have just a single replica in this deployment because my intention was to illustrate how a service behaves when it encounters an unhealthy pod. In a production environment, multiple pods will be available. The healthy pods will continue to handle traffic while any pods that do not pass the health probes will be taken out of the serving pool.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/liveness-probe-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/","title":"Kubernetes Probes in Combination","text":"<p>Now that we have learned how the liveness, readiness, and startup probes work independently, let's explore how they function when used together.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#probe-evaluation-order","title":"Probe Evaluation Order","text":"<ol> <li>If a startup probe is provided, all other probes are disabled until the startup probe succeeds.</li> <li>There is no specific order for readiness and liveness probes in Kubernetes.</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#docker-image","title":"Docker Image","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p> <p>We'll be using the <code>probes</code> tag of the image. reyanshkharga/nodeapp:probes is a node.js application with the following features:</p> <ul> <li>The app has a 60 seconds startup delay.</li> <li>The app has a 10% chance of failure.</li> </ul> <p>Note</p> <p>reyanshkharga/nodeapp:probes is a node application with following endpoints:</p> <ul> <li><code>GET /</code> Returns a JSON object containing <code>Host</code> and <code>Version</code></li> <li><code>GET /health</code> Returns the health status of the application</li> </ul> <p>Here's the flowchart to illustrate the working of <code>reyanshkharga/nodeapp:probes</code> app:</p> <pre><code>graph LR\n  A(Start) --&gt; B{Healthy?};\n  B --&gt;|True| C{{\"Generate random \n  number in [1,10]\"}};\n  B --&gt;|False| D(Error Response);\n  C --&gt;|Number is 9| E(\"Error response and \n  App becomes \n  unhealthy\");\n  C --&gt;|Number is not 9| F(Successful Response);</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#step-1-expose-application-using-a-service","title":"Step 1: Expose Application Using a Service","text":"<p>Let's create a LoadBalancer service to expose our application we'll create in the next step:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <ol> <li> <p>Create service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> </li> <li> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#step-2-create-deployment-without-any-probe","title":"Step 2: Create Deployment Without Any Probe","text":"<p>First, let's create a deployment without any probe and observe the behaviour of the app:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:probes\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <ol> <li> <p>Create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> </li> <li> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> </li> </ol> <p>Once the pod starts running, you will observe that the container is marked as <code>READY (1/1)</code>. At this point, the service <code>my-service</code> will start sending traffic to the pod as all the containers in the pod are ready.</p> <p>This situation is undesirable since the application experiences a delay of 60 seconds and cannot provide a prompt response. Also, the application might be unhealthy since it has a 10% chance of failure.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#step-3-access-application","title":"Step 3: Access Application","text":"<p>Open three seperate terminals to monitor the following:</p> <ol> <li> <p>Watch pods:</p> <pre><code>kubectl get pods -w\n</code></pre> </li> <li> <p>Stream logs:</p> <pre><code>kubectl logs -f &lt;pod-name&gt;\n</code></pre> </li> <li> <p>Access application:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n</code></pre> <p>Hit the root endpoint again and again until you get an error. Now, when you try to access the <code>health</code> endpoint you'll see that the app is unhealthy.</p> </li> </ol> <p>Observation</p> <ol> <li>You'll see <code>curl: (52) Empty reply from server</code> error in the beginning</li> <li>After 60 seconds you'll start getting response from the app</li> <li>The service keeps sending traffic to the pod even when it is unhealthy</li> </ol> <p>This behaviour is undesirable since the traffic is being served from an unhealthy pod.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#step-4-update-the-deployment-by-adding-a-liveness-probe","title":"Step 4: Update the Deployment By Adding a Liveness Probe","text":"<p>Let's update the deployment by adding startup, readiness, and liveness probes to the container.</p> <p>The updated deployment should look like the following:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:probes\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        startupProbe:\n          tcpSocket:\n            port: 5000\n          initialDelaySeconds: 70\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 1\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 5000\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 5000\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 1\n</code></pre> <pre><code># Update deployment\nkubectl apply -f my-deployment.yml\n</code></pre> <p>The deployment will be rolled out.</p> <p>Tip</p> <p>For each type of probe you can choose any health check mechanism. It really depends on the application you are running and your use case.</p> <p>For example let's say you want to use readiness probe to check if <code>deployment.jar</code> file is present in the container at a specified location or not. </p> <p>In that case your readiness probe may look something like this:</p> <pre><code>...\nreadinessProbe:\n  exec:\n    command:\n    - sh\n    - -c\n    - test -e /usr/app/deployment.jar\n</code></pre> <p>If <code>/usr/app/deployment.jar</code> doesn't exist, the readiness probe would retry the health check after a specified <code>periodSeconds</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#step-5-access-application-again","title":"Step 5: Access Application Again","text":"<p>Try to access the application again:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n</code></pre> <p>Hit the root endpoint again and again.</p> <p>You'll observe the following:</p> <ol> <li>The service doesn't send traffic to pods until the startup probe succeeds.</li> <li>The service doesn't send traffic to pods until readiness probe succeeds.</li> <li>The container is restarted when liveness probe fails.</li> <li>Livenss probe keeps checking app health and restarts the container when it becomes unhealthy.</li> <li>When a container is restarted, it undergoes startup, readiness, and liveness probes again.</li> </ol> <p>Note</p> <p>We currently have just a single replica in this deployment because my intention was to illustrate how a service behaves when it encounters an unhealthy pod. In a production environment, multiple pods will be available. The healthy pods will continue to handle traffic while any pods that do not pass the health probes will be taken out of the serving pool.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/probes-in-combination/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/","title":"Readiness Probe Demo","text":"<p>Keep in mind the following:</p> <ul> <li>Readiness probe indicates whether the container is ready to start accepting traffic.</li> <li>The probe is performed periodically until the container is terminated or the pod is deleted.</li> <li>If readiness probe fails, the service won't send traffic to the pod until the readiness probe indicates that the container is ready again.</li> </ul> <p> </p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#docker-image","title":"Docker Image","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p> <p>We'll be using the <code>delayed</code> tag of the image. reyanshkharga/nodeapp:delayed is a node.js application that has a 60 seconds startup delay.</p> <p>Note</p> <p>reyanshkharga/nodeapp:delayed is a node application with following endpoints:</p> <ul> <li><code>GET /</code> Returns a JSON object containing <code>Host</code> and <code>Version</code></li> <li><code>GET /health</code> Returns the health status of the application</li> <li><code>GET /random</code> returns a randomly generated number in <code>[1, 10]</code></li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#step-1-expose-application-using-a-service","title":"Step 1: Expose Application Using a Service","text":"<p>Let's create a LoadBalancer service to expose our application we'll create in the next step:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <ol> <li> <p>Create service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> </li> <li> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#step-2-create-deployment-without-readiness-probe","title":"Step 2: Create Deployment Without Readiness Probe","text":"<p>First, let's create a deployment without any readinesss probe and observe the behaviour of the app:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:delayed\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <ol> <li> <p>Create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> </li> <li> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> </li> </ol> <p>Once the pod starts running, you will observe that the container is marked as <code>READY (1/1)</code>. At this point, the service <code>my-service</code> will start sending traffic to the pod as all the containers in the pod are ready.</p> <p>This situation is undesirable since the application experiences a delay of 60 seconds and cannot provide a prompt response.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#step-3-access-application","title":"Step 3: Access Application","text":"<p>Open two seperate terminals to monitor the following:</p> <ol> <li> <p>Watch pods:</p> <pre><code>kubectl get pods -w\n</code></pre> </li> <li> <p>Access application:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n\n# random endpoint\ncurl &lt;load-balancer-dns&gt;/random\n</code></pre> </li> </ol> <p>Observation</p> <ul> <li><code>curl: (52) Empty reply from server</code> error is received in the beginning since the application starts with a delay of 60 seconds.</li> <li>It takes 60 seconds before we start receiving a successful response.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#step-4-update-the-deployment-by-adding-a-readiness-probe","title":"Step 4: Update the Deployment By Adding a Readiness Probe","text":"<p>Let's update the deployment by adding a readiness probe to the container.</p> <p>The updated deployment should look like the following:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:delayed\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 5000\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 1\n</code></pre> <p>Fields for readiness probes:</p> <ul> <li><code>initialDelaySeconds</code>: Number of seconds after the container has started before readiness probe is initiated. Defaults to 0 seconds. Minimum value is 0.</li> <li><code>periodSeconds</code>: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.</li> <li><code>timeoutSeconds</code>: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.</li> <li><code>successThreshold</code>: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup probes because the container is restarted after the probe is failed. Minimum value is 1.</li> <li><code>failureThreshold</code>: After a probe fails <code>failureThreshold</code> times in a row, kubernetes considers that the overall check has failed and the container is not ready, healthy, or live.</li> </ul> <pre><code># Update deployment\nkubectl apply -f my-deployment.yml\n</code></pre> <p>The deployment will be rolled out.</p> <p>Observation</p> <p>This time, even though the pod is in running state, the container is not <code>READY</code> because the readiness probe has not succeded yet.</p> <p>You can see the events by describing the pod:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#step-5-access-application-again","title":"Step 5: Access Application Again","text":"<p>Try to access the application again:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n\n# random endpoint\ncurl &lt;load-balancer-dns&gt;/random\n</code></pre> <p>You won't receive any response because the service won't send the traffic to pods that are <code>NOT READY</code>.</p> <p>This situation is desirable because we don't want to send traffic to a pod that is not ready to serve traffic.</p> <p>But once all the containers in the pod are ready, you will notice that the service starts sending traffic to the pods and you get a successful response.</p> <p>Note</p> <p>We currently have just a single replica in this deployment because my intention was to illustrate how a service behaves when it encounters an unhealthy pod. In a production environment, multiple pods will be available. The healthy pods will continue to handle traffic while any pods that do not pass the health probes will be taken out of the serving pool.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/readiness-probe-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/","title":"Startup Probe Demo","text":"<p>Keep in mind the following:</p> <ul> <li>Startup probe indicates whether the application within the container is started.</li> <li>Once the container has started successfully, the startup probe is no longer used until the pod is restarted.</li> <li>If startup probe fails, container is restarted.</li> </ul> <p> </p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#docker-image","title":"Docker Image","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p> <p>We'll be using the <code>delayed</code> tag of the image. reyanshkharga/nodeapp:delayed is a node.js application that has a 60 seconds startup delay.</p> <p>Note</p> <p>reyanshkharga/nodeapp:delayed is a node application with following endpoints:</p> <ul> <li><code>GET /</code> Returns a JSON object containing <code>Host</code> and <code>Version</code></li> <li><code>GET /health</code> Returns the health status of the application</li> <li><code>GET /random</code> returns a randomly generated number in <code>[1, 10]</code></li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#step-1-expose-application-using-a-service","title":"Step 1: Expose Application Using a Service","text":"<p>Let's create a LoadBalancer service to expose our application we'll create in the next step:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <ol> <li> <p>Create service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> </li> <li> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#step-2-create-deployment-without-startup-probe","title":"Step 2: Create Deployment Without Startup Probe","text":"<p>First, let's create a deployment without any readinesss probe and observe the behaviour of the app:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:delayed\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <ol> <li> <p>Create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> </li> <li> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> </li> </ol> <p>Once the pod starts running, you will observe that the container is marked as <code>READY (1/1)</code>. At this point, the service <code>my-service</code> will start sending traffic to the pod as all the containers in the pod are ready.</p> <p>This situation is undesirable since the application experiences a delay of 60 seconds and cannot provide a prompt response.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#step-3-access-application","title":"Step 3: Access Application","text":"<p>Open two seperate terminals to monitor the following:</p> <ol> <li> <p>Watch pods:</p> <pre><code>kubectl get pods -w\n</code></pre> </li> <li> <p>Access application:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n\n# random endpoint\ncurl &lt;load-balancer-dns&gt;/random\n</code></pre> </li> </ol> <p>Observation</p> <ul> <li><code>curl: (52) Empty reply from server</code> error is received in the beginning since the application starts with a delay of 60 seconds.</li> <li>It takes 60 seconds before we start receiving a successful response.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#step-4-update-the-deployment-by-adding-a-startup-probe","title":"Step 4: Update the Deployment By Adding a Startup Probe","text":"<p>Let's update the deployment by adding a startup probe to the container.</p> <p>The updated deployment should look like the following:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:delayed\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        startupProbe:\n          tcpSocket:\n            port: 5000\n          initialDelaySeconds: 70\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 1\n</code></pre> <p>Fields for startup probes:</p> <ul> <li><code>initialDelaySeconds</code>: Number of seconds after the container has started before startup probe is initiated. Defaults to 0 seconds. Minimum value is 0.</li> <li><code>periodSeconds</code>: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.</li> <li><code>timeoutSeconds</code>: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.</li> <li><code>successThreshold</code>: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup probes because the container is restarted after the probe is failed. Minimum value is 1.</li> <li><code>failureThreshold</code>: After a probe fails <code>failureThreshold</code> times in a row, kubernetes considers that the overall check has failed and the container is not ready, healthy, or live.</li> </ul> <p>Important Note</p> <p>Note that we have set the <code>initialDelaySeconds</code> to 70 seconds because we know that the app has a startup delay of 60 seconds. If we keep <code>initialDelaySeconds</code> value to a lower value (say 10 seconds) the container will keep restarting again and again because the startup probe would fail again and again.</p> <pre><code># Update deployment\nkubectl apply -f my-deployment.yml\n</code></pre> <p>The deployment will be rolled out.</p> <p>Observation</p> <p>This time, even though the pod is in running state, the container is not <code>READY</code> because the startup probe has not succeded yet.</p> <p>You can see the events by describing the pod:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#step-5-access-application-again","title":"Step 5: Access Application Again","text":"<p>Try to access the application again:</p> <pre><code># root endpoint\ncurl &lt;load-balancer-dns&gt;/\n\n# health endpoint\ncurl &lt;load-balancer-dns&gt;/health\n\n# random endpoint\ncurl &lt;load-balancer-dns&gt;/random\n</code></pre> <p>You won't receive any response because the service won't send the traffic to pods that are <code>NOT READY</code>.</p> <p>This situation is desirable because we don't want to send traffic to a pod that is not ready to serve traffic.</p> <p>But once all the containers in the pod are ready, you will notice that the service starts sending traffic to the pods and you get a successful response.</p> <p>Note</p> <p>We currently have just a single replica in this deployment because my intention was to illustrate how a service behaves when it encounters an unhealthy pod. In a production environment, multiple pods will be available. The healthy pods will continue to handle traffic while any pods that do not pass the health probes will be taken out of the serving pool.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/startup-probe-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/types-of-probes/","title":"Types of Probes in Kubernetes","text":"<p>There are three types of probes in kubernetes:</p> <ol> <li>Liveness Probe</li> <li>Readiness Probe</li> <li>Startup Probe</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/types-of-probes/#liveness-probe","title":"Liveness Probe","text":"<p>Liveness probe indicates whether the container is still running and responding to requests.</p> <p>If the liveness probe fails, kubernetes assumes that the container is no longer working properly and will automatically restart the container to try and restore the desired state of the deployment.</p> <p>For example let\u2019s imagine a scenario where your app has a nasty case of deadlock, causing it to hang indefinitely and stop serving requests. Because the process continues to run, by default kubernetes thinks that everything is fine and continues to send requests to the broken pod. </p> <p>By using a liveness probe, kubernetes detects that the app is no longer serving requests and restarts the offending pod.</p> <p>For instance, you can add a liveness probe to check if <code>HTTP GET</code> request to <code>/health</code> endpoint is successful.</p> <p>Here's a visual representation of how liveness probes work:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/types-of-probes/#readiness-probe","title":"Readiness Probe","text":"<p>Readiness probe indicates whether the container is ready to start accepting traffic.</p> <p>If the readiness probe fails, the kubernetes control plane assumes that the container is not ready to receive traffic and removes it from the service endpoints list. This means that the service won't send traffic to the pod until the readiness probe indicates that the container is ready again.</p> <p>Let's say you have a web application that takes some time to start up and become ready to serve traffic. When a new instance of the application is deployed, kubernetes may start sending traffic to it before it is fully ready, resulting in errors or slow responses. </p> <p>By using a readiness probe, kubernetes can check if the application instance is fully ready to start serving traffic. Once the instance passes the readiness probe check, kubernetes starts sending traffic to it.</p> <p>For instance, you can add a readiness probe to check if <code>HTTP GET</code> request to <code>/health</code> endpoint is successful.</p> <p>Here's a visual representation of how readiness probes work:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/types-of-probes/#startup-probe","title":"Startup Probe","text":"<p>Startup probe indicates whether the application within the container is started.</p> <p>If a startup probe is provided, all other probes are disabled until the startup probe succeeds.</p> <p>If the startup probe fails, kubernetes assumes that the container has failed to start properly and will restart the container. This is because kubernetes assumes that if the container has failed to start properly, it won't be able to function correctly and needs to be restarted.</p> <p>Let's say you have a web application that takes some time to start up and become ready to serve traffic. When a new instance of the application is deployed, kubernetes may start sending traffic to it before it is started properly, resulting in errors or slow responses. </p> <p>By using a startup probe, kubernetes can check if the application instance is started properly. Once the instance passes the startup probe check, kubernetes starts sending traffic to it.</p> <p>For example, you can add a startup probe to check if <code>TCP</code> connection on port <code>5000</code> succeeds.</p> <p>Here's a visual representation of how startup probes work:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/probes/types-of-probes/#kitchen-analogy-for-kubernetes-probes","title":"Kitchen Analogy For Kubernetes Probes","text":"<p>Imagine you're running a restaurant kitchen with multiple gas stoves, and you want to make sure that all stoves are functioning properly and ready to cook dishes. In kubernetes, stoves can be thought of as pods, and dishes can be thought of as requests.</p> <p>To ensure that the stoves are healthy and ready to cook, you can use probes, which are like kitchen timers that periodically check the health of each stove.</p> <p>Liveness Probe:</p> <p>A liveness probe is like a kitchen timer that periodically checks if a stove is still functioning properly. For example, you can set a timer to check the temperature of each stove every 5 minutes. If the temperature is within the expected range, the stove is considered healthy. If the temperature is too low or too high, the stove is considered unhealthy, and will be shut down and restarted.</p> <p>Readiness Probe:</p> <p>A readiness probe is like a kitchen timer that checks if a stove is ready to cook a dish. For example, you can check if the stove has reached the desired temperature before placing a pot on it. If the stove is not ready, you're notified to wait until it's ready before starting to cook. Similarly, in Kubernetes, a readiness probe checks if a pod is ready to serve requests. If a pod is not ready, kubernetes stops sending traffic to it until it passes the readiness check.</p> <p>Startup Probe:</p> <p>A startup probe is like a kitchen timer that checks if a stove has finished preheating and is ready to cook. For example, you can set a timer to check the temperature of a stove every 30 seconds for the first 2 minutes after it's turned on. If the stove reaches the desired temperature within that time, it's considered ready to cook. Similarly, in kubernetes, a startup probe checks if a pod has finished initializing and is ready to serve requests. If a pod is not ready, kubernetes restarts it until it passes the startup check.</p> <p>By using these probes, you can ensure that all stoves (container) in your restaurant (kubernetes cluster) kitchen (pod) are healthy and ready to cook dishes (serve requests), providing a great dining experience for your customers.</p> <p>References:</p> <ul> <li>Readiness and Liveness Probes - Google Article</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/challenges-with-replicasets/","title":"Challenges With ReplicaSets","text":"<p>Although <code>ReplicaSets</code> in Kubernetes ensure high availability of applications, they lack certain critical features that are essential for production-level deployments.</p> <p>The two main challenges with <code>ReplicaSets</code> are as follows:</p> <ol> <li> <p>Manual intervention is required to perform a rolling update of the application, which can be a time-consuming and error-prone process, especially for complex applications with multiple replicas.</p> </li> <li> <p>Rollbacks can be challenging to manage since they require the user to manually update the <code>ReplicaSet</code> to the previous version. This can be a complex and error-prone process, particularly for large and complex applications.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/challenges-with-replicasets/#1-rolling-update-using-replicaset","title":"1. Rolling Update Using ReplicaSet","text":"<p>In Kubernetes, a rolling update is a deployment strategy that allows updates to be made to a running application without downtime.</p> <p>It involves updating a subset of instances at a time, while ensuring that the application remains available to users during the update process.</p> <p>With <code>ReplicaSets</code> manual intervention is required to perform a rolling update of the application.</p> <p>Here's an example:</p> <p>Let's say you have a <code>ReplicaSet</code> named <code>ReplicaSet-A</code> running two identical pods and the name of pods are <code>Pod-A</code> and <code>Pod-B</code>. Let's say <code>Pod-A</code> and <code>Pod-B</code> are using image <code>nginx:v1</code>.</p> <p>Now, what if you want to update your application to use a new version of the image, let's say <code>nginx:v2</code>? Also, you want the update to be a rolling update to ensure your application is running without any downtime.</p> <p>With <code>ReplicaSet</code>, you don't have an easy option. You must perform the following operations to do the rolling update:</p> <ol> <li>Create a new <code>ReplicaSet</code>, let's say <code>ReplicaSet-B</code> that uses the updated image <code>nginx:v2</code></li> <li>Scale up <code>ReplicaSet-B</code> to add one pod</li> <li>Scale down <code>ReplicaSet-A</code> to remove one pod</li> <li>Scale up <code>ReplicaSet-B</code> to add another pod</li> <li>Scale down <code>ReplicaSet-A</code> to remove another pod</li> <li>Delete <code>ReplicaSet-A</code></li> </ol> <p>Here's a visual representation of the rolling update flow using <code>ReplicaSet</code>:</p> <p> </p> <p>You might say why not create <code>ReplicaSet-B</code> with 2 pods at once and then delete the <code>ReplicaSet-A</code>?</p> <p>While this is a valid option and will update the application, you are using extra resources (4 pods at a time) and it is not a good practice. You must optimize the resource (Worker node CPU and Memorey) utilization.</p> <p>In production you may have 50 replicas and it may not feasible to have 100 pods running at a time.</p> <p>This is a tedious process and is error-prone. What if there was an automated way to do this rolling update?</p> <p>And, that's exactly what a Kubernetes <code>Deployment</code> does.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/challenges-with-replicasets/#2-rollback-using-replicaset","title":"2. Rollback Using ReplicaSet","text":"<p>In Kubernetes, a rollback is the process of reverting an application to a previous version in case of issues or failures with the current version.</p> <p>It is a critical feature for ensuring high availability and reliability of applications in production environments.</p> <p>In this case too, you need to create a new <code>ReplicaSet</code> and then do the rolling update.</p> <p>In the previous example what is you need to rollback to the previous version of the application that was using the image <code>nginx:v1</code>?</p> <p>With <code>ReplicaSet</code>, you don't have an easy option. You must perform a manual rolling update to do the rollback:</p> <ol> <li>Create a new <code>ReplicaSet</code>, let's say <code>ReplicaSet-C</code> that uses the previous image <code>nginx:v1</code></li> <li>Scale up <code>ReplicaSet-C</code> to add one pod</li> <li>Scale down <code>ReplicaSet-B</code> to remove one pod</li> <li>Scale up <code>ReplicaSet-C</code> to add another pod</li> <li>Scale down <code>ReplicaSet-B</code> to remove another pod</li> <li>Delete <code>ReplicaSet-B</code></li> </ol> <p>Here's a visual representation of the rollback flow using <code>ReplicaSet</code>:</p> <p> </p> <p>Again, this is a manual and tedious process that is error-prone. You shouldn't do this in production.</p> <p>Kubernetes <code>Deployment</code> does it automatically! You just need to provide the version you need to rollback to. We'll see that later when we discuss kubernetes <code>Deployment</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/","title":"Create and Manage Kubernetes ReplicaSets","text":"<p>Let's see how you can create and manage <code>ReplicaSets</code>.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p> <p>Note</p> <p>You cannot create a <code>ReplicaSet</code> using imperative command. The only way to create ReplicaSets is by using the declarative approach.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-1-create-replicaset-manifest","title":"Step 1. Create ReplicaSet Manifest","text":"<p>First, we need to write the <code>ReplicaSet</code> manifest as follows:</p> <code>my-replicaset.yml</code> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: my-replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: backend\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 80\n</code></pre> <p>Required fields:</p> <ul> <li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object.</li> <li><code>kind</code> - What kind of object you want to create.</li> <li><code>metadata</code> - Data that helps uniquely identify the object, including a name string, UID , and - optional namespace.</li> <li><code>spec</code> - What state you desire for the object.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-2-create-replicaset","title":"Step 2: Create ReplicaSet","text":"<p>Let's use <code>kubectl apply</code> to apply the manifest and create the ReplicaSet.</p> <pre><code># Create replicaset\nkubectl apply -f my-replicaset.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-3-list-replicasets","title":"Step 3: List ReplicaSets","text":"<pre><code># List all replicasets\nkubectl get replicasets\n\n# List all replicasets with expanded (aka \"wide\") output\nkubectl get replicasets -o wide\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get rs \nkubectl get replicaset\nkubectl get replicasets\n</code></pre> <p>Note</p> <p><code>replicaset</code> is abbreviated as <code>rs</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-4-describe-a-replicaset","title":"Step 4: Describe a ReplicaSet","text":"<pre><code># Command template\nkubectl describe rs &lt;replicaset-name&gt;\n{OR}\nkubectl describe rs/&lt;replicaset-name&gt;\n\n# Actual command\nkubectl describe rs my-replicaset\n{OR}\nkubectl describe rs/my-replicaset\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-5-scale-a-replicaset","title":"Step 5: Scale a ReplicaSet","text":"<p>You can use <code>kubectl scale</code> command to scale a ReplicaSet up or down by changing the desired number of replicas.</p> <pre><code># Command template\nkubectl scale rs &lt;replicaset-name&gt; --replicas &lt;desired-count&gt;\n{OR}\nkubectl scale rs/&lt;replicaset-name&gt; --replicas &lt;desired-count&gt;\n\n# Actual command - scale up\nkubectl scale rs my-replicaset --replicas 3\n{OR}\nkubectl scale rs/my-replicaset --replicas 3\n\n# Actual command - scale down\nkubectl scale rs my-replicaset --replicas 1\n{OR}\nkubectl scale rs my-replicaset --replicas 1\n</code></pre> <p>Or, you can change the value of <code>replicas</code> field to a desired value in the manifest and then apply the manifest again. This method is desirable and recommended.</p> <pre><code># Update the replicaset\nkubectl apply -f my-replicaset.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-6-replicaset-in-action","title":"Step 6: ReplicaSet in Action","text":"<p>Let's delete a pod in the <code>ReplicaSet</code> and notice what happens.</p> <pre><code>kubectl delete pod &lt;pod-name&gt;\n</code></pre> <p>You will notice that as soon as the pod terminates, the <code>ReplicaSet</code> will spawn a new pod using the template to ensure that the desired number of pods is always running.</p> <pre><code># Watch and continuously update the status of pods in real-time\nkubectl get pods -w\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/create-replicaset/#step-7-delete-a-replicaset","title":"Step 7: Delete a ReplicaSet","text":"<pre><code># Command template\nkubectl delete rs &lt;replicaset-name&gt;\n{OR}\nkubectl delete rs/&lt;replicaset-name&gt;\n\n# Actual command\nkubectl delete rs my-replicaset\n{OR}\nkubectl delete rs/my-replicaset\n</code></pre> <p>References:</p> <ul> <li>ReplicaSet Concept</li> <li>ReplicaSet v1 apps</li> <li>Workload Resources - ReplicaSet</li> <li>ObjectMeta</li> <li>ReplicaSetSpec</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/","title":"How Does ReplicaSet Manage Pods?","text":"<p>Standalone pods are like orphans. Nobody cares even if they die. Your application will be unavailable if the pod dies.</p> <p>On the other hand, pods managed by a <code>ReplicaSet</code> have a much better life. If for some reason they die, <code>ReplicaSet</code> will launch a new identical Pod. This ensures your application is available all the time.</p> <p>But how does a <code>ReplicaSet</code> know which pods to manage so that it can restart a pod when required or kill the pods that are not needed?</p> <p>A <code>ReplicaSet</code> uses labels to match the pods that it will manage.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#example","title":"Example","text":"<p>Consider the following <code>ReplicaSet</code>:</p> <code>my-replicaset.yml</code> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: my-replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: backend\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 80\n</code></pre> <p>Here\u2019s what happens when you apply the manifest to create the <code>ReplicaSet</code>:</p> <ol> <li>The <code>ReplicaSet</code> controller checks for pods that match the labels defined in the <code>matchLabels</code> field of the <code>ReplicaSet</code> manifest. (<code>app=nginx</code> or <code>tier=backend</code>)</li> <li>If such a pod is found then the <code>ReplicaSet</code> controller checks if the pod is already managed by another controller such as a <code>ReplicaSet</code> or a <code>Deployment</code>. (<code>ownerReferences</code> field in the pod manifest can be used to find the owner of the object.)</li> <li>If the pod is not managed by any other controller, the <code>ReplicaSet</code> will start managing the pod. Subsequently, the <code>ownerReferences</code> field of the target pods will be updated to reflect the new owner\u2019s data (The <code>ReplicaSet</code> in this case).</li> <li>The <code>ReplicaSet</code> will also launch new pods if needed to maintain the stable set of replicas defined in the <code>ReplicaSet</code> manifest and update the <code>ownerReferences</code> field of those pods.</li> </ol> <p>Here's a visual representation of the flow described above:</p> <p> </p> <p>Let's see this in action!</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#step-1-create-a-standalone-pod","title":"Step 1: Create a Standalone Pod","text":"<p>First, create a standalone pod as follows:</p> <code>my-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: reyanshkharga/nginx:v2\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 80\n</code></pre> <p>Apply the manifest to create the pod:</p> <pre><code># Create standalone pod\nkubectl apply -f my-pod.yml\n</code></pre> <p>List pods to verify that the pod is running:</p> <pre><code>kubectl get pods\n</code></pre> <p>Verify that the pod doesn't have <code>ownerReferences</code> field in the metadata. You can do so by retrieving the detailed information about the pod and output in YAML format as follows:</p> <pre><code>kubectl get pod my-pod -o yaml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#step-2-create-a-replicaset","title":"Step 2: Create a ReplicaSet","text":"<p>Let's create a <code>ReplicaSet</code> as follows:</p> <code>my-replicaset.yml</code> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: my-replicaset\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: backend\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 80\n</code></pre> <p>Apply the manifest to create the ReplicaSet:</p> <pre><code># Create replicaset\nkubectl apply -f my-replicaset.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#step-3-list-pods","title":"Step 3: List Pods","text":"<p>Let's list all the pods:</p> <pre><code>kubectl get pods\n</code></pre> <p>You'll notice that only one new pod comes up even though <code>replica</code> is set to 2 in the ReplicaSet definition.</p> <p>This is because <code>my-pod</code> has the label <code>app: nginx</code> that the <code>ReplicaSet</code> uses to manage pods. And it has no owner.</p> <p>Therefore, the <code>ReplicaSet</code> starts managing <code>my-pod</code> and creates a new pod to maintain 2 replicas as defined in the <code>ReplicaSet</code> manifest.</p> <p>Also, the <code>ReplicaSet</code> adds a <code>ownerReferences</code> metadata to <code>my-pod</code> that indicates that <code>my-replicaset</code> is the owner of the pod <code>my-pod</code>.</p> <p>Verify that <code>my-pod</code> has a <code>ownerReferences</code> field in the metadata as follows:</p> <pre><code>kubectl get pod my-pod -o yaml\n</code></pre> <p>Verify the same for other pod that was created by <code>my-replicaset</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#step-4-delete-pods","title":"Step 4: Delete Pods","text":"<p>Now, let's see what happens when we delete one of the pods managed by the <code>ReplicaSet</code>.</p> <p>Delete a pod:</p> <pre><code>kubectl delete pod &lt;pod-name&gt;\n</code></pre> <p><code>ReplicaSet</code> will launch a new pod using the template defined in the <code>ReplicaSet</code> definition.</p> <p>Now, let's delete the pod <code>my-pod</code>:</p> <pre><code>kubectl delete pod my-pod\n</code></pre> <p><code>ReplicaSet</code> will again launch a new pod using the template defined in the <code>ReplicaSet</code> definition to maintain 2 replicas.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#important-note","title":"Important Note","text":"<p>In real world you rarely create a standalone pod. pods are usually managed by a <code>ReplicaSet</code> or a <code>Deployment</code> object in kubernetes.</p> <p>The idea of <code>ReplicaSets</code> is to manage identical Pods.</p> <p>But standalone pods may be associated with <code>ReplicaSets</code> if it has labels that the <code>ReplicaSet</code> uses to manage pods even though the standalone Pod is an entirely different application.</p> <p>So, even if you create standalone pods, make sure that it doesn't have a conflicting labels.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/how-does-replicaset-manage-pods/#clean-up","title":"Clean Up","text":"<pre><code># Delete pod\nkubectl delete -f my-pod.yml\n\n# Delete replicaset\nkubectl delete -f my-replicaset.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/introduction-to-replicaset/","title":"Introduction to Kubernetes ReplicaSet","text":"<p>In kubernetes, a <code>ReplicaSet</code> is a controller whose purpose is to maintain a stable set of replica pods running at any given time.</p> <p>Imagine you have a pod that needs to run multiple copies of itself in order to handle a high volume of requests. You can create a <code>ReplicaSet</code> object in Kubernetes that specifies how many replicas (copies) of the pod should be running at all times.</p> <p>If a pod fails or is deleted for some reason, the <code>ReplicaSet</code> will automatically create a new pod to replace it, ensuring that the desired number of replicas is always maintained. This helps to ensure that your application is always available and responsive to incoming requests.</p> <p>Overall, a <code>ReplicaSet</code> is an important tool for managing and scaling containerized applications in Kubernetes.</p> <p><code>ReplicaSet</code> uses labels to match the pods that it will manage.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/replicaset/introduction-to-replicaset/#replicaset-overview","title":"ReplicaSet Overview","text":"<p>Here's a GIF showing how a <code>ReplicaSet</code> maintains a stable set of replica pods:</p> <p> </p> <ol> <li>The <code>ReplicaSet</code> manages a pod with a replica count of 3.</li> <li>For some reason, the pod <code>my-pod-tzy</code> ceases to function.</li> <li>The ReplicaSet detects the pod <code>my-pod-tzy</code> has stopped and launches a replica pod <code>my-pod-bhj</code> to ensure that 3 replicas of the pod are always running.</li> <li><code>my-pod-abc</code> experiences an unexpected failure, and the <code>ReplicaSet</code> once again detects this issue and deploys a replica pod <code>my-pod-xyz</code> to maintain 3 replicas of the pod.</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/","title":"Configure Pods With Request and Limit","text":"<p>Let's see how we can configure pods with <code>requests</code> and <code>limits</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/#step-1-create-pods-with-request-and-limit","title":"Step 1: Create Pods With Request and Limit","text":"<p>Let's create pods with <code>request</code> and <code>limt</code>. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: stress\n  template:\n    metadata:\n      labels:\n        app: stress\n    spec:\n      containers:\n      - name: my-container\n        image: progrium/stress\n        command: ['sh', '-c', \"sleep 3600\"]\n        resources:\n          requests:\n            cpu: \"250m\"\n            memory: \"64Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"128Mi\"\n</code></pre> <p>Observe the following:</p> <ul> <li>We have requested <code>250m</code> CPU and <code>64Mi</code> Memory</li> <li>We have set a limit of <code>500m</code> on CPU and <code>128Mi</code> on memory</li> </ul> <p>Note</p> <p>The <code>progrium/stress</code> is a Docker image for <code>stress</code>, a tool for generating workload. It can produce CPU, memory, I/O, and disk stress.</p> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n\n# Describe pod\nkubectl describe pod &lt;pod-name&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/#step-3-view-cpu-and-memory-usage-of-pod","title":"Step 3: View CPU and Memory Usage of Pod","text":"<pre><code># View resource utilization\nkubectl top pod &lt;pod-name&gt;\n\n# View resource utilization of the pod periodically (defaults to every 2 seconds)\nwatch kubectl top pod &lt;pod-name&gt;\n\n# View resource utilization of the pod every second\nwatch -n 1 kubectl top pod &lt;pod-name&gt;\n</code></pre> <p>Due to the absence of any load and the process being idle, you will observe that the pod consumes minimal CPU and memory resources.</p> <p>Note that <code>kubectl top</code> command doesn't have <code>-w</code> option. We are using the <code>watch</code> command in linux as a workaround.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/#step-4-cpu-stress-test","title":"Step 4: CPU Stress Test","text":"<p>Let's induce CPU stress using the <code>stress</code> command-line utility in Linux and analyze the CPU utilization patterns of the pod.</p> <p>First, let's open two terminals. One to watch pods and another to monitor the CPU utilization of the pod:</p> <pre><code># Watch the pod every second\nkubectl get pod &lt;pod-name&gt; -w\n\n# View the resource utilization of the pod\nwatch kubectl top pod &lt;pod-name&gt;\n</code></pre> <p>Now, induce the CPU stress in the pod:</p> <pre><code># Start a shell session inside the pod\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Stress the cpu\nstress --cpu 2 --timeout 60s\n</code></pre> <p>Here's an explanation of the <code>stress</code> command:</p> <ul> <li><code>stress</code>: This is the name of the command-line tool used for generating system stress.</li> <li><code>--cpu 2</code>: This option specifies the number of CPU workers to create for inducing stress.</li> <li><code>--timeout 60s</code>: This option sets the duration for which the stress will be applied.</li> </ul> <p>You will notice that the CPU utilization begins to spike but does not exceed the cpu value specified in the <code>limits</code> field of the deployment manifest.</p> <p>Please note that kubernetes allows for short bursts of CPU usage that can exceed the specified limit. These bursts allow pods to utilize additional CPU resources temporarily. However, if the sustained usage consistently exceeds the <code>limit</code>, kubernetes will throttle the CPU allocation and enforce the <code>limit</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/#step-5-memory-stress-test","title":"Step 5: Memory Stress Test","text":"<p>Let's induce Memory stress using the <code>stress</code> command-line utility in Linux and analyze the Memory utilization patterns of the pod.</p> <p>First, let's open two terminals. One to watch pods and another to monitor the Memory utilization of the pod:</p> <pre><code># Watch the pod every second\nkubectl get pod &lt;pod-name&gt; -w\n\n# View the resource utilization of the pod\nwatch kubectl top pod &lt;pod-name&gt;\n</code></pre> <p>Now, induce the Memory stress in the pod:</p> <pre><code># Start a shell session inside the pod\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Stress the memory\nstress --vm 1000 --vm-bytes 1048576 --timeout 60s\n</code></pre> <p>Here's an explanation of the <code>stress</code> command:</p> <ul> <li><code>stress</code>: This is the name of the command-line tool used for generating system stress.</li> <li><code>--vm 1000</code>: This option specifies the number of virtual memory workers to create gradually.</li> <li><code>--vm-bytes 1048576</code>: This option sets the amount of memory allocated to each virtual memory worker. In this case, it is set to 1048576 bytes, which is equivalent to 1 mebibyte (Mi).</li> <li><code>--timeout 60s</code>: This option sets the duration for which the stress test will be active.</li> </ul> <p>In essence, the command attempts to utilize <code>1000Mi</code> of memory.</p> <p>You will notice that the memory utilization begins to spike but does not exceed the memory value specified in the <code>limits</code> field of the deployment manifest.</p> <p>In some cases, you may observe that the container is terminated with the status <code>OOMKilled</code>. This occurs when the main process within the container is unable to continue execution due to insufficient available resources, particularly memory.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/configure-pods-with-requests-and-limits/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/install-metrics-server/","title":"Install Metrics Server on EKS Cluster","text":"<p>Metrics Server is a tool in kubernetes that tracks and provides information about how much <code>CPU</code> and <code>memory</code> resources are being used by nodes and pods in the cluster.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/install-metrics-server/#step-1-deploy-the-metrics-server","title":"Step 1: Deploy the Metrics Server","text":"<p>Deploy the latest or specific release of metrics server as follows:</p> <pre><code># Deploy the latest metrics server\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n{OR}\n\n# Deploy a specific version of the metrics server\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.3/components.yaml\n</code></pre> <p>You can check all the releases here: metrics server releases</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/install-metrics-server/#step-2-verify-the-metrics-server","title":"Step 2: Verify the Metrics Server","text":"<p>Lets' verify the status of the <code>metrics-server</code> APIService. It could take a few minutes to be available.</p> <pre><code># Verify the status of metrics server\nkubectl get apiservice v1beta1.metrics.k8s.io -o json | jq '.status'\n</code></pre> <p>Once the <code>metrics server</code> is available, you should see an output similar to the one below:</p> <pre><code>{\n  \"conditions\": [\n    {\n      \"lastTransitionTime\": \"2023-06-04T03:44:08Z\",\n      \"message\": \"all checks passed\",\n      \"reason\": \"Passed\",\n      \"status\": \"True\",\n      \"type\": \"Available\"\n    }\n  ]\n}\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/install-metrics-server/#step-3-view-cpu-and-memory-usage-of-nodes","title":"Step 3: View CPU and Memory Usage of Nodes","text":"<pre><code># View CPU and Memory usage of all the nodes\nkubectl top nodes\n\n# View CPU and Memory usage of a particular node\nkubectl top node &lt;node-name&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/install-metrics-server/#step-4-view-cpu-and-memory-usage-of-pods","title":"Step 4: View CPU and Memory Usage of Pods","text":"<pre><code># View CPU and Memory usage of all the pods\nkubectl top pods\n\n# View CPU and Memory usage of a particular pod\nkubectl top pod &lt;pod-name&gt;\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/introduction-to-requests-and-limits/","title":"Introduction to Requests and Limits","text":"<p>By default, containers run with unbounded compute resources on a kubernetes cluster.</p> <p>When you create a pod, you can optionally specify how much of each resource a container needs. The most common resources to specify are <code>CPU</code> and <code>Memory</code>.</p> <p>When you specify the resource <code>request</code> for containers in a pod, the <code>kube-scheduler</code> uses this information to decide which node to place the pod on.</p> <p>When you specify a resource <code>limit</code> for a container, the <code>kubelet</code> enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.</p> <p>The <code>kubelet</code> also reserves at least the request amount of that system resource specifically for that container to use.</p> <p>If the node where a pod is running has enough of a resource available, it's possible (and allowed) for a container to use more resource than its <code>request</code>. However, a container is not allowed to use more than its resource <code>limit</code>.</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/introduction-to-requests-and-limits/#unit-of-cpu-and-memory-resources","title":"Unit of CPU and Memory Resources","text":"<p>In kubernetes, <code>CPU</code> resources are defined in <code>m</code> which stands for <code>millicores</code>. If your container needs two full cores to run, you would put the value <code>2000m</code>. If your container only needs one fourth of a core, you would put a value of <code>250m</code>.</p> <pre><code>1000m = 1 CPU Core\n</code></pre> <p>In kubernetes, <code>Memory</code> resources are defined in <code>Mi</code> which stands for <code>mebibytes</code>. </p> <p><code>Megabyte</code> and <code>mebibyte</code> are close in size. (<code>1 Mi = 1.048576 MB</code>).</p> <pre><code>1 Megabyte (MB) = (1000)^2 bytes = 1000000 bytes\n1 Mebibyte (Mi) = (1024)^2 bytes = 1048576 bytes\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/introduction-to-requests-and-limits/#requests-and-limits-an-example","title":"Requests and Limits - An Example","text":"<p>Suppose you allocate a memory <code>request</code> of <code>256 MiB</code> to a container within a pod that is scheduled to a node with <code>8GiB</code> of memory, and there are no other pods on that node. In such a scenario, the container has the flexibility to utilize more RAM if necessary.</p> <p>However, if you set a memory <code>limit</code> of <code>4GiB</code> for the same container, the <code>kubelet</code> (along with the container runtime) ensures that the <code>limit</code> is enforced. The container runtime takes measures to prevent the container from exceeding the specified resource <code>limit</code>.</p> <p>For instance, if a process within the container attempts to consume more memory than the allowed <code>limit</code>, the system kernel terminates that process, triggering an out of memory <code>(OOM)</code> error.</p> <p>Tip</p> <p>You can use <code>kubectl top</code> command to view memory and CPU usage of a pod. But you need to install <code>metrics server</code> before you can use the <code>kubectl top</code> command.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/requests-and-limits/introduction-to-requests-and-limits/#choosing-cpu-and-memory-requests-and-limits","title":"Choosing CPU and Memory Requests and Limits","text":"<p>To determine the appropriate amount of CPU and Memory <code>requests</code> and <code>limits</code> for a pod, consider the following factors:</p> <ol> <li> <p>Resource Requirements</p> <p>Analyze the application's resource needs, such as CPU usage and memory consumption, based on its workload patterns and expected traffic. This information helps in determining the initial <code>requests</code> and <code>limits</code>.</p> </li> <li> <p>Performance and Scalability</p> <p>Set CPU and memory <code>requests</code> to ensure the pod has enough resources to function properly. Consider the workload's scalability requirements and potential resource spikes to define appropriate <code>limits</code>.</p> </li> <li> <p>Monitoring and Analysis</p> <p>Continuously monitor the pod's resource utilization and performance metrics. Adjust the <code>requests</code> and <code>limits</code> based on observed patterns and bottlenecks to optimize resource allocation.</p> </li> <li> <p>Testing and Validation</p> <p>Conduct testing and performance profiling to validate the chosen CPU and memory settings, ensuring they meet the desired performance and stability requirements.</p> </li> </ol> <p>By considering these factors and regularly assessing the pod's resource utilization, you can determine the correct amount of CPU and Memory <code>requests</code> and <code>limits</code> to optimize performance and resource allocation.</p> <p>References:</p> <ul> <li>Requests and Limits</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/","title":"Create and Manage Resource Quotas","text":"<p>Let's see how we can create and manage Resource Quotas.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/#step-1-create-a-namespace","title":"Step 1: Create a Namespace","text":"<p>Let's create a namespace first:</p> <code>dev-namespace.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n  labels:\n    name: dev\n</code></pre> <p>Apply the manifest to create the <code>dev</code> namespace:</p> <pre><code>kubectl apply -f dev-namespace.yml\n</code></pre> <p>Verify the namespace:</p> <pre><code>kubectl get ns\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/#step-2-create-a-resource-quota","title":"Step 2: Create a Resource Quota","text":"<p>Let's create a Resource Quota for the <code>dev</code> namespace:</p> <code>dev-resource-quota.yml</code> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-resource-quota\n  namespace: dev\nspec:\n  hard:\n    requests.cpu: \"4\" # equivalent to \"4000m\"\n    limits.cpu: \"8\" # equivalent to \"8000m\"\n    requests.memory: 10Gi\n    limits.memory: 20Gi\n    pods: \"2\"\n</code></pre> <p>The following resource types can be specified in the <code>.spec.hard</code> field:</p> Resource Name Description <code>limits.cpu</code> Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value. <code>limits.memory</code> Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value. <code>requests.cpu</code> Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value <code>requests.memory</code> Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value. <code>pods</code> The total number of pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if <code>.status.phase</code> in (<code>Failed</code>, <code>Succeeded</code>) is true. <p>Apply the manifest to create the Resource Quota:</p> <pre><code>kubectl apply -f dev-resource-quota.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/#step-3-verify-the-resource-quota","title":"Step 3: Verify the Resource Quota","text":"<p>List the Resource Quotas:</p> <pre><code>kubectl get quota -n dev\n</code></pre> <p>Describe the quota:</p> <pre><code>kubectl describe quota dev-resource-quota -n dev\n</code></pre> <p>When you view the quota details, you will find the specific hard limit set for each resource, as well as the current usage of those resources.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/#step-4-create-pods-and-test-the-quota","title":"Step 4: Create Pods and Test the Quota","text":"<p>In our example, we can't have more than <code>2</code> pods running in the <code>dev</code> namepsace and aggregate <code>requests.cpu</code> across all pods in the <code>dev</code> namespace can't exceed <code>8</code>. The same is the case for other constraints specified in the resource quota.</p> <p>Let's create a deployment and test this out:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          requests:\n            cpu: \"110m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"510m\"\n            memory: \"600Mi\"\n</code></pre> <p>Warning</p> <p>For every pod in the namespace, each container must have a memory <code>request</code>, memory <code>limit</code>, CPU <code>request</code>, and CPU <code>limit</code> if there is a Resource Quota object in the namespace that puts a <code>limit</code> on the CPU and memory. This is because kubernetes needs to know whether the pod should be allowed to be scheduled based on the resources the pod requests.</p> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/#step-5-verify-deployment-and-pods","title":"Step 5: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments -n dev\n\n# List pods\nkubectl get pods -n dev\n</code></pre> <p>You'll notice that only 2 Ppds are created. This is because the resource quota in the <code>dev</code> namespace restricts the maximum number of pods that can be created to 2.</p> <p>You can verify the same by checking the events in the replicaset object as follows:</p> <pre><code># List replicasets in dev namespace\nkubectl get rs -n dev\n\n# Describe the replicaset to view the events\nkubectl describe rs &lt;replicaset-name&gt; -n dev\n</code></pre> <p>You'll observe an event similar to one below:</p> <pre><code>Error creating: pods \"my-deployment-5c74fcc755-f4hct\" is forbidden: exceeded quota: dev-resource-quota, requested: pods=1, used: pods=2, limited: pods=2\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/create-resource-quota/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- dev-namespace.yml\n\u2502   |-- dev-resource-quota.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/resource-quota/introduction-to-resource-quota/","title":"Introduction to Resource Quota","text":"<p>In a shared cluster with limited nodes, it's important to prevent one team from hogging all the resources. That's where resource quotas come in.</p> <p>Think of a <code>ResourceQuota</code> like a set of rules set by the administrators. These rules determine how much of the cluster's resources each team or user can use.</p> <p>For example, a resource quota can limit the number of objects (like <code>pods</code> or <code>services</code>) that can be created in a specific namespace. It can also put a cap on the total amount of computing power (CPU, memory) that can be used by the resources within that namespace.</p> <p>By setting these limits, administrators ensure that everyone gets their fair share of resources and that the cluster operates smoothly without any team monopolizing all the power.</p> <p>References:</p> <ul> <li>Resource Quotas</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/create-secret/","title":"Create and Manage Secrets","text":"<p>Let's see how we can create and manage Secrets.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/create-secret/#step-1-create-a-secret","title":"Step 1: Create a Secret","text":"<code>my-secret.yml</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:  \n  name: my-secret\ndata:\n  username: cmV5YW5zaA==\n  password: bXlkYnBhc3N3b3Jk\n</code></pre> <p>Apply the manifest to create the Secret:</p> <pre><code>kubectl apply -f my-secret.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/create-secret/#step-2-list-secrets","title":"Step 2: List Secrets","text":"<pre><code>kubectl get secrets\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get secret\nkubectl get secrets\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/create-secret/#step-3-describe-a-secret","title":"Step 3: Describe a Secret","text":"<pre><code># Command template\nkubectl describe secret &lt;secret-name&gt;\n{OR}\nkubectl describe secret/&lt;secret-name&gt;\n\n# Actual command\nkubectl describe secret my-secret\n{OR}\nkubectl describe secret/my-secret\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/create-secret/#step-4-delete-a-secret","title":"Step 4: Delete a Secret","text":"<pre><code># Command template\nkubectl delete secret &lt;secret-name&gt;\n{OR}\nkubectl delete secret/&lt;secret-name&gt;\n\n# Actual command\nkubectl delete secret my-secret\n{OR}\nkubectl delete secret/my-secret\n</code></pre> <p>You can also use the manifest to delete the resource as follows:</p> <pre><code>kubectl delete -f my-secret.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/introduction-to-secret/","title":"Introduction to Kubernetes Secret","text":"<p>A <code>Secret</code> is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a pod specification or in a container image.</p> <p>Using a <code>Secret</code> means that you don't need to include confidential data in your application code.</p> <p>Because <code>Secrets</code> can be created independently of the pods that use them, there is less risk of the <code>Secret</code> (and its data) being exposed during the workflow of creating, viewing, and editing pods.</p> <p>Secrets are similar to ConfigMaps but are specifically intended to hold confidential data. For example database credentials.</p> <p>You can specify the <code>data</code> and/or the <code>stringData</code> field when creating a configuration file for a <code>Secret</code>.</p> <p>The values for all keys in the <code>data</code> field have to be <code>base64-encoded</code> strings. If the conversion to <code>base64</code> string is not desirable, you can choose to specify the <code>stringData</code> field instead, which accepts arbitrary strings as values.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/introduction-to-secret/#base64-encoding-and-decoding","title":"Base64 Encoding and Decoding","text":"<p>Let's see how to <code>base64</code> encode and decode a string using bash:</p> <pre><code># Encode text data\necho \"reyansh\" | base64\n\n# Decode text data\necho \"cmV5YW5zaAo=\" | base64 --decode\n</code></pre> <p>Note</p> <p>A text can be encoded in two different <code>Base64</code> representations, but a single <code>Base64</code> encoding cannot have two distinct decodings.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/introduction-to-secret/#use-cases-of-secret","title":"Use Cases of Secret","text":"<ol> <li> <p>Sensitive Data Storage: <code>Secrets</code> securely store sensitive information like passwords, API keys, and certificates.</p> </li> <li> <p>Database Credentials: <code>Secrets</code> manage and provide access to credentials required for connecting to databases securely.</p> </li> <li> <p>File Mounts: <code>Secrets</code> can be used to mount confidential configuration files as volumes in pods, allowing applications to access sensitive data without including them in the container image.</p> </li> </ol> <p>References:</p> <ul> <li>Kubernetes Secrets</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/","title":"Use Secret as Volume","text":"<p>Let's see how we can use <code>Secret</code> as a Volume and mount it in a container.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/#step-1-create-a-secret","title":"Step 1: Create a Secret","text":"<p>Let's create a <code>Secret</code> that stores a certificate:</p> <code>my-secret.yml</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\ndata:\n  certificate.crt: |\n    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQW5LZ0F3SUJBZ0lKQUxkS2hN\n    WGN3RlFZRFZRUURFeHpKYjNORWNtRnVhU0lzSW1saGRDSTZNVGMyT0RBeU1UQXcKT0RBME1E\n    UXhNekl4TWpFd01USTVNRFl3T1RJeE1Gb1hEVEUzTVRjd09USTVNRFl3T1RJeE1Gb1hEVEUz\n    TVRjd09USTVNRFl3TgpPVEk1TURZd09USTVNRFl3T1RJeE1Gb1hEVEUzTVRjd09USTVNRFl3\n    T1RJeE1Gb1hEVEUzTVRjd09USTVNRFl3T1RJeE1Gb1hEVEUzTVRjd09USTVNRGIwCk1TMHdD\n    d1lEVlFRREV4cEpiaTVqY21sbGR6RU5NQXNHQTFVRUF3d2FVSFZ5WTI5dE1CNFhEVEV4TUM0\n    eEhUTXAKYldsdWJHbHVaR1Z5TG1OdmJRd3ZNQjRYRFRFNU1EQXdNRm9YRFRJNU1EWXhNVEF4\n    TURBM01Gb1hEVEUzTVRjd09USTVNRFl3T1RJeE1GbwpPVEl3TVNJd0lBWURWUVFERXhwSmJp\n    NWpjbmwwY21sdmJqMGlNUXd3RFFZSktvWklodmNOQVFFRkJRQURnZ0VOCkFEQ0NBUW9DZ2dF\n    QkFIS2Q2NmxkMmM0THdEMjFrMzlDVk0wcGhXcXFLM3BmN2FzTVlVcEJyUGtqczczZUdIUGMK\n    R3c4cHFuYWhhUDJibVRhZWR3SkhZcmM3dzNoMzRzRG5ER0tyUnp0ZlBkQmJUSURBUUFCbzJN\n    QkVHQTFVZEV3RUIvdgpDQk1CZ0RBSUI2QXdnRUJBSlZYS2lHQ2RmN0dFQXR3RnBIZThFSlNa\n    a2JNdmdrWnhWQlNUY0dIdW1YRUlBb0dBCkFBTU1rdDZJanNLQ0NnWUlLb1pJemowSUFRN0Jr\n    QkRZ\n</code></pre> <p>Apply the manifest to create Secret:</p> <pre><code>kubectl apply -f my-secret.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/#step-2-verify-secret","title":"Step 2: Verify Secret","text":"<pre><code># List secrets\nkubectl get secrets\n\n# Describe the secret\nkubectl describe secret my-secret\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/#step-3-create-pods-that-uses-secret-as-volume","title":"Step 3: Create Pods That Uses Secret as Volume","text":"<p>Let's create pods that uses <code>Secret</code> as volume and mounts it in a container. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: my-volume\n          mountPath: \"/config\"\n      volumes:\n      - name: my-volume\n        secret:\n          secretName: my-secret\n</code></pre> <p>Observe the following:</p> <ul> <li>The pod uses the Secret <code>my-secret</code> as volume</li> <li>The volume is mounted at <code>/config</code> directory in the <code>nginx</code> container</li> </ul> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/#step-5-verify-volume-mount-and-data","title":"Step 5: Verify Volume Mount and Data","text":"<ol> <li> <p>Open a shell session inside the nginx container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>View data:</p> <pre><code>cd /config\nls\ncat certificate.crt\n</code></pre> </li> </ol> <p>Please note that when a <code>Secret</code> is mounted as a volume in a container, each key in the <code>Secret</code> is stored as a file in the container's file system. This means that the container can read the contents of each file as if they were regular files in the container's file system.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-as-volume/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-secret.yml\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/","title":"Use Secret to Supply Environment Variables","text":"<p>Let's see how we can use <code>Secret</code> to supply environment variables to containers in a pod:</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/#step-1-create-a-secret","title":"Step 1: Create a Secret","text":"<p>Let's create a <code>Secret</code> with data that contains the required environment variables:</p> <code>my-secret.yml</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:  \n  name: my-secret\ndata:\n  username: cmV5YW5zaA==\n  password: bXlkYnBhc3N3b3Jk\n</code></pre> <p>Apply the manifest to create the Secret:</p> <pre><code>kubectl apply -f my-secret.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/#step-2-verify-secret","title":"Step 2: Verify Secret","text":"<pre><code># List secrets\nkubectl get secrets\n\n# Describe the secret\nkubectl describe secret my-secret\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/#step-3-create-pods-that-uses-environment-variables","title":"Step 3: Create Pods That Uses Environment Variables","text":"<p>Let's create pods that uses <code>Secret</code> to set environment variables for the container. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        envFrom:\n        - secretRef:\n            name: my-secret\n</code></pre> <p>Observe that we are using the keyword <code>envFrom</code> to supply a list of environment variables from the Secret <code>my-secret</code>.</p> <p>Apply the manifest to create deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/#step-5-verify-environment-variables","title":"Step 5: Verify Environment Variables","text":"<p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> <p>List environment variables available to the container:</p> <pre><code>env\n</code></pre> <p>You'll see a list of environment variables available to the container. This includes both <code>system-provided</code> as well as <code>user-provided</code> environment variables.</p> <p>Print values of the environment variables we set:</p> <pre><code># Print value of the environment variable username\necho $username\n\n# Print value of the environment variable password\necho $password\n</code></pre> <p>Note</p> <p>Kubernetes automatically does <code>base64</code> decoding for secrets used in the pod.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/secret/use-secret-to-supply-environment-variables/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-secret.yml\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Tip</p> <p>Since <code>Secret</code> is similar to <code>ConfigMap</code> you can repeat all the examples that we discussed in <code>ConfigMap</code> section for <code>Secret</code> as well. Just replace <code>configMapRef</code> by <code>secretRef</code> and <code>configMapKeyRef</code> by <code>secretKeyRef</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/","title":"Access Pods Without Kubernetes Service","text":"<p>Consider a deployment with 2 replicas of a pod. Now, let's assume there is no Kubernetes Service. How does other pods in the cluster access these pods?</p> <p>They do so through IP addresses of these pods. Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/nginx:v1</li> <li>reyanshkharga/nodeapp:v1</li> </ul> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/#step-1-create-pods-using-deployments","title":"Step 1: Create Pods Using Deployments","text":"<p>Let's create two deployments as follows:</p> <code>frontend-deployment.yml</code> <code>backend-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: nginx\n        tier: frontend\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        ports:\n          - containerPort: 80\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n      tier: backend\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n        tier: backend\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Here's what your folder structure should look like:</p> <pre><code>|-- manifests\n\u2502   |-- frontend-deployment.yml\n\u2502   |-- backend-deployment.yml\n</code></pre> <p>Apply the manifest to create the deployments:</p> <pre><code># Create frontend deployment\nkubectl apply -f frontend-deployment.yml\n\n# Create backend deployment\nkubectl apply -f backend-deployment.yml\n</code></pre> <p>Or, you can also apply them together as follows:</p> <pre><code>kubectl apply -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/#step-2-verify-deployments-and-pods","title":"Step 2: Verify Deployments and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre> <p>Also, let's list out the pods with wide option to view the IP addresses of the pods.</p> <pre><code>kubectl get pods -o wide\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/#step-3-access-backend-pod-from-a-frontend-pod","title":"Step 3: Access Backend Pod From a Frontend Pod","text":"<p>Let's try to access one of the backend pods from a frontend pod.</p> <ol> <li> <p>Start a shell session inside the conainer of one of the frontend pods:</p> <pre><code>kubectl exec -it &lt;frontend-pod-name&gt; -- bash\n</code></pre> </li> <li> <p>Access backend pods:</p> <pre><code># Access root endpoint\ncurl &lt;backend-pod-IP&gt;:5000\n\n# Access /health route\ncurl &lt;backend-pod-IP&gt;:5000/health\n\n# Access /random route\ncurl &lt;backend-pod-IP&gt;:5000/random\n</code></pre> </li> </ol> <p>You can verify the same with other backend pod.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/#problems-with-this-approach-of-accessing-pods","title":"Problems With This Approach of Accessing Pods","text":"<ol> <li> <p>What happens if let's say one of the pods go down? The Kubernetes deployment controller brings up another pod. Now the IP address list of these pods change and all the other pods need to keep track of the same.</p> </li> <li> <p>The same is the case when there is auto scaling enabled. The number of the pods increases or decreases based on demand.</p> </li> <li> <p>What if you want to enable load balancing for these pods? How do you do that? Implementing load balancing could be a tedious job.</p> </li> </ol> <p>And that's where kubernetes <code>Service</code> comes into the picture. It solves all of the above problems.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/access-pods-without-service/#clean-up","title":"Clean Up","text":"<p>Delete the deployments:</p> <pre><code># Delete frontend deployment\nkubectl delete -f frontend-deployment.yml\n\n# Delete backend deployment\nkubectl delete -f backend-deployment.yml\n</code></pre> <p>Or, you can also delete them together as follows:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/aws-load-balancer-types/","title":"AWS Load Balancer Types","text":"<p>Amazon Web Services (AWS) offers several types of load balancers that can help distribute incoming traffic across multiple instances or targets, increasing availability and fault tolerance.</p> <p>The main types of load balancers offered by AWS are:</p> <ol> <li> <p>Network Load Balancer (NLB):</p> <p>This load balancer operates at the transport layer (Layer 4) and is optimized for handling high volumes of traffic. It can handle millions of requests per second while maintaining ultra-low latency. </p> <p>It is typically used for TCP and UDP traffic, and supports static IP addresses, which can be used for whitelisting and more complex routing scenarios.</p> </li> <li> <p>Classic Load Balancer (CLB):</p> <p>This load balancer is the original load balancer offered by AWS, and operates at both the application and transport layers (Layer 7 and Layer 4). </p> <p>It provides basic load balancing capabilities and supports HTTP, HTTPS, TCP, and SSL protocols. However, it does not offer the advanced features and flexibility of the ALB or NLB.</p> </li> <li> <p>Application Load Balancer (ALB):</p> <p>This load balancer operates at the application layer (Layer 7) and provides advanced routing capabilities, including content-based routing, support for HTTP/2, WebSocket, and integration with AWS services such as AWS Certificate Manager, AWS WAF, and AWS Elastic Container Service (ECS). </p> <p>It can also route traffic to multiple targets within a single Availability Zone or across multiple Availability Zones.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-externalname-service/","title":"Create ExternalName Service","text":"<p>An <code>ExternalName</code> service is a type of service in kubernetes that allows you to create a service that simply points to an external service by its DNS name, rather than routing traffic to a set of pods.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-externalname-service/#step-1-create-externalname-service","title":"Step 1: Create ExternalName Service","text":"<p>Let's create an ExternalName service as follows:</p> <code>my-externalname-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-externalname-service\nspec:\n  type: ExternalName\n  externalName: google.com\n</code></pre> <p>Please note that we do not have <code>selectors</code> available for <code>ExternalName</code> services as they do not route traffic directly to pods but rather use an external DNS to resolve the service.</p> <p>Let's apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-externalname-service.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-externalname-service/#step-2-verify-the-service","title":"Step 2: Verify the Service","text":"<pre><code># List services\nkubectl get svc\n</code></pre> <p>Note the <code>EXTERNAL-IP</code> field in the output. You'll see the external DNS the service resolves to.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-externalname-service/#step-3-access-the-service","title":"Step 3: Access the Service","text":"<p>Let's see what happens when you access the service.</p> <ol> <li> <p>Create a simple pod that has ping Linux utility:</p> <pre><code>kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c \"sleep 3600\"\n</code></pre> </li> <li> <p>Start a shell session inside busybox container of the pod:</p> <pre><code>kubectl exec -it busybox -- sh\n</code></pre> </li> <li> <p>Ping the service:</p> <pre><code>ping my-externalname-service\n</code></pre> </li> </ol> <p>You'll notice that it resolves to <code>google.com</code> which is the external DNS name configured for the service.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-externalname-service/#clean-up","title":"Clean Up","text":"<pre><code># Delete service\nkubectl delete -f my-externalname-service.yml\n\n# Delete busybox pod\nkubectl delete pod busybox\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/","title":"Create LoadBalancer Service With NLB","text":"<p>A Network Load Balancer (NLB) is a type of load balancer that operates at the Transport layer (Layer 4) of the OSI model, which allows it to handle a large number of requests per second and distribute traffic to targets across multiple Availability Zones.</p> <p>This makes it a good choice for applications that require high throughput and low latency, such as gaming applications, large-scale database deployments, or DNS resolution.</p> <p>To configure a kubernetes service to use a Network Load Balancer, add the annotation <code>service.beta.kubernetes.io/aws-load-balancer-type</code> and set its value to <code>nlb</code>.</p> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#docker-image","title":"Docker Image","text":"<p>Let's see the examples we discussed in action!</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#step-1-create-a-deployment","title":"Step 1. Create a Deployment","text":"<p>First, we need a set of pods that we want to expose using the LoadBalancer Service.</p> <p>So, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <pre><code># Create deployment\nkubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#step-3-create-loadbalancer-service","title":"Step 3: Create LoadBalancer Service","text":"<p>Let's create a LoadBalancer service that creates a network load balancer (NLB) as follows:</p> <code>my-nlb-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Note that you can also specify a <code>nodePort</code> field with desired value.</p> <p>If you don't specify <code>nodePort</code> field, kubernetes will automatically allocate a port within the valid range (<code>30000-32767</code>) for you.</p> <p>Let's apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-nlb-service.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#step-4-verify-the-service","title":"Step 4: Verify the Service","text":"<pre><code># List services\nkubectl get svc\n</code></pre> <p>Notice the <code>PORT(S)</code> field in the output. You'll see the values of service port as well as <code>nodePort</code> mentioned there. (e.g. <code>80:32124</code>)</p> <p>You'll also observe that the <code>EXTERNAL-IP</code> field is set to the value of the DNS name of the network load balancer that was created.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#step-5-verify-the-network-load-balancer-nlb-in-aws-console","title":"Step 5: Verify the Network Load Balancer (NLB) in AWS Console","text":"<p>Visit AWS Console and verify that a network load balancer (NLB) was created.</p> <p>You'll observe the following:</p> <ul> <li>The network load balancer (NLB) spans across the same set of AZs as the EKS cluster</li> <li>A target group is created and worker nodes are added to the target group</li> <li>The load balancer uses the TCP port for health check that kubernetes assigned to the service (<code>nodePort</code>)</li> <li>Listeners are added with ports as defined in service definition.</li> <li>Security group of worker nodes is updated to allow layer 4 traffic. (Notice the ICMP rule.)</li> </ul> <p>Note</p> <p>Network Load Balancers (NLBs) in AWS don't have a security group because they operate at the network layer (Layer 4) of the OSI model.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#step-6-access-the-service-using-network-load-balancer-nlb-dns","title":"Step 6: Access the Service Using Network Load Balancer (NLB) DNS","text":"<p>Since this is a LoadBalancer service we can use the Load Balancer DNS to access the Kubernetes service.</p> <p>First, we need to get the LoadBalancer DNS.</p> <p>The load balancer DNS can be obtained through either of the following methods:</p> <ol> <li>Via the AWS Console</li> <li>Through the kubernetes service</li> </ol> <p>Let's get the load balancer DNS using service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Copy the <code>EXTERNAL-IP</code> value. This is the load balancer DNS.</p> <p>Visit any browser on your local machine and hit <code>&lt;EXTERNAL-IP&gt;:80</code>. You'll get the response form the kubernetes service.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service-with-nlb/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-nlb-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>When you delete a kubernetes service, any AWS resources that were created alongside it, such as the Load Balancer and the security group for the Load Balancer, will also be deleted.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/","title":"Create LoadBalancer Service","text":"<p>A <code>LoadBalancer</code> service in kubernetes is a way to distribute incoming network traffic across multiple instances of your application running in a kubernetes cluster.</p> <p>It acts as a middleman between the client and the servers and helps distribute requests evenly across multiple pods or nodes.</p> <p>Think of it like a traffic cop at a busy intersection. The traffic cop helps distribute the cars evenly across all lanes so that no lane becomes overwhelmed with traffic and causes a traffic jam.</p> <p>Similarly, the load balancer service helps distribute incoming network traffic across all the pods or nodes in the cluster, ensuring that no one pod or node becomes overwhelmed with traffic and causes a bottleneck.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#what-happens-when-you-create-a-loadbalancer-service","title":"What Happens When You Create a LoadBalancer Service?","text":"<p>When you create a <code>LoadBalancer</code> service in kubernetes, the following happens:</p> <ol> <li>The cloud provider (such as AWS or GCP) provisions a new load balancer resource in the cloud environment.</li> <li>The load balancer is configured to forward traffic to the service's endpoints.</li> </ol> <p>Let's see this in action!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#docker-image","title":"Docker Image","text":"<p>Let's see the examples we discussed in action!</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#step-1-create-a-deployment","title":"Step 1. Create a Deployment","text":"<p>First, we need a set of pods that we want to expose using the <code>LoadBalancer</code> Service.</p> <p>So, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <pre><code># Create deployment\nkubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#step-3-create-loadbalancer-service","title":"Step 3: Create LoadBalancer Service","text":"<p>Let's create a LoadBalancer service as follows:</p> <code>my-loadbalancer-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Note that you can also specify a <code>nodePort</code> field with desired value.</p> <p>If you don't specify <code>nodePort</code> field, kubernetes will automatically allocate a port within the valid range (<code>30000-32767</code>) for you.</p> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-loadbalancer-service.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#step-4-verify-the-service","title":"Step 4: Verify the Service","text":"<pre><code># List services\nkubectl get svc\n</code></pre> <p>Notice the <code>PORT(S)</code> field in the output. You'll see the values of service port as well as <code>nodePort</code> mentioned there. (e.g. <code>80:32124</code>)</p> <p>You'll also observe that the <code>EXTERNAL-IP</code> field is set to the value of the DNS name of the classic load balancer that was created.</p> <p>Note</p> <p>By default Kubernetes creates a classic load balancer (CLB) but we can configure it to create a network load balancer or an application load balancer.</p> <p>We'll see that later in this course.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#step-5-verify-the-classic-load-balancer-in-aws-console","title":"Step 5: Verify the Classic Load Balancer in AWS Console","text":"<p>Visit AWS Console and verify that a classic load balancer was created.</p> <p>You'll observe the following:</p> <ul> <li>The classic load balancer spans across the same set of AZs as the EKS cluster</li> <li>The worker nodes (instances) are added to the load balancer</li> <li>The load balancer uses the TCP port for health check that kubernetes assigned to the service (<code>nodePort</code>)</li> <li>Listeners are added with ports as defined in the service definition.</li> <li>Security group of worker nodes is updated to allow traffic from the load balancer</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#step-6-access-the-service-using-load-balancer-dns","title":"Step 6: Access the Service Using Load Balancer DNS","text":"<p>Since this is a <code>LoadBalancer</code> service we can use the Load Balancer DNS to access the kubernetes service.</p> <p>First, we need to get the LoadBalancer DNS.</p> <p>The load balancer DNS can be obtained using either of the following methods:</p> <ol> <li>Via the AWS Console</li> <li>Through the kubernetes service</li> </ol> <p>Let's get the load balancer DNS using kubernetes service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Copy the <code>EXTERNAL-IP</code> value. This is the load balancer DNS.</p> <p>Visit any browser on your local machine and hit <code>&lt;EXTERNAL-IP&gt;:80</code>. You'll get the response form the Kubernetes Service.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-loadbalancer-service/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-loadbalancer-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>When you delete a kubernetes service, any AWS resources that were created alongside it, such as the Load Balancer and the security group for the Load Balancer, will also be deleted.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/","title":"Create NodePort Service","text":"<p>A <code>NodePort</code> service is a type of kubernetes service that exposes a set of pods to the outside network.</p> <p>When you create a <code>NodePort</code> service, kubernetes allocates a <code>static port</code> on each node in the cluster, and then forwards traffic sent to that port to the corresponding pods.</p> <p>This allows external clients to access the service by connecting to any node's IP address and the allocated static port.</p> <p><code>NodePort</code> services are often used to expose a service to the outside world for testing or development purposes, or for services that need to be accessible from outside the cluster.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#docker-image","title":"Docker Image","text":"<p>Let's see the examples we discussed in action!</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#step-1-create-a-deployment","title":"Step 1. Create a Deployment","text":"<p>First, we need a set of pods that we want to expose using the <code>NodePort</code> service.</p> <p>So, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <pre><code># Create deployment\nkubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#step-3-create-nodeport-service","title":"Step 3: Create NodePort Service","text":"<p>Let's create a <code>NodePort</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  type: NodePort\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n      nodePort: 30000\n</code></pre> <p>Note that we have also specified a <code>nodePort</code> of <code>30000</code>, which means that traffic sent to this port on any node in the cluster will be forwarded to the service.</p> <p>The <code>nodePort</code> value can be any valid TCP or UDP port number between <code>30000</code> and <code>32767</code>.</p> <p>Note</p> <p>If you don't specify <code>nodePort</code> field, kubernetes will automatically allocate a port within the valid range (<code>30000-32767</code>) for you.</p> <p>Let's apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-nodeport-service.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#step-4-verify-the-service","title":"Step 4: Verify the Service","text":"<pre><code># List services\nkubectl get svc\n</code></pre> <p>Notice the <code>PORT(S)</code> field. You'll see the values of service <code>port</code> as well as <code>nodePort</code> mentioned there. (e.g. <code>80:30000</code>).</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#step-5-access-the-service-using-nodeport","title":"Step 5: Access the Service Using NodePort","text":"<p>Since this is a <code>NodePort</code> service we can use any woker node to access the service using the <code>nodePort</code> that we specified.</p> <p>First, we need to get IP address of the worker nodes:</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Copy the <code>EXTERNAL-IP</code> value (public IP) of any node you want to connect the service through.</p> <p>Visit any browser on your local machine and hit <code>&lt;EXTERNAL-IP&gt;:30000</code>. You'll get the response form the kubernetes service.</p> <p>Note</p> <p>You must whitelist port <code>30000</code> in the security group of the worker nodes or else you won't be able to connect to it.</p> <p>You can also use the <code>INTERNAL-IP</code> value of any node to connect through the service. But internal IP is accessible only from within the VPC where this node is.</p> <p>Let's test this out!</p> <ol> <li> <p>Create a simple pod:</p> <pre><code>kubectl run nginx --image=nginx\n</code></pre> </li> <li> <p>Start a shell session inside the nginx container:</p> <pre><code>kubectl exec -it nginx -- bash\n</code></pre> </li> <li> <p>Access the service through node port:</p> <pre><code>curl &lt;INTERNAL-IP&gt;:30000\n</code></pre> <p>You'll get the response from the kubernetes service.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-nodeport-service/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code># Delete deployment and services\nkubectl delete -f manifests/\n\n# Delete nginx pod\nkubectl delete pod nginx\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/","title":"Create and Manage Kubernetes Service","text":"<p>From this topic onwards we won't be using imperative commands to create Kubernetes resources unless required.</p> <p>Kubernetes imperative commands are useful for quickly creating and managing simple Kubernetes resources, but they can become less efficient and more error-prone for complex objects like services.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/nginx:v1</li> <li>reyanshkharga/nodeapp:v1</li> </ul> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-1-create-a-deployment","title":"Step 1. Create a Deployment","text":"<p>First, we need a set of pods that we want to expose using a service.</p> <p>So, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nodeapp\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <pre><code># Create deployment\nkubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-3-create-service-manifest","title":"Step 3: Create Service Manifest","text":"<p>First, we need to write the service manifest as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: ClusterIP\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>The <code>port</code> field defines the port number on which the service will listen for incoming traffic.</p> <p>The <code>targetPort</code> field defines the port number on which the pods associated with the service are listening for incoming traffic.</p> <p>Required fields:</p> <ul> <li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object.</li> <li><code>kind</code> - What kind of object you want to create.</li> <li><code>metadata</code> - Data that helps uniquely identify the object, including a name string, UID , and optional namespace.</li> <li><code>spec</code> - What state you desire for the object.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-4-create-service","title":"Step 4: Create Service","text":"<p>Let's use <code>kubectl apply</code> to apply the manifest and create the service:</p> <pre><code># Create service\nkubectl apply -f my-service.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-5-list-services","title":"Step 5: List Services","text":"<pre><code># List all services\nkubectl get services\n\n# List all services with expanded (aka \"wide\") output\nkubectl get services -o wide\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms.</p> <p>The following commands produce the same output:</p> <pre><code>kubectl get svc \nkubectl get service\nkubectl get services\n</code></pre> <p>Note</p> <p><code>service</code> is abbreviated as <code>svc</code>.</p> <p>You'll notice the following fields:</p> <ul> <li><code>TYPE</code>: Service type</li> <li><code>CLUSTER-IP</code>: Virtual IP address assigned to a Service in Kubernetes, which is used for communication within the cluster.</li> <li><code>EXTERNAL-IP</code>: The IP address or Load Balancer DNS name that external clients can use to access the Service from outside of the cluster. (Applicable only for <code>LoadBalancer</code> Service)</li> <li><code>PORT(S)</code>: The ports that the service is listening on</li> </ul> <p>You'll also notice a service with name kubernetes which is a built-in service in kubernetes that is automatically created when you deploy a kubernetes cluster. It is a special service that provides a way for the various components of the kubernetes control plane to communicate with each other.</p> <p>In general, you don't need to interact with the kubernetes service directly, as it is managed by the kubernetes control plane. However, it is an important component of a kubernetes cluster, as it provides a way for the various components of the control plane to work together and manage the cluster.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-6-access-service-locally","title":"Step 6: Access Service Locally","text":"<p>As the service is configured as <code>ClusterIp</code>, it cannot be accessed externally. Hence, we require a proxy to access it.</p> <p>Let's use <code>kubectl port-forward</code> command to access the service.</p> <p>The <code>kubectl port-forward</code> command creates a proxy that allows you to access a resource in a kubernetes cluster from your local machine.</p> <p>The following command forwards traffic from the local port <code>8080</code> to port <code>80</code> on the service. You can replace the local port number with any available port on your local machine.</p> <pre><code>kubectl port-forward svc/my-service 8080:80\n</code></pre> <p>Open any browser on your local machine and access <code>localhost:8080</code>.</p> <p>Important Note</p> <p>When you use the <code>kubectl port-forward</code> command to access a kubernetes service, by default it will forward traffic to a single pod selected by the kubernetes service's load-balancing algorithm. This means that if the service has multiple pods backing it, the <code>kubectl port-forward</code> command will only forward traffic to one of the pods.</p> <p>This behavior is by design to prevent multiple users from interfering with each other's port forwarding sessions.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-7-access-service-from-a-pod-in-the-cluster","title":"Step 7: Access Service From a Pod in the Cluster","text":"<p>Let's see how to access the service from another pod in the cluster.</p> <ol> <li> <p>Create a simple pod:</p> <pre><code>kubectl run nginx --image=reyanshkharga/nginx:v1\n</code></pre> </li> <li> <p>Start a shell session inside nginx container of the pod:</p> <pre><code>kubectl exec -it nginx -- bash\n</code></pre> </li> </ol> <p>Now, there are two ways to access a kubernetes service:</p> <ol> <li> <p>Using the Service's Cluster IP:</p> <p>This method allows other resources within the kubernetes cluster to access the service using its Cluster IP address.</p> <pre><code># Access the service using cluster IP\ncurl &lt;cluster-ip-of-service&gt;:&lt;service-port&gt;\n</code></pre> </li> <li> <p>Using the Service's Fully Qualified Domain Name (FQDN):</p> <p>This method allows resources outside the kubernetes cluster to access the service using its FQDN, which is typically a combination of the service name and the cluster's domain name.</p> <pre><code># Know the cluster domain\nkubectl get cm coredns -n kube-system -o jsonpath='{.data.Corefile}' | grep \"kubernetes\" | awk '{print $2}'\n</code></pre> <p>The domain name <code>cluster.local</code> is commonly used as the default cluster domain name in kubernetes installations.</p> <pre><code># Command template to access the service using FQDN\ncurl &lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;:&lt;service-port&gt;\n\n# Actual command\ncurl my-service.default.svc.cluster.local:80\n</code></pre> </li> </ol> <p>Hit the service a few times and you'll see the traffic being served from the pods the service manages. You'll also notice that the service performs load balancing for the traffic.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#step-8-delete-the-service","title":"Step 8: Delete the Service","text":"<p>Let's delete the service:</p> <pre><code>kubectl delete -f manifests/my-service.yml\n{OR}\nkubectl delete svc/my-service\n{OR}\nkubectl delete svc my-service\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/create-service/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>You can delete all the kubernetes objects we created in one go:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>Service v1 core</li> <li>Service Resources - Service</li> <li>ObjectMeta</li> <li>ServiceSpec</li> <li>Service Concept</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/exposure-level-of-service-types/","title":"Exposure Level of Different Service Types","text":"<p>In Kubernetes, there are different service types that can be used to expose your application to the outside world.</p> <p>These service types differ in their exposure level, which refers to how accessible the service is to external users.</p> <p>Here's a quick overview:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/exposure-level-of-service-types/#1-exposure-level-of-clusterip-service","title":"1. Exposure Level of ClusterIP Service","text":"<ul> <li>This service type is the least exposed of all kubernetes services.</li> <li>It is only accessible within the cluster, which means that external users cannot access it.</li> <li>It is ideal for services that need to communicate with other services within the cluster, but do not need to be accessed by external users.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/exposure-level-of-service-types/#2-exposure-level-of-nodeport-service","title":"2. Exposure Level of NodePort Service","text":"<ul> <li>This service type exposes the service on a specific port on all nodes in the cluster.</li> <li>This means that external users can access the service using the IP address of any node in the cluster, along with the specified port number.</li> <li><code>NodePort</code> services are typically used for development or testing purposes.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/exposure-level-of-service-types/#3-exposure-level-of-loadbalancer-service","title":"3. Exposure Level of LoadBalancer Service","text":"<ul> <li>This service type exposes the service through a load balancer that is provisioned by your cloud provider.</li> <li>The load balancer distributes traffic to the different nodes running your service, making it highly available and scalable.</li> <li><code>LoadBalancer</code> services are ideal for applications that need to handle a large amount of traffic and require high availability.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/exposure-level-of-service-types/#4-exposure-level-of-externalname-service","title":"4. Exposure Level of ExternalName Service","text":"<ul> <li>This service type allows you to map an external DNS name to an internal service in your cluster.</li> <li>This means that external users can access the service using the external DNS name, but the service itself is still only accessible within the cluster.</li> <li><code>ExternalName</code> services are typically used for applications that need to access external resources.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/introduction-to-service/","title":"Introduction to Kubernetes Service","text":"<p>Kubernetes <code>Service</code> is an abstract way to expose an application running on a set of pods as a network service.</p> <p> </p> <p>In plain and simple language, it's a way to make a group of containers accessible to other applications or users outside of the kubernetes cluster.</p> <p>Kubernetes creates DNS records for services. You can contact services with consistent DNS names instead of IP addresses.</p> <p>Kubernetes service does load balancing by default. When you create a kubernetes service, it automatically distributes incoming network traffic among the pods that are part of the service.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/loadbalancer-service-with-alb/","title":"Kubernetes Service With AWS Application Load Balancer (ALB)","text":"<p>It is not possible to create an Application Load Balancer (ALB) in AWS directly from a kubernetes service without using any controller or <code>Ingress</code>.</p> <p>Kubernetes service type <code>LoadBalancer</code> only creates a <code>NodePort</code> service, and it relies on cloud-specific integrations (like controllers) to create and manage the external load balancers in the cloud provider.</p> <p>Without any controller or Ingress, the kubernetes cluster has no knowledge of how to provision and manage the ALB in AWS. So, the kubernetes service type <code>LoadBalancer</code> alone cannot create an ALB in AWS.</p> <p>We'll learn how to use <code>AWS Load Balancer Controller</code> to create kubernetes service with ALB later in this course. Stay tuned!</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/port-vs-targetport-vs-nodeport/","title":"Difference Between port, targetPort, And nodePort in Kubernetes Service","text":"<p>A kubernetes service can have different types of ports fields, including <code>port</code>, <code>targetPort</code>, and <code>nodePort</code>.</p> <p>Here's a brief explanation of each:</p> <ul> <li> <p><code>port</code>: This is the port on which the service listens. When a client sends a request to the service, it connects to this port.</p> </li> <li> <p><code>targetPort</code>: This is the port on which the pods in the service are listening. When the service forwards traffic to the pods, it uses this port.</p> </li> <li> <p><code>nodePort</code>: This is a port that is exposed on each node in the cluster. When traffic comes to this port, it is forwarded to the service's port.</p> </li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/port-vs-targetport-vs-nodeport/#port-vs-targetport-vs-nodeport-an-example","title":"port vs targetPort vs nodePort - An Example","text":"<p>Let's understand the difference between <code>port</code>, <code>targetPort</code>, and <code>nodePort</code> in more detail with an example.</p> <p>Remember that an application can run multiple services on different ports in a container. (Refer to the image below)</p> <p>Suppose we have a Node.js application running inside a container that serves the API requests on port <code>5000</code>. Also, the container has a metrics service running on port <code>6000</code> to expose application metrics.</p> <p> </p> <p>The kubernetes service manifest would look something like this:</p> <code>my-service.yml</code> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: MyApp\n  ports:\n    - name: http\n      nodePort: 30000\n      port: 8080\n      targetPort: 5000\n    - name: metrics\n      nodePort: 31000\n      port: 8081\n      targetPort: 6000\n</code></pre> <p>If you access <code>my-service:8080</code> the traffic is routed to <code>5000</code> of the container. Similarly, if you access <code>my-service:8081</code> then it is redirected to <code>6000</code> of the container.</p> <p>To access the service from outside the kubernetes cluster someone needs to expose the port on worker nodes so that the traffic is redirected to a port of the container. This is node port(port exposed on the worker node).</p> <p>In the example above, you can access the service from outside the cluster using <code>&lt;node-ip-address&gt;:&lt;nodePort&gt;</code>.</p> <p>Suppose IP address of one of the worker nodes is <code>3.109.154.210</code>. You can access the <code>http</code> and <code>metrics endpoint</code> using <code>3.109.154.210:30000</code> and <code>3.109.154.210:31000</code> respectively.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/","title":"Understanding the Role of Selector in Kubernetes Service","text":"<p>The selector is used by the kubernetes service to identify the target pods to forward the incoming network traffic to. By matching the selector labels defined in the service with the corresponding labels defined in the pod, the service can route the traffic to the correct set of pods.</p> <p>Important points to keep in mind:</p> <ul> <li>The service sends traffic to the pods that has all the labels defined in <code>spec.selector</code> field of the service definition.</li> <li>The service has nothing to do with the <code>Deployments</code>. Services send traffic directly to the pods matching the labels defined in <code>spec.selector</code> field of the service definition.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#example-1-selector-with-single-label","title":"Example 1: Selector With Single Label","text":"<p>In the above example you will notice that the service sends traffic only to pods that has the label \"<code>app=nodeapp</code>\".</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#example-2-selector-with-multiple-labels","title":"Example 2: Selector With Multiple Labels","text":"<p>In the above example you will notice that the service sends traffic only to pods that has both the labels \"<code>app=nodeapp</code>\" and \"<code>tier=backend</code>\".</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#docker-image","title":"Docker Image","text":"<p>Let's see the examples we discussed in action!</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#step-1-create-a-service","title":"Step 1: Create a Service","text":"<p>Let's create a service that selects pods with label \"<code>app=nodeapp</code>\" as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: ClusterIP\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Create service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify the service:</p> <pre><code>kubectl get svc -o wide\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#step-2-create-pods","title":"Step 2: Create Pods","text":"<p>Let's create two pods as follows:</p> <code>first-pod.yml</code> <code>second-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: first-pod\n  labels:\n    app: nodeapp\nspec:\n  containers:\n  - name: nodeapp\n    image: reyanshkharga/nodeapp:v1\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 5000\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: second-pod\n  labels:\n    app: nodeapp\n    tier: backend\nspec:\n  containers:\n  - name: nodeapp\n    image: reyanshkharga/nodeapp:v1\n    imagePullPolicy: Always\n    ports:\n    - containerPort: 5000\n</code></pre> <pre><code># Create first-pod\nkubectl apply -f first-pod.yml\n\n# Create second-pod\nkubectl apply -f second-pod.yml\n</code></pre> <p>Note the following:</p> <ul> <li>The first-pod has a label \"<code>app=nodeapp</code>\"</li> <li>The second-pod has labels \"<code>app=nodeapp</code>\" and \"<code>tier=backend</code>\"</li> </ul> <p>List pods showing labels:</p> <pre><code>kubectl get pods --show-labels\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#step-3-access-the-service","title":"Step 3: Access the Service","text":"<p>Let's create a simple pod and try to access the service.</p> <ol> <li> <p>Create a pod:</p> <pre><code># Create nginx pod\nkubectl run nginx --image=nginx\n</code></pre> </li> <li> <p>Start a shell session inside the nginx container in the pod:</p> <pre><code>kubectl exec -it nginx -- bash\n</code></pre> </li> <li> <p>Access the service endpoint:</p> <pre><code>curl my-service:80\n</code></pre> </li> </ol> <p>You'll notice that the traffic is being served from both <code>first-pod</code> and <code>second-pod</code> because both have the label \"<code>app=nodeapp</code>\" the the service use to select pods.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#step-4-update-the-service","title":"Step 4: Update the Service","text":"<p>Let's update the service by adding one more label \"<code>tier=backend</code>\" in the selector.</p> <p>The updated service should look like the following:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: ClusterIP\n  selector:\n    app: nodeapp\n    tier: backend\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest again to update the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#step-5-access-the-service-again","title":"Step 5: Access the Service Again","text":"<ol> <li> <p>Start a shell session inside the nginx container in the pod:</p> <pre><code>kubectl exec -it nginx -- bash\n</code></pre> </li> <li> <p>Access the service endpoint:</p> <pre><code>curl my-service:80\n</code></pre> </li> </ol> <p>This time you'll notice that the traffic is being served only from the <code>second-pod</code> because only <code>second-pod</code> has both \"<code>app=nodeapp</code>\" and \"<code>tier=backend</code>\" labels that the service use to select pods.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/role-of-selector-in-service/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-service.yml\n\u2502   |-- first-pod.yml\n|   |-- second-pod.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code># Delete deployment and service\nkubectl delete -f manifests/\n\n# Delete nginx pod\nkubectl delete pod nginx\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/service-types/","title":"Service Types in Kubernetes","text":"<p>In Kubernetes, there are several different types of services that can be used to make your application accessible to other parts of your cluster or to external users.</p> <p>Here are the <code>Service</code> types available in kubernetes:</p> <ol> <li><code>ClusterIp</code></li> <li><code>NodePort</code></li> <li><code>LoadBalancer</code></li> <li><code>ExternalName</code></li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/service-types/#1-clusterip-service","title":"1. ClusterIp Service","text":"<ul> <li><code>ClusterIP</code> exposes the service on a cluster-internal IP.</li> <li>Choosing this value makes the service only reachable from within the cluster.</li> <li>This is the default value if you don't explicitly specify a type for a service.</li> <li><code>ClusterIP</code> servie can be accessed as <code>&lt;clusterIP&gt;:&lt;service-port&gt;</code></li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/service-types/#2-nodeport-service","title":"2. NodePort Service","text":"<ul> <li><code>NodePort</code> exposes the service on each Node's IP at a static port (the <code>NodePort</code>).</li> <li><code>NodePort</code> service can be accessed as <code>&lt;NodeIp&gt;:&lt;nodePort&gt;</code></li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/service-types/#3-loadbalancer-service","title":"3. LoadBalancer Service","text":"<ul> <li><code>LoadBalancer</code> exposes the service externally using a cloud provider's load balancer.</li> <li>For example AWS Application Load Balancer (ALB)</li> <li><code>LoadBalancer</code> service can be accessed using Load balancer URL</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/service/service-types/#4-externalname-service","title":"4. ExternalName Service","text":"<ul> <li>An <code>ExternalName</code> service is a special case of service that does not have <code>selectors</code>.</li> <li>It does not define any ports or endpoints.</li> <li>It serves as a way to return an alias to an external service residing outside the cluster.</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/introduction-to-storage-in-kubernetes/","title":"Introduction to Storage in Kubernetes","text":"<p>Let's explore the fundamentals of storage in kubernetes and the key concepts and benefits of storage for containerized applications.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/introduction-to-storage-in-kubernetes/#what-is-volume-in-kubernetes","title":"What is Volume in Kubernetes?","text":"<p>Kubernetes offers a flexible storage solution for managing data across containers and nodes.</p> <p>In kubernetes, a volume is an abstraction that represents a storage device that can be mounted to a container in a pod.</p> <p>At its core, a volume is a directory, possibly with some data in it, which is accessible to the containers in a pod.</p> <p>Volumes are an essential component of kubernetes, as they provide a way to store and share data between containers in a pod.</p> <p>Volumes are also an essential part of deploying stateful applications in kubernetes, as they provide a way to persist data beyond the lifecycle of a container.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/introduction-to-storage-in-kubernetes/#ephemeral-vs-persistent-storage","title":"Ephemeral vs Persistent Storage","text":"<p>Kubernetes storage can be classified into two main types:</p> <ol> <li>Ephemeral storage</li> <li>Persistent storage</li> </ol> <p>Ephemeral storage is short-term storage that is tied to the lifecycle of a container. It is used to store data that is only needed for the duration of a container's life. When the container is deleted, the ephemeral storage is also deleted. Ephemeral storage is often used for caching, temporary files, or as scratch space.</p> <p>Persistent storage, on the other hand, is used to store data that needs to persist beyond the lifecycle of a container. This type of storage is used to store data that needs to be shared between multiple containers or to support stateful applications.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/introduction-to-storage-in-kubernetes/#persistent-storage-options-in-kubernetes","title":"Persistent Storage Options in Kubernetes","text":"<p>Kubernetes provides several options for persistent storage, including local storage, network-attached storage, and cloud-based storage solutions, such as Amazon EBS or Google Persistent Disks.</p> <ol> <li> <p>Local Storage</p> <p>Local storage is storage that is directly attached to a node in the cluster. This can be a physical disk or a virtual disk created using a local volume. Local storage is often used for applications that require high-performance storage or for applications that need to store data that is not shared between containers.</p> </li> <li> <p>Network Attached Storage</p> <p>Network attached storage, or NAS, is a type of storage that is accessed over a network. Kubernetes supports several types of <code>NAS</code>, including <code>NFS</code>, <code>GlusterFS</code>, and <code>CephFS</code>. <code>NAS</code> is often used for applications that require shared storage, such as databases or file servers.</p> </li> <li> <p>Cloud Based Storage</p> <p>Cloud based storage solutions, such as <code>Amazon EBS</code> or <code>Google Persistent Disks</code>, provide scalable and highly available storage that can be used to support stateful applications or to provide shared storage for multiple containers running in a kubernetes environment.</p> </li> </ol> <p>In summary, kubernetes provides a flexible and extensible storage framework that allows you to manage persistent data across multiple containers and nodes in a kubernetes cluster. With kubernetes storage, you can choose the right storage solution for your application's needs, whether it is local storage, network-attached storage, or cloud-based storage solutions.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/introduction-to-storage-in-kubernetes/#types-of-volumes","title":"Types of Volumes","text":"<p>Kubernetes supports several types of volumes. The most commonly used volume types are as follows:</p> <ul> <li><code>emptyDir</code></li> <li><code>hostPath</code></li> <li><code>AWS EBS CSI</code></li> <li><code>azureDisk CSI</code></li> <li><code>GCE CSI</code></li> </ul> <p>This is just a small list of most commonly used volume types. Here's a full list of supported volume types.</p> <p>This is a small list of the most commonly used volume types. For a complete list of supported volume types in kubernetes, please refer to the documentation.</p> <p>References:</p> <ul> <li>Types of Volumes in Kubernetes</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/emptydir-volume-demo/","title":"emptyDir Volume Demo","text":"<p>Let's see how we can use <code>emptyDir</code> volume as temporary storage.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/emptydir-volume-demo/#step-1-create-a-deployment-with-emptydir-volume","title":"Step 1: Create a Deployment With emptyDir Volume","text":"<p>Let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /data/my-temp-data.txt; sleep 5; done\"]\n        volumeMounts:\n        - name: my-volume\n          mountPath: /data\n      volumes:\n      - name: my-volume\n        emptyDir: {}\n</code></pre> <p>Observe the following:</p> <ol> <li>We have added an <code>emptyDir</code> volume named <code>my-volume</code> in the pod template</li> <li>The pod has only one container</li> <li>The <code>emptyDir</code> volume is mounted on <code>/data</code> directory of the container</li> <li>The container writes some data in the <code>my-temp-data.txt</code> file every 5 seconds</li> </ol> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/emptydir-volume-demo/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/emptydir-volume-demo/#step-3-verify-volume-mount-and-data","title":"Step 3: Verify Volume Mount and Data","text":"<p>Let's verify if the <code>emptyDir</code> volume was mounted in the container.</p> <ol> <li> <p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>Verify if <code>/data</code> directory is present in the container:</p> <pre><code>ls /data\n</code></pre> </li> <li> <p>View the content of <code>/data/my-temp-data.txt</code> file:</p> <pre><code>tail -f /data/my-temp-data.txt\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/emptydir-volume-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/introduction-to-emptydir-volume/","title":"Introduction to emptyDir Volume","text":"<p>An <code>emptyDir</code> volume is first created when a pod is assigned to a node, and exists as long as that pod is running on that node.</p> <p>As the name says, the <code>emptyDir</code> volume is initially empty.</p> <p>All containers in the pod can read and write the same files in the <code>emptyDir</code> volume, though that volume can be mounted at the same or different paths in each container.</p> <p> </p> <p>When a pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted permanently.</p> <p><code>emptyDir</code> volume falls into the ephemeral storage category.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/introduction-to-emptydir-volume/#use-cases-of-emptydir-volume","title":"Use Cases of emptyDir Volume","text":"<p>Here are a few use cases of <code>emptyDir</code> volume:</p> <ol> <li> <p>Temporary storage</p> <p><code>emptyDir</code> volumes can be used as temporary storage for containers that need to store data temporarily. For example, if you have a container that generates a report, you can store the report in an <code>emptyDir</code> volume until it is ready to be sent to a database or displayed to users.</p> </li> <li> <p>Sharing data between containers in the same pod</p> <p><code>emptyDir</code> volumes can be used to share data between containers running in the same pod. For example, if you have a pod running two containers, one of which generates data and the other processes it, you can use an <code>emptyDir</code> volume to share the data between the two containers.</p> </li> <li> <p>Caching data</p> <p><code>emptyDir</code> volumes can be used to cache data that is frequently accessed by containers. For example, if you have a container that frequently accesses a large dataset, you can store a copy of the dataset in an <code>emptyDir</code> volume to speed up access times.</p> </li> </ol> <p>References:</p> <ul> <li>emptyDir Volume in Kubernetes</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/using-emptydir-volume-to-share-data-between-containers/","title":"Using emptyDir Volume to Share Data Between Containers","text":"<p>Let's see how we can use <code>emptyDir</code> volume to share data between containers.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/using-emptydir-volume-to-share-data-between-containers/#step-1-create-a-deployment-with-emptydir-volume","title":"Step 1: Create a Deployment With emptyDir Volume","text":"<p>Let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: writer\n        image: reyanshkharga/nginx:v1\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /writer-data/my-data.txt; sleep 5; done\"]\n        volumeMounts:\n        - name: my-volume\n          mountPath: /writer-data\n      - name: reader\n        image: busybox\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do tail -f /reader-data/my-data.txt; sleep 5; done\"]\n        volumeMounts:\n        - name: my-volume\n          mountPath: /reader-data\n      volumes:\n      - name: my-volume\n        emptyDir: {}\n</code></pre> <p>Observe the following:</p> <ol> <li>We have added an <code>emptyDir</code> volume in the pod template</li> <li>The pod has two containers named writer and reader</li> <li>The <code>emptyDir</code> volume is mounted on <code>/writer-data</code> directory of the writer container</li> <li>The <code>emptyDir</code> volume is mounted on <code>/reader-data</code> directory of the reader container</li> <li>The writer container writes some data in the <code>my-data.txt</code> file every 5 seconds</li> <li>The reader container reads the same data from <code>my-data.txt</code> file every 5 seconds</li> </ol> <p>You might wonder why the contents of the <code>/writer-data/my-data.txt</code> and <code>/reader-data/my-data.txt</code> files are the same.</p> <p>The reason is that the <code>my-data.txt</code> file is stored in the shared <code>my-volume</code> volume, which is mounted to both containers on different directories. So any changes made to the file in one container are immediately visible in the other container, since they are both accessing the same file in the shared volume.</p> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/using-emptydir-volume-to-share-data-between-containers/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<p>Let's verify if the <code>emptyDir</code> volume was mounted in both the containers.</p> <ol> <li> <p>Start a shell session inside the <code>writer</code> container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -c writer -- bash\n</code></pre> </li> <li> <p>Verify if <code>/writer-data</code> directory is present in the <code>writer</code> container:</p> <pre><code>ls /writer-data\n</code></pre> </li> <li> <p>View the content of <code>/writer-data/my-data.txt</code> file:</p> <pre><code>tail -f /writer-data/my-data.txt\n</code></pre> </li> <li> <p>Exit out of the <code>writer</code> container:</p> <pre><code>exit\n</code></pre> </li> <li> <p>Start a shell session inside the <code>reader</code> container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -c reader -- sh\n</code></pre> </li> <li> <p>Verify if <code>/reader-data</code> directory is present in the <code>reader</code> container:</p> <pre><code>ls /reader-data\n</code></pre> </li> <li> <p>View the content of <code>/reader-data/my-data.txt</code> file:</p> <pre><code>tail -f /reader-data/my-data.txt\n</code></pre> </li> <li> <p>Exit out of the <code>reader</code> container:</p> <pre><code>exit\n</code></pre> </li> <li> <p>View <code>reader</code> container logs:</p> <pre><code>kubectl logs -f &lt;pod-name&gt; -c reader\n</code></pre> </li> </ol> <p>Observations:</p> <ol> <li>The content of <code>my-data.txt</code> is shared between <code>writer</code> and <code>reader</code> containers.</li> <li>Any changes made to the <code>my-data.txt</code> file in the <code>writer</code> container are immediately visible in the <code>reader</code> container.</li> <li>The <code>reader</code> container is able to read the shared data. (Verified by checking reader container logs)</li> </ol> <p>Note</p> <p>The <code>busybox</code> image doesn't have <code>bash</code> so we have used <code>sh</code> instead to start a shell session inside <code>reader</code> container that uses the <code>busybox</code> image.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/emptydir-volume/using-emptydir-volume-to-share-data-between-containers/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/","title":"hostPath Volume Demo","text":"<p>Let's see how we can use <code>emptyDir</code> volume as temporary storage.</p> <p>Here is the Docker Image used in this tutorial: reyanshkharga/nginx</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#step-1-create-a-deployment-with-hostpath-volume","title":"Step 1: Create a Deployment With hostPath Volume","text":"<p>Let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /my-data/my-persistent-data.txt; sleep 5; done\"]\n        volumeMounts:\n        - name: my-volume\n          mountPath: /my-data\n      volumes:\n      - name: my-volume\n        hostPath:\n          path: /data\n          type: DirectoryOrCreate\n</code></pre> <p>Observe the following:</p> <ol> <li>We have added an <code>hostPath</code> volume in the pod template</li> <li>Directory location for <code>hostPath</code> volume on host is <code>/data</code></li> <li>The pod has only one container</li> <li>The <code>hostPath</code> volume is mounted on <code>/my-data directory</code> of the container</li> <li>The container writes some data in the <code>my-persistent-data.txt</code> file every 5 seconds</li> </ol> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#step-3-verify-volume-mount-and-data","title":"Step 3: Verify Volume Mount and Data","text":"<p>Let's verify if the <code>hostPath</code> volume was mounted in the container.</p> <ol> <li> <p>Start a shell session inside the container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>Verify if <code>/my-data</code> directory is present in the container:</p> <pre><code>ls /my-data\n</code></pre> </li> <li> <p>View the content of <code>/my-data/my-persistent-data.txt</code> file:</p> <pre><code>tail -f /my-data/my-persistent-data.txt\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#step-4-verify-hostpath-volume-on-worker-node","title":"Step 4: Verify hostPath Volume on Worker Node","text":"<p>Let's verify if the <code>/data</code> directory was created on the host machine where the pod is running.</p> <ol> <li> <p>Find node where the pod is running:</p> <pre><code>kubectl get pods -o wide\n</code></pre> </li> <li> <p>Access the worker node (EC2 Instance) by logging in through SSH or session manager.</p> </li> <li> <p>Verify if <code>/data</code> directory is present and has <code>my-persistent-data.txt</code> file in it:</p> <pre><code>ls /data\n</code></pre> </li> <li> <p>View the content of <code>/data/my-persistent-data.txt</code> file:</p> <pre><code>tail -f /data/my-persistent-data.txt\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#step-5-delete-the-deployment","title":"Step 5: Delete the Deployment","text":"<p>Let's delete the deployment and see what happens to the <code>hostPath</code> volume.</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#step-6-verify-hostpath-volume-on-worker-node-again","title":"Step 6: Verify hostPath Volume on Worker Node Again","text":"<p>Access the node by logging in through SSH or session manager and verify if the data persists.</p> <p>Observation:</p> <ol> <li>The directory and files created by pods persists</li> <li>Any data generated by pods persists.</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/hostpath-volume-demo/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/introduction-to-hostpath-volume/","title":"Introduction to hostPath Volume","text":"<p>A <code>hostPath</code> volume is a type of volume that allows a pod to mount a directory or file from the host node's filesystem into the pod.</p> <p>This means that the contents of the host directory are exposed to the pod as if they were part of the pod's own filesystem.</p> <p> </p> <p>Any file or directory created by <code>hostPath</code> persists even when the pod is terminated.</p> <p>However, the <code>hostPath</code> volume will be deleted when the node itself is deleted, which means that any data stored within the <code>hostPath</code> volume will also be deleted.</p> <p><code>hostPath</code> volume falls into persistent storage category.</p> <p>Warning</p> <p><code>hostPath</code> volumes pose security risks; it's best practice to minimize their use. They grant container access to the host file system, potentially leading to unauthorized access and data breaches. Opt for more secure storage solutions when possible.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/hostpath-volume/introduction-to-hostpath-volume/#use-cases-of-hostpath-volume","title":"Use Cases of hostPath Volume","text":"<p>Here are a few use cases of <code>hostPath</code> volume:</p> <ol> <li> <p>Development and testing</p> <p><code>hostPath</code> volumes can be used during development and testing to mount source code, configuration files, or other development assets that are stored on the host node's filesystem into the pod. This can help speed up the development process and reduce the time required for containerization.</p> </li> <li> <p>Data persistence</p> <p><code>hostPath</code> volumes can be used to store data that needs to persist across pod restarts, such as databases or log files. This can be particularly useful in stateful applications where data needs to be stored and accessed across multiple containers or pods.</p> </li> <li> <p>System-level access</p> <p><code>hostPath</code> volumes can be used to provide system-level access to the pod, allowing it to access and modify files on the host node's filesystem.</p> </li> </ol> <p>References:</p> <ul> <li>hostPath Volume in Kubernetes</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/","title":"Dynamic Provisioning of Persistent Volume Using EBS","text":"<p>Let's create a <code>StorageClass</code> that specifies the properties of the storage that will be dynamically provisioned for a Persistent Volume.</p> <code>my-sc.yml</code> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: my-sc\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: Immediate\nparameters:\n  type: gp3\n  tagSpecification_1: \"Name=my-ebs-volume\"\n  tagSpecification_2: \"CreatedBy=aws-ebs-csi-driver\"\nreclaimPolicy: Delete\n</code></pre> <p>Observe the following:</p> <ul> <li>The <code>EBS CSI Driver</code> is used as the provisioner to provision EBS volume dynamically</li> <li>The <code>parameters</code> field specifies the properties of the storage provisioned</li> <li>We have set the <code>reclaimPolicy</code> to <code>Delete</code>. This means that the EBS volume will be deleted when the associated Persistent Volume is deleted.</li> <li>The <code>tagSpecification_&lt;i&gt;</code> is used to specify the tags of EBS volume that will be provisioned</li> </ul> <p>For the <code>ebs.csi.aws.com</code> (EBS CSI Driver) storage provisioner, the following are some of the additional fields that can be added to the parameters field:</p> <ol> <li><code>fsType</code>: The file system type to use for the volume. For example, <code>ext4</code> or <code>xfs</code>.</li> <li><code>encrypted</code>: A boolean value that indicates whether to create an encrypted volume. Default is <code>false</code>.</li> <li><code>kmsKeyId</code>: The ID of the KMS key to use for encryption. This field is only used if encrypted is set to <code>true</code>.</li> <li><code>iopsPerGB</code>: The number of IOPS per GiB for the volume. This field is only used for specific volume types, such as <code>io1</code>.</li> <li><code>throughput</code>: The throughput in <code>MiB/s</code> for the volume. This field is only used for specific volume types, such as <code>gp3</code>.</li> </ol> <p>You can check the documentation of the specific storage provisioner you're using to see what other parameters can be added to the parameters field.</p> <p>The <code>volumeBindingMode</code> field in a kubernetes <code>StorageClass</code> object specifies how <code>PersistentVolumeClaims</code> (PVCs) should be bound to <code>PersistentVolumes</code> (PVs) when using this <code>StorageClass</code>.</p> <p>The possible values for the <code>volumeBindingMode</code> field are:</p> <ul> <li><code>Immediate</code>: The PVC is bound to a PV as soon as it is created. This is the default value.</li> <li><code>WaitForFirstConsumer</code>: The PV is only bound to a PVC when a pod using the PVC is scheduled to a node. This is useful when resources are scarce and you only want to provision storage when a pod is actually using it.</li> </ul> <p>In summary, if the <code>volumeBindingMode</code> is set to <code>Immediate</code>, the PV will be created immediately and bound to the PVC. If the value is set to <code>WaitForFirstConsumer</code>, the PV will be created only when a pod using the PVC is scheduled to a node.</p> <p>Apply the manifest to create StorageClass:</p> <pre><code>kubectl apply -f my-sc.yml\n</code></pre> <p>List and verify StorageClass:</p> <pre><code>kubectl get sc\n{OR}\nkubectl get storageclass\n{OR}\nkubectl get storageclasses\n</code></pre> <p>Note</p> <p>This won't provision any EBS volume. The <code>StorageClass</code> only specifies the property of the EBS volume.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/#step-2-create-a-persistent-volume-claim-pvc","title":"Step 2: Create a Persistent Volume Claim (PVC)","text":"<p>Now that we have a <code>StorageClass</code> created, we can create a <code>Persistent Volume Claim</code> (PVC) that will provision the <code>Persistent Volume</code> (PV) dynamically.</p> <p>Create PVC as follows:</p> <code>my-pvc.yml</code> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: my-sc\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Observe the following:</p> <ol> <li> <p>The PVC uses the StorageClass <code>my-sc</code> and, therefore, the storage that will be provisioned dynamically will have the properties defined in <code>my-sc</code>.</p> </li> <li> <p>The access mode is set to <code>ReadWriteOnce</code> and, therefore, the PV that will be created will have the access mode set to <code>ReadWriteOnce</code>.</p> </li> <li> <p>The storage that will be provisioned dynamically will have <code>5Gi</code> storage available.</p> </li> </ol> <p>Apply the manifest to create the Persistent Volume Claim:</p> <pre><code>kubectl apply -f my-pvc.yml\n</code></pre> <p>Once the PVC is created, the following actions are automatically taken:</p> <ol> <li>A new EBS volume will be provisioned</li> <li>A PV will be created that references the newly provisioned EBS volume</li> <li>The PVC is bound to the newly created PV</li> </ol> <p>Please note that if the <code>volumeBindingMode</code> field in the StorageClass (<code>my-sc</code> in this case) used by the PVC is set to <code>WaitForFirstConsumer</code>, the EBS volume and PV will not be created until a new pod that uses the PVC is scheduled on a node. But in this case we have set it to <code>Immediate</code> and, therefore, the EBS and PV will be created as soon as the PVC is created.</p> <p>Go to AWS console and verify the newly created EBS volume. Check for tags that was used in <code>StorageClass</code> definition. The EBS volume will also have additional tags indicating that it was dynamically provisioned using the EBS CSI driver.</p> <p> </p> <p>Also, the Amazon EBS volume state will be <code>Available</code>. The status will change to <code>in-use</code> only when the EBS volume is mounted on a worker node.</p> <p>Verify the Persistent Volume Claim (PVC):</p> <pre><code>kubectl get pvc\n</code></pre> <p>Verify the automatically created Persistent Volume (PV):</p> <pre><code># List PVs\nkubectl get pv\n\n# Describe PV\nkubectl describe pv &lt;pv-name&gt;\n\n# View PV in yaml format\nkubectl get pv &lt;pv-name&gt; -o yaml\n</code></pre> <p>Check the <code>VolumeHandle</code> field. You'll find that it references the newly created EBS volume.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/#step-3-create-pods-that-uses-the-persistent-volume-claim","title":"Step 3: Create Pods That Uses the Persistent Volume Claim","text":"<p>Let's create pods that uses the Persistent Volume Claim we created in the previous step. We'll use a deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /my-data/my-persistent-data.txt; sleep 5; done\"]\n        volumeMounts:\n        - name: my-volume\n          mountPath: /my-data\n      volumes:\n      - name: my-volume\n        persistentVolumeClaim:\n          claimName: my-pvc\n</code></pre> <p>Obeserve the following:</p> <ul> <li>The deployment creates pods with 1 replica</li> <li>The pod has one container named <code>nginx</code></li> <li>A volume named <code>my-volume</code> is created using the PVC <code>my-pvc</code> we created in the previous step</li> <li>The volume <code>my-volume</code> is mounted on <code>/my-data</code> directory of the <code>nginx</code> container</li> </ul> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify the EBS volume state in AWS console. You'll observe that as soon as the pod is scheduled on a node, the EBS volume is mounted on that node and the state changes from <code>Available</code> to <code>in-use</code>.</p> <p>You can see the <code>Attached instances</code> field in the EBS details section in the AWS console.</p> <p>Note</p> <p>In the deployment described above, if the number of replicas is set to a value greater than one, only one pod will be created and scheduled due to the limitations of EBS volumes, which cannot be mounted on multiple nodes and, therefore, multiple pods.</p> <p>This limitation can be overcome by using StatefulSets, which allow for dynamic provisioning of storage for each pod in the deployment, ensuring that each pod has its own unique storage that persists even if the pod is terminated or rescheduled.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/#step-4-verify-deployment-and-pods","title":"Step 4: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods -o wide\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/#step-5-verify-volume-mount-and-data","title":"Step 5: Verify Volume Mount and Data","text":"<ol> <li> <p>Open a shell session inside the nginx container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>View data:</p> <pre><code>tail -f /my-data/my-persistent-data.txt\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/#step-6-delete-the-deployment-and-persistent-volume-claim-pvc","title":"Step 6: Delete the Deployment and Persistent Volume Claim (PVC)","text":"<p>You need to delete the deployment before you can delete the PVC because pods uses the claim as volume.</p> <ol> <li> <p>Delete the deployment:</p> <pre><code>kubectl delete -f my-deployment.yml\n</code></pre> </li> <li> <p>Delete the PVC:</p> <pre><code>kubectl delete -f my-pvc.yml\n</code></pre> </li> </ol> <p>As soon as the PVC is deleted the PV will also be deleted. The EBS volume will be retained or deleted based on the <code>reclaimPolicy</code> that was set for the <code>StorageClass</code>. In our case, the EBS volume will be deleted.</p> <p>Verify that the PV is deleted and doesn't exist anymore:</p> <pre><code>kubectl get pv\n</code></pre> <p>Go to AWS console and verify if the EBS volume was deleted.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/dynamic-provisioning-of-pv-using-ebs/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-pv.yml\n\u2502   |-- my-pvc.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>StorageClass Parameters for EBS CSI Driver</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/install-ebs-csi-driver/","title":"Install EBS CSI Driver","text":"<p>Let's install the Amazon <code>EBS CSI Driver</code> in our EKS cluster which will enable us to use Amazon EBS volumes as persistent volumes.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/install-ebs-csi-driver/#step-1-verify-or-create-the-iam-oidc-provider","title":"Step 1: Verify or Create the IAM OIDC provider","text":"<p>The Amazon <code>EBS CSI driver</code> requires the use of the IAM OpenID Connect (OIDC) provider for authentication and authorization.</p> <p>This means that before installing the driver, we need to ensure that the IAM OIDC provider is created for your EKS cluster.</p> <p>We have the IAM OIDC provider already created when we created the cluster using <code>eksctl</code>.</p> <p>Let's verify if we have the IAM OIDC provider created for the cluster.</p> <ol> <li> <p>Retrieve your cluster's OIDC provider ID and store it in a variable:</p> <pre><code># Command template\noidc_id=$(aws eks describe-cluster --name &lt;cluster-name&gt; --region &lt;region-name&gt; --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\n\n# Actual command\noidc_id=$(aws eks describe-cluster --name my-cluster --region ap-south-1 --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\n</code></pre> <p>If the AWS profile being used already has the region configured, there is no requirement for you to provide the <code>--region</code> option.</p> </li> <li> <p>Determine whether an IAM OIDC provider with your cluster's ID is already in your account:</p> <pre><code>aws iam list-open-id-connect-providers | grep $oidc_id\n</code></pre> <p>If output is returned from the above command, then you already have a provider for your cluster and you can skip the next step.</p> </li> <li> <p>Create IAM OIDC provider for your cluster:</p> <p>If no output is returned in the previous command, then you must create an IAM OIDC provider for your cluster. </p> <pre><code># Command template\neksctl utils associate-iam-oidc-provider --cluster &lt;cluster-name&gt; --region &lt;region-name&gt; --approve\n\n# Actual command\neksctl utils associate-iam-oidc-provider --cluster my-cluster --region ap-south-1 --approve\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/install-ebs-csi-driver/#step-2-install-the-amazon-ebs-csi-driver","title":"Step 2: Install the Amazon EBS CSI Driver","text":"<p>Let's install the latest <code>EBS CSI Driver</code> usign the following command:</p> <pre><code>kubectl apply -k \"github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\"\n</code></pre> <p>Note that we are using <code>-k</code> here and not <code>-f</code>. The <code>kubectl apply -k</code> command is used to apply a set of resources defined in a directory. It allows you to define the configuration for a set of resources using a <code>kustomization.yaml</code> file in the directory.</p> <p><code>Kustomize</code> is a tool built into kubernetes that allows you to customize and manage configuration files for kubernetes applications. It simplifies the management of configuration files, allows you to version them, and provides a way to customize them for different environments.</p> <p>Tip</p> <p>You can also install <code>aws-ebs-csi-driver</code> as <code>add-on</code> from the AWS console or using <code>eksctl</code> CLI.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/install-ebs-csi-driver/#step-3-verify-ebs-csi-driver-installation","title":"Step 3: Verify EBS CSI Driver Installation","text":"<p>Let's verify if <code>EBS CSI Driver</code> is installed in our EKS cluster:</p> <pre><code># Get all kubernetes objects in kube-system namespace\nkubectl get all -n kube-system\n</code></pre> <p>You will see the resources created by <code>EBS CSI Driver</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/install-ebs-csi-driver/#what-is-kubernetes-sigs-github-organization-bonus-information","title":"What is kubernetes-sigs GitHub organization (Bonus Information)","text":"<p>The kubernetes-sigs GitHub organization is a collection of Kubernetes Special Interest Groups (SIGs) that focus on specific areas of the kubernetes project. </p> <p>These SIGs are responsible for developing and maintaining specific features, subsystems, and aspects of the Kubernetes ecosystem.</p> <p>Some examples of popular repositories hosted by <code>kubernetes-sigs</code> include:</p> <ul> <li><code>kustomize</code>: A tool for customizing kubernetes configuration files.</li> <li><code>cert-manager</code>: A kubernetes add-on for managing and automating TLS certificates.</li> <li><code>cluster-api</code>: A kubernetes subproject for declaratively managing infrastructure on cloud providers.</li> <li><code>aws-load-balancer-controller</code>: A kubernetes controller that manages Application Load Balancers (ALBs) on AWS.</li> </ul> <p>References:</p> <ul> <li>Install EBS CSI Driver</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/introduction-to-ebs-csi-driver/","title":"Introduction to EBS CSI Driver","text":"<p>Before we start learning about the <code>EBS CSI Driver</code>, let's first get a basic understanding of what the Container Storage Interface (CSI) is. This will make it easier to understand the <code>EBS CSI driver</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/introduction-to-ebs-csi-driver/#what-is-container-storage-interface-csi","title":"What is Container Storage Interface (CSI)?","text":"<p>Container Storage Interface (CSI) is an industry standard interface for external storage systems to be used with container orchestrators like kubernetes.</p> <p>It provides a way for storage vendors to develop and deploy their own plugins that can be integrated with kubernetes to provide storage capabilities to containers.</p> <p>The CSI specification defines a set of interfaces that storage vendors can implement to allow kubernetes to interact with their storage systems. This allows kubernetes to dynamically provision and manage storage resources for containers running in the cluster.</p> <p>With CSI, kubernetes users can now choose from a wide variety of storage options from different vendors, and seamlessly integrate them into their kubernetes clusters.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/introduction-to-ebs-csi-driver/#what-is-ebs-csi-driver","title":"What is EBS CSI Driver?","text":"<p>Amazon Elastic Block Store (EBS) Container Storage Interface (CSI) Driver is a plugin for kubernetes that allows you to use EBS volumes as persistent volumes in kubernetes.</p> <p>The <code>EBS CSI Driver</code> provides a seamless integration between EBS and Kubernetes, allowing you to easily create, attach, and mount EBS volumes to your Kubernetes pods.</p> <p>The <code>EBS CSI Driver</code> supports a variety of EBS volume types, including General Purpose SSD (gp2, gp3), Provisioned IOPS SSD (io1), Throughput Optimized HDD (st1), and Cold HDD (sc1). It also provides support for snapshotting and cloning EBS volumes.</p> <p>Using the <code>EBS CSI Driver</code>, you can manage your EBS volumes and Kubernetes pods independently, while still allowing them to interact seamlessly. This makes it easy to create and manage stateful applications in Kubernetes using EBS as the underlying storage.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/","title":"Static Provisioning of Persistent Volume Using EBS","text":"<p>Let's see how we can use EBS CSI Driver to create a Persistent Volume from an existing Amazon EBS volume.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-1-create-an-amazon-ebs-volume","title":"Step 1: Create an Amazon EBS Volume","text":"<p>Create an Amazon EBS volume:</p> <pre><code>aws ec2 create-volume \\\n    --volume-type gp3 \\\n    --size 5 \\\n    --encrypted \\\n    --availability-zone ap-south-1a \\\n    --tag-specifications 'ResourceType=volume,Tags=[{Key=Name,Value=my-ebs-volume}]'\n</code></pre> <p>Make sure you specify the <code>availability-zone</code> where at least one kubernetes worker node is running. This is because this storage will eventually be mounted on a worker node.</p> <p>If the command executes successfully, you should see an output similar to below:</p> <pre><code>{\n    \"AvailabilityZone\": \"ap-south-1a\",\n    \"CreateTime\": \"2023-04-21T08:22:12+00:00\",\n    \"Encrypted\": true,\n    \"Size\": 5,\n    \"SnapshotId\": \"\",\n    \"State\": \"creating\",\n    \"VolumeId\": \"vol-0acc3dba885b0f700\",\n    \"Iops\": 3000,\n    \"Tags\": [\n        {\n            \"Key\": \"Name\",\n            \"Value\": \"my-ebs-volume\"\n        }\n    ],\n    \"VolumeType\": \"gp3\",\n    \"MultiAttachEnabled\": false,\n    \"Throughput\": 125\n}\n</code></pre> <p>Note down the <code>VolumeId</code> from the output. We'll need it when we create Persistent Volume from this EBS volume.</p> <p>Also, go to AWS console and verify the EBS volume we just created.</p> <p>The volume state should be <code>Available</code> which means that the volume is not currently attached to any instance and is therefore not in use.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-2-create-persistent-volume-pv","title":"Step 2: Create Persistent Volume (PV)","text":"<p>Let's create a Persistent Volume using the Amazon EBS volume we created in the first step:</p> <code>my-pv.yml</code> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 5Gi\n  persistentVolumeReclaimPolicy: Retain\n  csi:\n    driver: ebs.csi.aws.com\n    fsType: ext4\n    volumeHandle: vol-0acc3dba885b0f700\n</code></pre> <p>Replace the value of <code>volumeHandle</code> with the volume id that you recorded in Step 1.</p> <p>Observe the following:</p> <ul> <li>The reclaim policy is set to <code>Retain</code></li> <li>The access mode is set to <code>ReadWriteOnce</code></li> <li>The PV uses the <code>csi</code> provisioner</li> <li>The provisioner uses the <code>EBS CSI driver</code> to create PV using Amazon EBS volume</li> <li>The volume will be formatted and mounted using the <code>ext4</code> file system.</li> </ul> <p>Create persistent volume:</p> <pre><code>kubectl apply -f my-pv.yml\n</code></pre> <p>Verify the status of persistent volume:</p> <pre><code>kubectl get pv\n</code></pre> <p>You'll observe that the status of the Persistent Volume <code>my-pv</code> is <code>Available</code> and it is not bound to any claim.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-3-create-persistent-volume-claim-pvc","title":"Step 3: Create Persistent Volume Claim (PVC)","text":"<p>Let's create a Persistent Volume Claim as follows:</p> <code>my-pvc.yml</code> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: \"\" # Empty string must be explicitly set otherwise default StorageClass will be set\n  volumeName: my-pv # Optional. If not set the PVC will bind to a PV that satisfies the PVC resource requirements\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Observe the following:</p> <ul> <li>The <code>storageClassName</code> is set to <code>\"\"</code> because we are not using dyanic provisioning.</li> <li>The access mode is set to <code>ReadWriteOnce</code>. This means the PVC can only be bound to PVs that have the access mode set to <code>ReadWriteOnce</code>.</li> <li>The PVC requests <code>5Gi</code> storage. This means the PVC can only be bound to PVs that have at least <code>5Gi</code> storage available.</li> <li>We have explicitly specified the name of the PV to which the PVC should be bound. If we omit the <code>volumeName</code> field, the PVC will bind to a PV that satisfies the PVC's resource requirements, such as the amount of storage needed and the required access mode.</li> </ul> <p>Create persistent volume claim:</p> <pre><code>kubectl apply -f my-pvc.yml\n</code></pre> <p>Verify the status of persistent volume claim:</p> <pre><code>kubectl get pvc\n</code></pre> <p>You'll observe that the PVC <code>my-pvc</code> is bound to the PV <code>my-pv</code>.</p> <p>Verify the status of persistent volume again:</p> <pre><code>kubectl get pv\n</code></pre> <p>You'll notice that the status of the PV <code>my-pv</code> changes from <code>Available</code> to <code>Bound</code>.</p> <p>The Amazon EBS volume state will still be <code>Available</code>. The status will change to <code>in-use</code> only when the EBS volume is mounted on a worker node.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-4-create-pods-that-uses-the-persistent-volume-claim","title":"Step 4: Create Pods That Uses the Persistent Volume Claim","text":"<p>Let's create pods that uses the Persistent Volume Claim we created in the previous step. We'll use a deployment to create pods.</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do echo $(date -u) &gt;&gt; /my-data/my-persistent-data.txt; sleep 5; done\"]\n        volumeMounts:\n        - name: my-volume\n          mountPath: /my-data\n      volumes:\n      - name: my-volume\n        persistentVolumeClaim:\n          claimName: my-pvc\n</code></pre> <p>Obeserve the following:</p> <ul> <li>The deployment creates pods with 1 replica</li> <li>The pod has one container named <code>nginx</code></li> <li>A volume named <code>my-volume</code> is created using the PVC <code>my-pvc</code> we created in the previous step</li> <li>The volume <code>my-volume</code> is mounted on <code>/my-data</code> directory of the <code>nginx</code> container</li> </ul> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify the EBS volume state in AWS console. You'll observe that as soon as the pod is scheduled on a node, the EBS volume is mounted on that node and the state changes from <code>Available</code> to <code>in-use</code>.</p> <p>You can see the <code>Attached instances</code> field in the EBS details section in the AWS console.</p> <p>Important Note</p> <p>In the deployment described above, if the number of replicas is set to a value greater than one, only one pod will be created and scheduled due to the limitations of EBS volumes, which cannot be mounted on multiple nodes and, therefore, multiple pods.</p> <p>This limitation can be overcome by using <code>StatefulSets</code>, which allow for dynamic provisioning of storage for each pod in the deployment, ensuring that each pod has its own unique storage that persists even if the pod is terminated or rescheduled.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-5-verify-deployment-and-pods","title":"Step 5: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods -o wide\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-6-verify-volume-mount-and-data","title":"Step 6: Verify Volume Mount and Data","text":"<ol> <li> <p>Open a shell session inside the nginx container:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>View data:</p> <pre><code>tail -f /my-data/my-persistent-data.txt\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-7-delete-the-deployment-and-persistent-volume-claim-pvc","title":"Step 7: Delete the Deployment and Persistent Volume Claim (PVC)","text":"<p>You need to delete the deployment before you can delete the PVC because Pods uses the claim as volume.</p> <ol> <li> <p>Delete the deployment:</p> <pre><code>kubectl delete -f my-deployment.yml\n</code></pre> </li> <li> <p>Delete the PVC:</p> <pre><code>kubectl delete -f my-pvc.yml\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-8-verify-the-status-of-persistent-volume-pv","title":"Step 8: Verify the Status of Persistent Volume (PV)","text":"<p>List PVs:</p> <pre><code>kubectl get pv\n</code></pre> <p>You'll observe that the status of PV is <code>Released</code> because the claim bound to this PV has been deleted.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-9-delete-the-persistent-volume-pv","title":"Step 9: Delete the Persistent Volume (PV)","text":"<p>Delete the PV:</p> <pre><code>kubectl delete -f my-pv.yml\n</code></pre> <p>The EBS volume will not be deleted even if you set the reclaim policy to <code>Delete</code>. This is because the EBS volume was not provisioned dynamically and, therefore, is not managed by any provisioner or driver.</p> <p>Also, verify the EBS volume state in AWS console. You'll observe that as soon as the Persistent Volume is deleted, the EBS volume state changes from <code>in-use</code> to <code>Available</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volume-using-amazon-ebs/static-provisioning-of-pv-using-ebs/#step-10-clean-up","title":"Step 10: Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-pv.yml\n\u2502   |-- my-pvc.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>Also, delete the EBS volume:</p> <pre><code>aws ec2 delete-volume --region &lt;region-name&gt; --volume-id &lt;volume-id&gt;\n</code></pre> <p>Go to AWS console and verify if the EBS volume was deleted.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/","title":"Introduction to Persistent Volumes","text":"<p>A <code>PersistentVolume</code> (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using <code>Storage Classes</code>.</p> <p>A <code>Storage Class</code> in kubernetes is a definition that specifies the properties of the storage that will be dynamically provisioned for a <code>PersistentVolume</code>. For example, it could be an Amazon EBS volume of type <code>gp3</code>.</p> <p><code>PersistentVolume</code> is a resource in the cluster just like a node is a cluster resource. PVs have a lifecycle independent of any individual pod that uses the PV.</p> <p>Kubernetes does not restrict PVs to a namespace, which means that a pod in any namespace can claim a PV for storage.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#what-is-a-persistent-volume-claim","title":"What is a Persistent Volume Claim?","text":"<p>A <code>PersistentVolumeClaim</code> (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume PV resources.</p> <p>By using <code>PersistentVolumeClaim</code> (PVCs) together with <code>StorageClass</code>, users can dynamically provision and use storage resources from various storage providers, such as AWS EBS or GCE PD, in a kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#kitchen-analogy-to-understand-pv-and-pvc","title":"Kitchen Analogy to Understand PV and PVC","text":"<p>Imagine you are a restaurant owner who needs to store ingredients for your dishes. The ingredients are like data in a kubernetes cluster. You can store the ingredients in different types of containers, such as a fridge, a freezer, or a pantry. Similarly, in kubernetes, you can store data in different types of storage, such as local storage, network-attached storage (NAS), or storage area network (SAN).</p> <p>Now, let's say you want to reserve a specific container to store a certain type of ingredient. This container is like a <code>Persistent Volume</code> (PV) in Kubernetes. It's a pre-allocated storage resource that is available for use by your applications. Just like you can reserve a container to store specific ingredients, you can reserve a PV to store specific data.</p> <p>However, you don't want to manage the containers yourself. Instead, you want your kitchen staff to be able to request specific containers when they need them. In Kubernetes, this is done through a <code>Persistent Volume Claim</code> (PVC). A PVC is a request for a specific amount and type of storage that an application needs. It's like your kitchen staff requesting a specific container to store a specific ingredient.</p> <p>In summary, a <code>Persistent Volume</code> (PV) is a pre-allocated storage resource, while a <code>Persistent Volume Claim</code> (PVC) is a request for a specific type and amount of storage. They work together to provide a way for applications to access and manage data storage resources in kubernetes.</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#static-vs-dynamic-provisioning-of-persistent-volume","title":"Static vs Dynamic Provisioning of Persistent Volume","text":"<p>PVs may be provisioned in two different ways:</p> <ol> <li>Statically</li> <li>Dynamically</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#static-provisioning-of-persistent-volume","title":"Static Provisioning of Persistent Volume","text":"<p>Static provisioning of PVs is when you pre-create a PV and it is available for use by applications in the cluster.</p> <p>This method is useful when you know in advance the amount and type of storage resources that your applications will need.</p> <p>To use a statically provisioned PV, an application needs to make a request for that specific PV by name.</p> <p>Static Provisioning Flow:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#dynamic-provisioning-of-persistent-volume","title":"Dynamic Provisioning of Persistent Volume","text":"<p>Dynamic provisioning of PVs, on the other hand, is when PVs are created on-demand as applications make requests for storage resources.</p> <p>With dynamic provisioning, you don't need to pre-create PVs, which can be helpful when you have changing storage requirements or when you don't know in advance how much storage you will need.</p> <p>Dynamic provisioning is made possible by the use of <code>Storage Classes</code>, which define the type and configuration of storage that will be used to create the PV.</p> <p>A <code>Persistent Volume Provisioner</code> in kubernetes is a component that dynamically creates and manages <code>Persistent Volumes</code> (PVs) in response to the creation of <code>Persistent Volume Claims</code> (PVCs) by users.</p> <p>The provisioner acts as a plugin to the kubernetes API server, intercepting PVC creation events and responding by dynamically creating or deleting the requested volumes.</p> <p>This allows users to easily request and use storage resources in a dynamic and scalable way, without having to manually provision or manage storage resources themselves.</p> <p>Some popular examples of kubernetes Persistent Volume Provisioners include AWS EBS, Azure Disk, and NFS.</p> <p>To use dynamically provisioned PVs, an application needs to make a request for storage by creating a <code>Persistent Volume Claim</code> (PVC). The PVC specifies the amount and type of storage required by the application. When a PVC is created, kubernetes checks the available storage resources and provisions a new PV that meets the requirements specified in the PVC.</p> <p>Dynamic Provisioning Flow:</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#kitchen-analogy-to-understand-static-and-dynamic-provisioning-of-storage","title":"Kitchen Analogy to Understand Static and Dynamic Provisioning of Storage","text":"<p>Let's say you have two types of ingredients that you need to store: flour and sugar. You know that you'll need a fixed amount of storage for each of them. For example, you might need a 10-liter container for flour and a 5-liter container for sugar. In kubernetes, this is like a static provisioning of storage.</p> <p>Static provisioning is when you pre-allocate a specific amount of storage in advance, just like you pre-allocate specific containers for specific ingredients. In kubernetes, you can pre-allocate storage by creating <code>Persistent Volumes</code> (PVs) with a specific size and type of storage.</p> <p>Now, Suppose you have introduced a new dish to your menu that requires a new ingredient, but you don't know how much storage this ingredient will require. In the restaurant context, this might be a new spice that you want to use in a limited number of dishes.</p> <p>To store this new ingredient, you could dynamically provision a new container for it as needed. For example, when the kitchen staff needs to prepare a dish that requires this new spice, they could request a new container from the storage room to store the spice temporarily. Once the dish is prepared, the container can be returned to the storage room for later use.</p> <p>In kubernetes, this dynamic provisioning of storage works in a similar way. When an application needs to store data and makes a request through a PVC, kubernetes can automatically provision a new <code>Persistent Volume</code> (PV) to store the data if one isn't already available. This new PV is dynamically created to meet the requirements specified in the PVC.</p> <p>Just like the kitchen staff in the restaurant can request a new container to store the new spice, applications in kubernetes can request dynamic storage provisioning for new data through PVCs. This helps to ensure that the application has the necessary storage resources available to it without having to pre-allocate resources that may go unused.</p> <p>In summary, static provisioning is like pre-allocating specific containers for specific ingredients, while dynamic provisioning is like allocating new containers on demand for new ingredients that you haven't used before.</p> <p>Static provisioning is useful when you know in advance how much storage you need, while dynamic provisioning is useful when you don't know in advance and need to allocate storage on demand.</p> <p> </p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#lifecycle-of-a-persistent-volume-and-claim","title":"Lifecycle of a Persistent Volume and Claim","text":"<p>The interaction between PVs and PVCs follows this lifecycle:</p> <ol> <li> <p>Provisioning</p> <p><code>Persistent Volume</code> (PV) is provisioned. The PV can be provisioned statically or dynamically.</p> </li> <li> <p>Binding</p> <p>A user creates, or in the case of dynamic provisioning, has already created, a <code>PersistentVolumeClaim</code> with a specific amount of storage requested.</p> <p>A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC.</p> <p>Once bound, <code>PersistentVolumeClaim</code> binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping.</p> </li> <li> <p>Using</p> <p>Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a pod.</p> <p>Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule pods and access their claimed PVs by including a <code>persistentVolumeClaim</code> section in a pod's volumes block.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#persistent-volume-access-modes","title":"Persistent Volume Access Modes","text":"<p>The access mode is set during PV creation and tells kubernetes how the volume should be mounted.</p> <p>There are three access modes available:</p> <ul> <li> <p><code>ReadWriteOnce (RWO)</code>: The volume can be mounted as read-write by a single node in the cluster.</p> </li> <li> <p><code>ReadOnlyMany (ROX)</code>: The volume can be mounted as read-only by many nodes in the cluster.</p> </li> <li> <p><code>ReadWriteMany (RWX)</code>: The volume can be mounted as read-write by many nodes in the cluster.</p> </li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#persistent-volume-reclaim-policy","title":"Persistent Volume Reclaim Policy","text":"<p>The reclaim policy for a <code>PersistentVolume</code> tells the cluster what to do with the volume after it has been released of its claim.</p> <p>Currently, volumes can either be <code>Retained</code>, <code>Recycled</code>, or <code>Deleted</code>.</p> <ul> <li><code>Retain</code> - The volume will not be automatically deleted or scrubbed, and it will remain available for use.</li> <li><code>Recycle</code> - Basic scrub (<code>rm -rf /thevolume/*</code>) is performed on the volume and is made available again for a new claim.</li> <li><code>Delete</code> - Associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted</li> </ul> <p>Currently, only <code>NFS</code> and <code>hostPath</code> volumes support recycling.</p> <p>The supported <code>Persistent Volume</code> (PV) reclaim policies for AWS EBS, GCE PD, Azure Disk, and Cinder volumes are limited to <code>Retain</code> and <code>Delete</code>, meaning that the underlying storage asset can either be kept for future use or permanently deleted when the PV is released.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/introduction-to-persistent-volumes/#use-cases-of-persistent-volumes","title":"Use Cases of Persistent Volumes","text":"<ol> <li> <p>Storage for Database applications</p> <p><code>Persistent Volumes</code> provide a reliable and scalable storage solution for stateful database applications running in kubernetes, ensuring data persistence even during node failures or pod restarts.</p> </li> <li> <p>Storage for application logs</p> <p>By storing application logs on <code>Persistent Volumes</code>, kubernetes enables centralized log management, improves troubleshooting and debugging, and ensures that logs persist even after a pod restart or scaling event.</p> </li> <li> <p>Storage for stateful applications</p> <p>Stateful applications, which require persistent storage can benefit from <code>Persistent Volumes</code> to ensure data durability, consistency, and availability throughout the application lifecycle in kubernetes.</p> </li> </ol> <p>References:</p> <ul> <li>Persistent Volumes</li> <li>Storage Classes</li> </ul>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/","title":"Persistent Volume Demo Using Local Storage","text":"<p>Let's see how we can create a <code>Persistent Volume</code> from a local storage using the <code>hostPath</code> provisioner.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-1-verify-directory-on-worker-nodes","title":"Step 1: Verify Directory on Worker Nodes","text":"<p>We'll use <code>/mnt/nginx</code> directory on worker nodes for <code>Persistent Volume</code>.</p> <p>Verify that <code>/mnt/nginx</code> directory doesn't exist on any worker node:</p> <pre><code># Change directory to /mnt\ncd /mnt\n\n# List contents of the directory\nls\n</code></pre> <p>You'll find that there is no directory named <code>nginx</code> in <code>/mnt</code> directory on worker nodes.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-2-create-a-persistent-volume","title":"Step 2: Create a Persistent Volume","text":"<p>Let's create a Persistent Volume from local storage using <code>hostPath</code> as follows:</p> <code>my-pv.yml</code> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 5Gi\n  persistentVolumeReclaimPolicy: Retain\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/nginx\"\n</code></pre> <p>Observe the following:</p> <ul> <li>The reclaim policy is set to <code>Retain</code></li> <li>The access mode is set to <code>ReadWriteOnce</code></li> <li>The PV uses the <code>hostPath</code> provisioner</li> </ul> <p>Create Persistent Volume:</p> <pre><code># Create PV\nkubectl apply -f my-pv.yml\n</code></pre> <p>List Persistent Volumes:</p> <pre><code>kubectl get pv\n{OR}\nkubectl get persistentvolume\n{OR}\nkubectl get persistentvolumes\n</code></pre> <p>Describe a persistent Volume:</p> <pre><code>kubectl describe pv &lt;pv-name&gt;\n{OR}\nkubectl describe persistentvolume &lt;pv-name&gt;\n{OR}\nkubectl describe persistentvolumes &lt;pv-name&gt;\n</code></pre> <p>Note</p> <p>The default <code>Reclaim Policy</code> for Persistent Volume is <code>Retain</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-3-create-a-persistent-volume-claim","title":"Step 3: Create a Persistent Volume Claim","text":"<p>Now, let's create a PVC to request storage needed for the pod we'll create in next step:</p> <code>my-pvc.yml</code> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: \"\" # Empty string must be explicitly set otherwise default StorageClass will be set\n  volumeName: my-pv # Optional. If not set the PVC will bind to a PV that satisfies the PVC resource requirements\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Create Persistent Volume Claim:</p> <pre><code># Create PVC\nkubectl apply -f my-pvc.yml\n</code></pre> <p>The default <code>Storage Class</code> will be used if you don't explicitly set <code>storageClassName</code> to <code>\"\"</code>. We don't want that since we are not using dynamic provisioning.</p> <p>A default storage class is automatically created when you create an Amazon EKS cluster. You'll notice that it is marked <code>default</code>.</p> <p>View the default storage class:</p> <pre><code>kubectl get sc\n{OR}\nkubectl get storageclass\n{OR}\nkubectl get storageclasses\n</code></pre> <p>You'll see the following output:</p> <pre><code>NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  30d\n</code></pre> <p>Also, all PVCs that have <code>storageClassName</code> set to <code>\"\"</code> can be bound only to PVs that have <code>storageClassName</code> also set to <code>\"\"</code>.</p> <p>List Persistent Volume Claims:</p> <pre><code>kubectl get pvc\n{OR}\nkubectl get persistentvolumeclaim\n{OR}\nkubectl get persistentvolumeclaims\n</code></pre> <p>Describe a persistent Volume Claim:</p> <pre><code>kubectl describe pvc &lt;pvc-name&gt;\n{OR}\nkubectl describe persistentvolumeclaim &lt;pv-name&gt;\n{OR}\nkubectl describe persistentvolumeclaims &lt;pv-name&gt;\n</code></pre> <p>Observe the following:</p> <ol> <li>The status of <code>my-pvc</code> is <code>Bound</code></li> <li>The Persistent Volume Claim (PVC) <code>my-pvc</code> is bound to <code>my-pv</code> Persistent Volume (PV)</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-4-create-pods-that-uses-the-persistent-volume-claim","title":"Step 4: Create Pods That Uses the Persistent Volume Claim","text":"<p>Let's create pods that uses the Persistent Volume Claim we created in the previous step. We'll use a deployment to create pods:</p> <p>Note</p> <p>While it is possible for multiple pods to utilize the same PVC, the practical implementation can be more intricate. When multiple pods need to access a <code>Persistent Volume</code> mounted with a <code>ReadWriteOnce</code> access mode, they must be scheduled on the same node to enable simultaneous access to the volume.</p> <p>To keep it simple we'll create deployment with only 1 replica pod as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n          ports:\n            - containerPort: 80\n              name: \"http-server\"\n          volumeMounts:\n            - mountPath: \"/usr/share/nginx/html\"\n              name: my-volume\n      volumes:\n        - name: my-volume\n          persistentVolumeClaim:\n            claimName: my-pvc\n</code></pre> <p>Obeserve the following:</p> <ul> <li>The deployment creates pods with 1 replica</li> <li>Each pod has one container named <code>nginx</code></li> <li>A volume named <code>my-volume</code> is created from the <code>Persistent Volume Claim</code></li> <li>The volume <code>my-volume</code> is mounted on <code>/usr/share/nginx/html</code> directory of the <code>nginx</code> container</li> </ul> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-5-verify-deployment-and-pods","title":"Step 5: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods -o wide\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-6-view-the-page-served-by-nginx-container","title":"Step 6: View the Page Served By Nginx Container","text":"<ol> <li> <p>Open a shell session inside the <code>nginx</code> container of one of the pods:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- bash\n</code></pre> </li> <li> <p>Get the nginx page:</p> <pre><code>curl localhost\n</code></pre> </li> </ol> <p>You'll receive <code>403</code> error page because there is nothing at <code>/usr/share/nginx/html</code>.</p> <p>Usually there is a default <code>index.html</code> file at <code>/usr/share/nginx/html</code> but since we mounted the local storage on worker node to <code>/usr/share/nginx/html</code>, the content of <code>/usr/share/nginx/html</code> in the <code>nginx</code> container is overwritten.</p> <p>In Summary, whatever is present on <code>/mnt/nginx</code> on worker node, the same will be available to <code>nginx</code> container on <code>/usr/share/nginx/html</code>.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-7-add-the-nginx-page","title":"Step 7: Add the Nginx Page","text":"<p>Connect to the worker node where the pod is running using SSH or session manager.</p> <p>You'll see the directory <code>/mnt/nginx</code> is created as soon as the pod comes up.</p> <p>Create a file called <code>index.html</code> with the content below at <code>/mnt/nginx</code>:</p> <code>index.html</code> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;Hello from nginx pod&lt;/h1&gt;\n&lt;p&gt;The pod uses a persistent volume&lt;/p&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Access the nginx page again:</p> <pre><code># Open a shell session inside the same nginx container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Get the nginx page\ncurl localhost\n</code></pre> <p>You'll observe that the nginx serves the html page we created.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-8-delete-the-deployment-and-persistent-volume-claim-pvc","title":"Step 8: Delete the Deployment and Persistent Volume Claim (PVC)","text":"<p>You need to delete the deployment before you can delete the PVC because pods uses the claim as volume.</p> <ol> <li> <p>Delete the deployment:</p> <pre><code>kubectl delete -f my-deployment.yml\n</code></pre> </li> <li> <p>Delete the PVC:</p> <pre><code>kubectl delete -f my-pvc.yml\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-9-verify-the-status-of-persistent-volume-pv","title":"Step 9: Verify the Status of Persistent Volume (PV)","text":"<p>List PVs:</p> <pre><code>kubectl get pv\n</code></pre> <p>You'll observe that the status of PV is <code>Released</code> because the claim bound to this PV has been deleted.</p>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-10-delete-the-persistent-volume-pv","title":"Step 10: Delete the Persistent Volume (PV)","text":"<p>Delete the PV we created:</p> <pre><code>kubectl delete -f my-pv.yml\n</code></pre>"},{"location":"kubernetes-on-eks/kubernetes-fundamentals/storage-in-kubernetes/persistent-volumes/persistent-volume-demo-using-local-storage/#step-11-verify-that-data-is-retained","title":"Step 11: Verify That Data is Retained","text":"<p>Since the reclaim policy is set to the default value <code>Retain</code>, the data on worker node will be retained.</p> <p>You can verify it by logging into the worker node.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/","title":"Introduction to Kubernetes","text":"<p>Before we jump into Kubernetes, it's essential to know what Container Orchestration is and why it's beneficial. Once we've got that down, understanding Kubernetes and why it matters will be a breeze.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#what-is-container-orchestration","title":"What is Container Orchestration?","text":"<p>Container orchestration is the process of managing, deploying, scaling, and monitoring a large number of containers.</p> <p>It involves automating the deployment, scaling, and management of containers to ensure that they run reliably and efficiently.</p> <p>Container orchestration helps manage complex containerized applications that are composed of multiple services, each running in its own container.</p> <p>With container orchestration, these services can be easily managed and scaled up or down as needed.</p> <p>Some popular container orchestration tools are Kubernetes, Docker Swarm, Nomad, and Mesos.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#benefits-of-container-orchestration","title":"Benefits of Container Orchestration","text":""},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#1-scalability","title":"1. Scalability","text":"<p>Container orchestration platforms such as <code>Kubernetes</code> provide powerful tools for scaling applications quickly and efficiently. You can easily spin up additional instances of an application to handle increased traffic or demand, and Kubernetes will automatically distribute the workload across all instances.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#2-high-availability","title":"2. High Availability","text":"<p>With container orchestration, you can ensure that your applications are highly available and always up and running. Kubernetes, for example, provides features such as automatic failover, load balancing, and self-healing to help prevent application downtime and ensure that your services remain available to users.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#3-automation","title":"3. Automation","text":"<p>Container orchestration platforms automate many of the tasks involved in deploying and managing applications, such as scaling, rolling updates, and self-healing. This saves time and reduces the risk of errors, allowing you to focus on developing and improving your application.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#4-resource-optimization","title":"4. Resource Optimization","text":"<p>Container orchestration can help optimize resource utilization by automatically scaling up or down based on demand, and by distributing workloads across multiple nodes or instances. This improves the efficiency of the overall system and can help reduce costs.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#5-security","title":"5. Security","text":"<p>Container orchestration platforms provide built-in security features such as network isolation, access controls, and secrets management. Additionally, with orchestration tools, you can ensure that all containers are up-to-date with the latest security patches, reducing the risk of vulnerabilities and potential breaches.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.</p> <p>Kubernetes was developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF).</p> <p>Kubernetes automates the deployment process by scheduling containers onto nodes, monitoring their health, and restarting or replacing them if they fail.</p> <p>It also provides a wide range of features for scaling and managing applications, such as load balancing, auto-scaling, and rolling updates.</p> <p>Kubernetes uses a declarative approach to manage applications, which means that users specify what they want to run, and Kubernetes takes care of the how.</p> <p>It can be deployed on-premises or in the cloud, and it supports a wide range of container runtimes, including Docker and containerd.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#key-features-of-kubernetes","title":"Key Features of Kubernetes","text":""},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#1-service-discovery","title":"1. Service Discovery","text":"<p>Kubernetes provides service discovery, which allows applications to automatically discover and communicate with other services running within the cluster. This feature eliminates the need for manual configuration of service endpoints and makes it easy to manage communication between different parts of the application.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#2-load-balancing","title":"2. Load Balancing","text":"<p>Kubernetes offers built-in load balancing, which automatically distributes incoming traffic across multiple instances of an application. This feature helps ensure high availability and performance, even under heavy load.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#3-storage-orchestration","title":"3. Storage Orchestration","text":"<p>Kubernetes can orchestrate storage solutions for applications, such as persistent volumes, so that data can be stored and accessed by the appropriate containers. This feature provides a unified approach to storage management and eliminates the need for manual configuration.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#4-automated-rollouts-and-rollbacks","title":"4. Automated Rollouts and Rollbacks","text":"<p>Kubernetes provides automated rollouts and rollbacks, which makes it easy to deploy new versions of applications without downtime or disruption. This ensures that applications are always up-to-date and running smoothly.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#5-automatic-bin-packing","title":"5. Automatic Bin Packing","text":"<p>Kubernetes provides automatic bin packing, which helps to optimize resource utilization by packing containers onto nodes based on available resources and application requirements. This ensures that resources are used efficiently and reduces the need for manual intervention.</p> <p>Bin Packing Problem</p> <p>The bin packing problem is a classic optimization problem in computer science and mathematics. It involves packing a set of items, each with a specific size, into a set of bins, each with a limited capacity, in a way that minimizes the number of bins used.</p> <p>In Kubernetes, you can think of Pods as the items, and the worker nodes as the bins.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#6-self-healing","title":"6. Self Healing","text":"<p>Kubernetes provides self-healing, which automatically detects and replaces failed containers or nodes. This helps to ensure that applications are highly available and reduces the need for manual intervention.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#7-secret-management","title":"7. Secret Management","text":"<p>Kubernetes provides secret management, which makes it easy to securely store and manage sensitive information such as API keys, passwords, and tokens. This helps to ensure that sensitive information is kept safe and secure.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/introduction-to-kubernetes/#8-configuration-management","title":"8. Configuration Management","text":"<p>Kubernetes provides configuration management, which makes it easy to store and manage application configurations across multiple environments. This ensures that applications are consistently configured and reduces the risk of errors or misconfigurations.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/kubernetes-architecture/","title":"Architecture of Kubernetes","text":"<p>Kubernetes has a master-slave architecture. The \"master\" controls and manages the cluster, while the \"slaves\" (also known as nodes) are the worker machines where applications run. The master ensures that applications are deployed, scaled, and maintained as per your specifications, making it a powerful system for container management.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/kubernetes-architecture/#components-of-kubernetes","title":"Components of Kubernetes","text":""},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/kubernetes-architecture/#1-control-plane","title":"1. Control Plane","text":"<p>The control plane is the brain of the Kubernetes system, responsible for managing and orchestrating all of the cluster's resources.</p> <p>It consists of several key components, including the Kubernetes <code>API server</code>, <code>etcd</code>, <code>controller manager</code>, <code>scheduler</code>, and <code>cloud controller manager</code>.</p> <p>Control Plane Components:</p> <ol> <li>API server: API server exposes the Kubernetes API which is used to interact with the cluster.</li> <li>Etcd: The <code>etcd</code> is a distributed key-value store that stores the state and configuration data.</li> <li> <p>Controller manager: The controller manager monitors the state of the cluster and makes required changes.</p> <p>The Controller Manager includes several essential components:</p> <ul> <li><code>Node Controller</code>: Monitors and manages nodes' health and status.</li> <li><code>Replication Controller</code>: Maintains the desired number of pod replicas.</li> <li><code>Endpoint Controller</code>: Populates endpoint resources with pod IPs for services.</li> <li><code>Service Account Controllers</code>: Manage service accounts and associated tokens.</li> <li><code>Service Controller</code>: Manages services and their load-balanced endpoints.</li> <li><code>Job Controller</code>: Manages batch jobs, ensuring they run to completion.</li> <li><code>Namespace Controller</code>: Handles the lifecycle and policies related to namespaces within the cluster.</li> </ul> <p>These controllers work to ensure the cluster's resources align with the desired state specified in kubernetes configurations, aiding in self-healing and maintaining stability.</p> </li> <li> <p>Scheduler: The <code>scheduler</code> schedules the containers to run on the worker nodes.</p> </li> <li>Cloud controller manager: The <code>cloud controller manager</code> manages the underlying cloud infrastructure.</li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/kubernetes-architecture/#2-nodes","title":"2. Nodes","text":"<p>Nodes are the worker machines that run containerized applications in a Kubernetes cluster. Each node runs a container runtime, such as Docker, and is managed by the control plane. </p> <p>Nodes communicate with the control plane via the Kubernetes API and are responsible for running and managing containers. Nodes also run several key components, including the <code>kubelet</code>, <code>kube-proxy</code>, and <code>container runtime</code>.</p> <p>Node Components:</p> <ol> <li> <p>Kubelet: The <code>kubelet</code> is an agent that runs on each node in the cluster and is responsible for managing the state of the pods running on that node. Its main responsibility is to ensure that containers are running and healthy.</p> </li> <li> <p>Kube proxy: The <code>kube-proxy</code> is responsible for managing the network connectivity between the pods and services in the cluster.</p> </li> <li> <p>Container runtime: The <code>container runtime</code> is responsible for starting and stopping containers on the node. Kubernetes supports multiple container runtimes, including Docker, containerd, and CRI-O.</p> </li> </ol>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/kubernetes-architecture/#3-cloud-provider-api","title":"3. Cloud Provider API","text":"<p>The cloud provider API is an optional component of Kubernetes that allows Kubernetes to interact with cloud provider-specific services, such as load balancers, storage, and networking. </p> <p>Cloud provider APIs are implemented by cloud providers and enable Kubernetes to provision and manage cloud resources directly from within the Kubernetes system.</p>"},{"location":"kubernetes-on-eks/kubernetes-overview-and-architecture/kubernetes-architecture/#kubernetes-add-ons","title":"Kubernetes Add-ons","text":"<p>Addons are optional components that can be installed to extend the functionality of a Kubernetes cluster.</p> <p>Addons can provide additional features or services that are not part of the core Kubernetes platform.</p> <p>Some examples of Kubernetes add-ons include <code>Dashboard</code>, <code>DNS</code>, <code>Ingress controller</code>, and <code>Metrics server</code>.</p> <p>Add-ons are typically installed as Kubernetes resources, such as <code>deployments</code> or <code>daemonsets</code>, and are managed by Kubernetes controllers.</p> <p>Add-ons can be installed using various tools, such as <code>kubectl</code> or <code>Helm</code> charts.</p> <p>While add-ons are optional, they may be necessary for certain Kubernetes configurations or workloads. For example, all Kubernetes clusters should have cluster DNS Addon.</p> <p>References:</p> <ul> <li>Kubernetes Components</li> </ul>"},{"location":"kubernetes-on-eks/logging/logging-using-fluent-bit-and-cloudwatch/","title":"Kubernetes Logging Using Fluent Bit and Amazon CloudWatch","text":"<p>To send logs from your containers running in kubernetes to Amazon CloudWatch, you can use <code>Fluent Bit</code> or <code>Fluentd</code>.</p> <p><code>Fluentd</code> is a versatile log collector that gathers logs from different places and sends them to databases or other tools like Kafka, Elasticsearch, CloudWatch, InfluxDB, etc. It's highly adaptable and works well in complex setups.</p> <p><code>Fluent Bit</code> is a lightweight counterpart to <code>Fluentd</code> ideal for places where resources are limited, like in edge computing or kubernetes environments. It efficiently processes and sends logs while using very little memory.</p> <p>In this course we will use Fluent Bit for logging due to the following reasons:</p> <ol> <li><code>Fluent Bit</code> has a smaller resource footprint and is more resource efficient with memory and CPU usage than <code>FluentD</code></li> <li>The <code>Fluent Bit</code> image is developed and maintained by AWS. This gives AWS the ability to adopt new Fluent Bit image features and respond to issues much quicker.</li> </ol> <p>Additionally, Amazon CloudWatch will serve as the output for Fluent Bit. This means that Fluent Bit, running as a DaemonSet, will collect logs from containers on each node and then send them to CloudWatch.</p> <p> </p>"},{"location":"kubernetes-on-eks/logging/logging-using-fluent-bit-and-cloudwatch/#prerequisites","title":"Prerequisites","text":"<p>The IAM role that is attached to the worker nodes must have sufficient permissions which allows Fluent Bit to ship logs to CloudWatch.</p> <p>In our case the worker nodes already have the required permissions. Remember the <code>cloudwatch</code> IAM add-on policies that we used when we created the EKS cluster using <code>eksctl</code>.</p>"},{"location":"kubernetes-on-eks/logging/logging-using-fluent-bit-and-cloudwatch/#set-up-and-deploy-fluent-bit","title":"Set Up and Deploy Fluent Bit","text":"<ol> <li> <p>Create Namespace:</p> <p>First, create a namespace called <code>amazon-cloudwatch</code> as follows:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/cloudwatch-namespace.yaml\n</code></pre> </li> <li> <p>Create ConfigMap for Fluent Bit:</p> <p>Run the following command to create a ConfigMap named <code>cluster-info</code> with the cluster name and the Region to send logs to. Replace <code>cluster-name</code> and <code>cluster-region</code> with your EKS cluster's name and Region.</p> <pre><code># Set variables\nClusterName=&lt;cluster-name&gt;\nRegionName=&lt;cluster-region&gt;\nFluentBitHttpPort='2020'\nFluentBitReadFromHead='Off'\n[[ ${FluentBitReadFromHead} = 'On' ]] &amp;&amp; FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'\n[[ -z ${FluentBitHttpPort} ]] &amp;&amp; FluentBitHttpServer='Off' || FluentBitHttpServer='On'\n\n# Create ConfigMap\nkubectl create configmap fluent-bit-cluster-info \\\n--from-literal=cluster.name=${ClusterName} \\\n--from-literal=http.server=${FluentBitHttpServer} \\\n--from-literal=http.port=${FluentBitHttpPort} \\\n--from-literal=read.head=${FluentBitReadFromHead} \\\n--from-literal=read.tail=${FluentBitReadFromTail} \\\n--from-literal=logs.region=${RegionName} -n amazon-cloudwatch\n</code></pre> <p>Verify if the ConfigMap was created as expected:</p> <pre><code># List configmaps in amazon-cloudwatch namespace \nkubectl get configmap -n amazon-cloudwatch\n\n# View the content of the configmap\nkubectl get configmap fluent-bit-cluster-info -n amazon-cloudwatch -o yaml\n</code></pre> </li> <li> <p>Deploy Fluent Bit DaemonSet to the Cluster:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/fluent-bit/fluent-bit.yaml\n</code></pre> <p>Validate the deployment by entering the following command. Each node should have one pod named <code>fluent-bit-*</code>:</p> <pre><code>kubectl get pods -n amazon-cloudwatch\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/logging/logging-using-fluent-bit-and-cloudwatch/#verify-the-fluent-bit-setup","title":"Verify the Fluent Bit setup","text":"<ol> <li> <p>Open the CloudWatch console.</p> </li> <li> <p>In the navigation pane, choose Log groups.</p> </li> <li> <p>Make sure that you're in the Region where you deployed Fluent Bit.</p> </li> <li> <p>Check the list of log groups in the Region. You should see the following:</p> <ul> <li><code>/aws/containerinsights/Cluster_Name/application</code></li> <li><code>/aws/containerinsights/Cluster_Name/host</code></li> <li><code>/aws/containerinsights/Cluster_Name/dataplane</code></li> </ul> </li> <li> <p>Navigate to one of these log groups and check the Last Event Time for the log streams. If it is recent relative to when you deployed Fluent Bit, the setup is verified.</p> <p>Note</p> <p>There might be a slight delay in creating the <code>/dataplane</code> log group. This is normal as these log groups only get created when Fluent Bit starts sending logs for that log group.</p> </li> </ol>"},{"location":"kubernetes-on-eks/logging/logging-using-fluent-bit-and-cloudwatch/#filter-log-events-from-a-particular-namespace","title":"Filter Log Events From a Particular Namespace","text":"<ol> <li> <p>Navigate to the log group named <code>/aws/containerinsights/Cluster_Name/application</code> in CloudWatch console.</p> </li> <li> <p>In the Filter events search bar add the following filter to view logs from a particular namespace (<code>backend</code> in this case).</p> <pre><code>{ $.kubernetes.namespace_name = \"backend\" }\n</code></pre> </li> </ol> <p>Tip</p> <p>Make sure the microservices from the previous section are up and running in your cluster. These microservices will generate logs that you can subsequently access and view in CloudWatch.</p>"},{"location":"kubernetes-on-eks/logging/logging-using-fluent-bit-and-cloudwatch/#filter-log-events-from-a-multiple-namespaces","title":"Filter Log Events From a Multiple Namespaces","text":"<ol> <li> <p>Navigate to the log group named <code>/aws/containerinsights/Cluster_Name/application</code> in CloudWatch console.</p> </li> <li> <p>In the Filter events search bar add the following filter to view logs from a multiple namespaces (<code>mongodb</code>, <code>backend</code>, and <code>frontend</code> in this case).</p> <pre><code>{ $.kubernetes.namespace_name = \"mongodb\" || $.kubernetes.namespace_name = \"backend\" || $.kubernetes.namespace_name = \"frontend\" }\n</code></pre> </li> </ol> <p>References:</p> <ul> <li>Send logs to CloudWatch Logs</li> <li>Set up Fluent Bit as a DaemonSet to send logs to CloudWatch Logs</li> </ul>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/","title":"Deploy Microservices in Kubernetes","text":"<p>Now that we have a good understanding of kubernetes and related AWS services, let's deploy a few microservices in our EKS kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/nodeapp:mongo</li> <li>reyanshkharga/reactapp:v1</li> <li>mongo:5.0.2</li> </ul> <p>Note</p> <ol> <li> <p><code>reyanshkharga/nodeapp:mongo</code> is a Node.js backend application that uses MongoDB to store and retrieve data.</p> <p>Environment Variables:</p> <ul> <li><code>MONGODB_URI</code> (Required)</li> <li><code>POD_NAME</code> (Optional)</li> </ul> <p>Tha app has the following routes:</p> <ul> <li><code>GET /</code> Returns a JSON object containing <code>Host</code> and <code>Version</code>. If the <code>POD_NAME</code> environment variable is set, the value of the <code>Host</code> will be the value of the variable.</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> <li><code>GET /books</code> Returns the list of books</li> <li><code>POST /addBook</code> Adds a book</li> <li><code>POST /updateBook</code> Updates the <code>copies_sold</code> value for the given book <code>id</code>.</li> <li><code>POST /deleteBook</code> Deletes the book record that matches the given id.</li> </ul> <p>Sample body for <code>POST /addBook</code>:</p> <pre><code>{\n  \"id\": 1,\n  \"title\": \"Book1\",\n  \"copies_sold\": 0\n}\n</code></pre> <p>Sample body for <code>POST /updateBook</code>:</p> <pre><code>{\n  \"id\": 1,\n  \"copies_sold\": 100\n}\n</code></pre> <p>Sample body for <code>POST /deleteBook</code>:</p> <pre><code>{\n  \"id\": 1\n}\n</code></pre> </li> <li> <p><code>reyanshkharga/reactapp:v1</code> is a React application. It's a frontend application that interacts with <code>reyanshkharga/nodeapp:mongo</code> backend to perform the CRUD operation.</p> <p>Environment variables:</p> <ul> <li><code>REACT_APP_API_ENDPOINT</code> (Optional)</li> </ul> <p>The environment variable <code>REACT_APP_API_ENDPOINT</code> is optional. If provided, you will be able to do the CRUD operations.</p> </li> <li> <p><code>mongo:5.0.2</code> is MongoDB database. Our backend will use it to store and retrieve data to perform CRUD operations.</p> </li> </ol>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#objective","title":"Objective","text":"<p>We are going to deploy the following microservices on our EKS kubernetes cluster:</p> <ol> <li><code>MongoDB Database microservice</code>: uses docker image <code>mongo:5.0.2</code></li> <li><code>Node.js Backend microservice</code>: uses docker image <code>reyanshkharga/nodeapp:mongo</code></li> <li><code>React Frontend microservice</code>: uses docker image <code>reyanshkharga/reactapp:v1</code></li> </ol> <p>The following diagram illustrates the communication between microservices:</p> <pre><code>graph LR\n  A(Frontend) ---&gt; B(Backend);\n  B -..-&gt; C[(Database)];</code></pre> <p>Note</p> <p>We will use the same load balancer for both backend and frontend microservices because using more load balancers will be expensive since load balancers are charged hourly. We can achieve this using IngressGroup.</p>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#step-1-deploy-mongodb-database-microservice","title":"Step 1: Deploy MongoDB Database Microservice","text":"<p>Let's create the kubernetes objects for our MongoDB database microservice as follows:</p> <code>00-namespace.yml</code> <code>storageclass.yml</code> <code>pvc.yml</code> <code>deployment-and-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: mongodb\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: mongodb-storageclass\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  tagSpecification_1: \"Name=eks-mongodb-storage\"\n  tagSpecification_2: \"CreatedBy=aws-ebs-csi-driver\"\nreclaimPolicy: Delete\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongodb-pvc\n  namespace: mongodb\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: mongodb-storageclass\n  resources:\n    requests:\n      storage: 4Gi\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-deployment\n  namespace: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongodb\n  template:\n    metadata:\n      labels:\n        app: mongodb\n    spec:\n      containers:\n      - name: mongodb\n        image: mongo:5.0.2\n        ports:\n          - containerPort: 27017\n        volumeMounts:\n        - name: mongodb-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongodb-storage\n        persistentVolumeClaim:\n          claimName: mongodb-pvc\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongodb-service\n  namespace: mongodb\nspec:\n  type: ClusterIP\n  selector:\n    app: mongodb\n  ports:\n    - port: 27017\n      targetPort: 27017\n</code></pre> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- mongodb\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for MongoDB database microservice:</p> <pre><code>kubectl apply -f mongodb/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>mongodb</code></li> <li>A <code>StorageClass</code> (SC) for dynamic provisioning of persistent volume</li> <li>A <code>PersistentVolumeClaim</code> (PVC) in the <code>mongodb</code> namespace</li> <li>MongoDB deployment in the <code>mongodb</code> namespace</li> <li>MongoDB service in the <code>mongodb</code> namespace</li> </ol> <p>Note</p> <p>The order in which yaml files are applied doesn't matter since every relation except namespace is handled by label selectors, so it fixes itself once all resources are deployed.</p> <p>We are using Amazon EBS to persist the MongoDB data. EBS is provisioned dynamically using AWS EBS-CSI driver.</p> <p>With persistent volume even if the MongoDB pod goes down the data will remain intact. When the new pod comes up we'll have the access to the same data.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in mongodb namespace\nkubectl get all -n mongodb\n\n# List StorageClass\nkubectl get sc\n\n# List PersistentVolume\nkubectl get pv\n\n# List PersistenvVolumeClaim\nkubectl get pvc -n mongodb\n</code></pre> <p>Verify if MongoDB is working as expected:</p> <pre><code># Start a shell session inside the mongodb container\nkubectl exec -it &lt;mongodb-pod-name&gt; -n mongodb -- bash\n\n# Start the mongo Shell to interact with MongoDB\nmongo\n\n# List Databases\nshow dbs\n\n# Switch to a Database\nuse &lt;db-name&gt;\n\n# List collections\nshow collections\n</code></pre>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#step-2-deploy-nodejs-backend-microservice","title":"Step 2: Deploy Node.js Backend Microservice","text":"<p>Let's create the kubernetes objects for our Node.js backend microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>ingress.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: backend\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-deployment\n  namespace: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backend\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:mongo\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        env:\n        - name: MONGODB_URI\n          # &lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local\n          value: mongodb://mongodb-service.mongodb.svc.cluster.local:27017\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\n  namespace: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: backend\n  ports:\n    - port: 5000\n      targetPort: 5000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: backend-ingress\n  namespace: backend\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: backend.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-service\n            port:\n              number: 5000\n</code></pre> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- backend\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Node.js backend microservice:</p> <pre><code>kubectl apply -f backend/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>backend</code></li> <li>Backend deployment in the <code>backend</code> namespace</li> <li>Backend service in the <code>backend</code> namespace</li> <li>Ingress for backend service</li> </ol> <p>The ingress creates an internet-facing load balancer and the SSL certificate is attached to the load balancer.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in backend namespace\nkubectl get all -n backend\n\n# List ingress in backend namespace\nkubectl get ing -n backend\n</code></pre> <p>Go to AWS console and verify if the load balancer was created and a record was added to Route 53 for the host specified in ingress.</p> <p>Open any browser on your local host machine and hit the URL to access the backend service:</p> <pre><code>https://backend.example.com\n</code></pre> <p>Note</p> <p>In real world it is best to have authorization and authentication in place for the backend services that is accessible on the internet. But for the sake of simplicity we have not used any authorization or authentication for the backend service.</p>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#step-3-deploy-react-frontend-microservice","title":"Step 3: Deploy React Frontend Microservice","text":"<p>Let's create the kubernetes objects for our React frontend microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>ingress.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: frontend\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-deployment\n  namespace: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: reactapp\n        image: reyanshkharga/reactapp:v1\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n        env:\n        - name: REACT_APP_API_ENDPOINT\n          value: https://backend.example.com\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-service\n  namespace: frontend\nspec:\n  type: ClusterIP\n  selector:\n    app: frontend\n  ports:\n    - port: 3000\n      targetPort: 3000\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  namespace: frontend\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: frontend.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-service\n            port:\n              number: 3000\n</code></pre> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- frontend\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for React frontend microservice:</p> <pre><code>kubectl apply -f frontend/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>frontend</code></li> <li>Frontend deployment in the <code>frontend</code> namespace</li> <li>Frontend service in the <code>frontend</code> namespace</li> <li>Ingress for frontend service</li> </ol> <p>The ingress creates an internet-facing load balancer and the SSL certificate is attached to the load balancer.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Note</p> <p>The frontend microservice uses the API provided by the backend microservice to perform the CRUD operations.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in frontend namespace\nkubectl get all -n frontend\n\n# List ingress in frontend namespace\nkubectl get ing -n frontend\n</code></pre> <p>Go to AWS console and verify if the load balancer was created and a record was added to Route 53 for the host specified in ingress.</p> <p>Open any browser on your local host machine and hit the URL to access the frontend service:</p> <pre><code>https://frontend.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#step-4-perform-crud-operations-and-verify-data-in-mongodb","title":"Step 4: Perform CRUD Operations and Verify Data in MongoDB","text":"<p>Access the frontend service from your browser and perform some CRUD operations as follows:</p> <ol> <li>Get Books</li> <li>Add a Book</li> <li>Update a Book</li> <li>Delete a Book</li> <li>Check Health</li> </ol> <code>Get Books</code> <code>Add Book</code> <code>Update Book</code> <code>Delete Book</code> <code>Check Health</code> <p><p> </p></p> <p><p> </p></p> <p><p> </p></p> <p><p> </p></p> <p><p> </p></p> <p>Verify if the records have been inserted into the MongoDB database:</p> <pre><code># Start a shell session inside the mongodb container\nkubectl exec -it &lt;mongodb-pod-name&gt; -n mongodb -- bash\n\n# Start the mongo Shell to interact with MongoDB\nmongo\n\n# List Databases\nshow dbs\n\n# Switch to mydb database\nuse mydb\n\n# List collections\nshow collections\n\n# List items in the books collection\ndb.books.find()\n</code></pre>"},{"location":"kubernetes-on-eks/microservices/deploy-microservices-in-kubernetes/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- mongodb\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n|   |-- backend\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n|   |-- frontend\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>All AWS resources, such as load balancers, Route 53 records, etc., created by AWS Load Balancer Controller via ingress or service objects, will also be deleted.</p> <p>References:</p> <ul> <li>Kubernetes Resource Creation Order</li> </ul>"},{"location":"kubernetes-on-eks/microservices/introduction-to-microservices/","title":"Introduction to Microservices","text":"<p>Microservices are a contemporary architectural approach in software development. They break down large applications into smaller, independent services, each responsible for specific functions.</p> <p>Unlike traditional monolithic structures, where the entire application is tightly integrated, microservices operate as autonomous units, communicating through APIs.</p> <p>For example, consider an e-commerce platform. Instead of a monolithic structure, it comprises distinct services like orders, payments, delivery, and user management, each functioning as a separate entity, communicating via APIs.</p> <p>Typically, each microservice has its own database, but this can vary depending on your specific use case.</p> <pre><code>graph LR\n  A[[Order]] --&gt; B[[Payment]];\n  B --&gt; C[[Delivery]];\n  B --&gt; D[[User Management]];\n  A --&gt; C;\n  C --&gt; D;\n  A -.-&gt; AD[(Database)];\n  B -.-&gt; BD[(Database)];\n  C -.-&gt; CD[(Database)];\n  D -.-&gt; DD[(Database)];</code></pre>"},{"location":"kubernetes-on-eks/microservices/introduction-to-microservices/#key-characteristics-of-microservices","title":"Key Characteristics of Microservices","text":"<p>The services in a microservice architecture are loosely coupled, allowing teams to develop, deploy, and scale them independently. They enable rapid development cycles, as different teams can work on distinct services simultaneously. Each service can use a different programming language or technology stack, promoting flexibility.</p>"},{"location":"kubernetes-on-eks/microservices/introduction-to-microservices/#benefits-of-microservices","title":"Benefits of Microservices","text":"<p>Scalability is a significant advantage; only the necessary services can be scaled based on demand, optimizing resource utilization. Moreover, microservices enhance fault isolation. If one service fails, it doesn\u2019t bring down the entire system. This architecture improves resilience and enables easier maintenance and updates.</p>"},{"location":"kubernetes-on-eks/microservices/introduction-to-microservices/#challenges-and-considerations","title":"Challenges and Considerations","text":"<p>Implementing microservices requires robust communication between services, often through APIs. Managing multiple services demands careful orchestration and monitoring to ensure seamless interaction and performance optimization.</p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/","title":"Create Grafana Dashboards to Monitor Kubernetes","text":"<p>Now that we have Prometheus and Grafana set up properly. Let's create a few dashboards to visualize kubernetes metrics.</p> <p>We will import community created dashboards.</p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-dashboards-using-script","title":"Import Dashboards Using Script","text":"<p>I have written a bash script that you can use to import all the required dashboards without manual effort.</p> <code>import-kubernetes-grafana-dashboards.sh</code> <pre><code># Address of Grafana\nGRAFANA_HOST=\"https://grafana.example.com\"\n# Login credentials, if authentication is used\nGRAFANA_CRED=\"admin:RP6xkxD\"\n# The name of the Prometheus data source to use\nGRAFANA_DATASOURCE=\"Prometheus\"\n# Import all Kubernetes dashboards\nfor DASHBOARD in 17682 17683 17684 17685 17686; do\n    REVISION=\"$(curl -s https://grafana.com/api/dashboards/${DASHBOARD}/revisions -s | jq \".items[] | .revision\")\"\n    curl -s https://grafana.com/api/dashboards/${DASHBOARD}/revisions/${REVISION}/download &gt; /tmp/dashboard.json\n    echo \"Importing $(cat /tmp/dashboard.json | jq -r '.title') (revision ${REVISION}, id ${DASHBOARD})...\"\n    curl -s -k -u \"$GRAFANA_CRED\" -XPOST \\\n        -H \"Accept: application/json\" \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\\\"dashboard\\\":$(cat /tmp/dashboard.json),\\\"overwrite\\\":true, \\\n            \\\"inputs\\\":[{\\\"name\\\":\\\"DS_PROMETHEUS\\\",\\\"type\\\":\\\"datasource\\\", \\\n            \\\"pluginId\\\":\\\"prometheus\\\",\\\"value\\\":\\\"$GRAFANA_DATASOURCE\\\"}]}\" \\\n        $GRAFANA_HOST/api/dashboards/import\n    echo -e \"\\nDone\\n\"\ndone\n</code></pre> <p>Make sure to replace the values of <code>GRAFANA_HOST</code> and <code>GRAFANA_CRED</code> variables with the values specific to your Grafana set up.</p> <p>Now, run the script to import dashboards to your Grafana:</p> <pre><code># Give execute permission to the script\nchmod +x import-kubernetes-grafana-dashboards.sh\n\n# Execute the script\n./import-kubernetes-grafana-dashboards.sh\n</code></pre> <p>Go to grafana console to verify if the dashboards were imported successfully. Also, it would be better to move these dashboards to a seperate folder called Kubernetes.</p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-dashboards-manually","title":"Import Dashboards Manually","text":"<p>Follow the below procedures if you would like to import individual dashboards manually.</p> <p>First, login to Grafana using credentials supplied during configuration. Use the username <code>admin</code> and get the <code>password</code> by running the following command:</p> <pre><code>kubectl get secret --namespace grafana grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-kubernetes-cluster-overview-dashboard","title":"Import Kubernetes Cluster Overview Dashboard","text":"<p>Follow the instructions below to create a grafana dashboard to monitor the kubernetes cluster:</p> <ol> <li>Click on Dashboards on left panel and click <code>+ Import</code></li> <li>Enter 17686 to import the dashboard</li> <li>Click Load</li> <li>Enter <code>Kubernetes - Cluster Overview</code> as the Dashboard name</li> <li>Enter <code>Kubernetes</code> as the folder name. It will create the folder if it doesn't exist.</li> <li>Select <code>Prometheus</code> as the endpoint under prometheus data sources drop down</li> <li>Click <code>Import</code></li> <li>Save the dashboard by clicking the <code>Save dashboard</code> icon on the top right corner</li> </ol> <p>This will dispaly an overview of the kubernetes cluster. The dashboard should look something like this:</p> <p> </p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-kubernetes-deployment-overview-dashboard","title":"Import Kubernetes Deployment Overview Dashboard","text":"<p>Follow the instructions below to create a grafana dashboard to monitor the kubernetes cluster:</p> <ol> <li>Click on Dashboards on left panel and click <code>+ Import</code></li> <li>Enter 17685 to import the dashboard</li> <li>Click Load</li> <li>Enter <code>Kubernetes - Deployment Overview</code> as the Dashboard name</li> <li>Enter <code>Kubernetes</code> as the folder name. It will create the folder if it doesn't exist.</li> <li>Select <code>Prometheus</code> as the endpoint under prometheus data sources drop down</li> <li>Click <code>Import</code></li> <li>Save the dashboard by clicking the <code>Save dashboard</code> icon on the top right corner</li> </ol> <p>This will dispaly an overview of the kubernetes deployments in the cluster. The dashboard should look something like this:</p> <p> </p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-kubernetes-pod-overview-dashboard","title":"Import Kubernetes Pod Overview Dashboard","text":"<p>Follow the instructions below to create a grafana dashboard to monitor the kubernetes pods:</p> <ol> <li>Click on Dashboards on left panel and click <code>+ Import</code></li> <li>Enter 17684 to import the dashboard</li> <li>Click Load</li> <li>Enter <code>Kubernetes - Pod Overview</code> as the Dashboard name</li> <li>Enter <code>Kubernetes</code> as the folder name. It will create the folder if it doesn't exist.</li> <li>Select <code>Prometheus</code> as the endpoint under prometheus data sources drop down</li> <li>Click <code>Import</code></li> <li>Save the dashboard by clicking the <code>Save dashboard</code> icon on the top right corner</li> </ol> <p>This will dispaly an overview of the kubernetes pods in the cluster. The dashboard should look something like this:</p> <p> </p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-kubernetes-persistentvolume-overview-dashboard","title":"Import Kubernetes PersistentVolume Overview Dashboard","text":"<p>Follow the instructions below to create a grafana dashboard to monitor the kubernetes persistent volumes:</p> <ol> <li>Click on Dashboards on left panel and click <code>+ Import</code></li> <li>Enter 17682 to import the dashboard</li> <li>Click Load</li> <li>Enter <code>Kubernetes - PersistentVolume Overview</code> as the Dashboard name</li> <li>Enter <code>Kubernetes</code> as the folder name. It will create the folder if it doesn't exist.</li> <li>Select <code>Prometheus</code> as the endpoint under prometheus data sources drop down</li> <li>Click <code>Import</code></li> <li>Save the dashboard by clicking the <code>Save dashboard</code> icon on the top right corner</li> </ol> <p>This will dispaly an overview of the kubernetes persistent volumes in the cluster. The dashboard should look something like this:</p> <p> </p>"},{"location":"kubernetes-on-eks/monitoring/create-grafana-dashboards/#import-kubernetes-node-overview-dashboard","title":"Import Kubernetes Node Overview Dashboard","text":"<p>Follow the instructions below to create a grafana dashboard to monitor the kubernetes nodes:</p> <ol> <li>Click on Dashboards on left panel and click <code>+ Import</code></li> <li>Enter 17683 to import the dashboard</li> <li>Click Load</li> <li>Enter <code>Kubernetes - Node Overview</code> as the Dashboard name</li> <li>Enter <code>Kubernetes</code> as the folder name. It will create the folder if it doesn't exist.</li> <li>Select <code>Prometheus</code> as the endpoint under prometheus data sources drop down</li> <li>Click <code>Import</code></li> <li>Save the dashboard by clicking the <code>Save dashboard</code> icon on the top right corner</li> </ol> <p>This will dispaly an overview of the kubernetes nodes in the cluster. The dashboard should look something like this:</p> <p> </p> <p>References:</p> <ul> <li>Grafana Dashboards for Kubernetes - By Reyansh Kharga</li> <li>Kubernetes Persistent Volume Dashboard Credit</li> <li>Kubernetes Nodes Overview Dashboard Credit</li> </ul>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/","title":"Monitoring Kubernetes Using Prometheus and Grafana","text":"<p>We can use Prometheus and Grafana to monitor kubernetes resources, workloads and other metrics.</p> <p>Prometheus and Grafana form a powerful duo for monitoring Kubernetes. They aid in tracking resource utilization, workload performance, and diverse metrics critical for maintaining cluster health.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#what-is-prometheus","title":"What is Prometheus?","text":"<p>Prometheus is a monitoring solution for storing time series data like metrics. It operates by periodically scraping metrics from configured targets using a pull-based model.</p> <p>The core components of Prometheus include the <code>Prometheus Server</code>, which stores time-series data, a multidimensional data model, <code>service discovery</code> for dynamic target identification, and an <code>Alert Manager</code> for alerting based on predefined rules.</p> <p>Using <code>PromQL</code>, users query and analyze metrics, while exporters facilitate the exposure of specific metrics from various systems.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#what-is-grafana","title":"What is Grafana?","text":"<p>Grafana is a visualization and monitoring tool, offering a user-friendly interface to create dashboards and analyze data from various sources, including Prometheus.</p> <p>Its key components include versatile dashboard creation, data source integration (like Prometheus), and a rich library of plugins for extended functionality.</p> <p>Grafana simplifies complex data visualization, enabling users to build insightful, customizable dashboards that display metrics from multiple sources.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#metrics-and-exporters","title":"Metrics and Exporters","text":"<p>Kubernetes exposes some basic metrics by default through an endpoint named <code>/metrics</code>. These metrics include information about the state of the kubernetes components and resources, such as the number of running pods, CPU and memory usage, API server latency, and more. </p> <p>the <code>/metrics</code> endpoint is accessible on the kubernetes API server and provides valuable insights into the cluster's health and performance. These metrics can be utilized by monitoring tools like Prometheus to gather information and create visualizations for better cluster management.</p> <p> </p> <p>You can also set up custom exporters in kubernetes to capture custom metrics from your applications or services. These tailored tools expand monitoring capabilities, providing detailed insights beyond the standard metrics, addressing your specific monitoring needs.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#prerequisites","title":"Prerequisites","text":"<p>We will use <code>helm</code> to install Prometheus and Grafana monitoring tools. So, make sure <code>helm</code> is installed on your local machine.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#step-1-add-prometheus-and-grafana-helm-repository","title":"Step 1: Add Prometheus and Grafana Helm Repository","text":"<pre><code># Add prometheus Helm repo\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\n# Add grafana Helm repo\nhelm repo add grafana https://grafana.github.io/helm-charts\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#step-2-deploy-prometheus","title":"Step 2: Deploy Prometheus","text":"<p>Let's deploy Prometheus in our EKS kubernetes cluster.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#1-install-prometheus","title":"1. Install Prometheus","text":"<p>In this example, we are primarily going to use the standard configuration. In production you might want to override default prometheus values using <code>--set</code> command.</p> <pre><code># Create namespace\nkubectl create namespace prometheus\n\n# Install prometheus\nhelm install prometheus prometheus-community/prometheus --namespace prometheus --set server.persistentVolume.size=20Gi --set server.retention=15d\n</code></pre> <p>The output should look something like this:</p> <p> </p> <p>Make note of the prometheus endpoint in helm response (you will need this later). It should look similar to below:</p> <pre><code>The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:\nprometheus-server.prometheus.svc.cluster.local\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#2-verify-prometheus-components","title":"2. Verify Prometheus Components","text":"<p>Verify if Prometheus components were deployed as expected:</p> <pre><code># List helm installations\nhelm list -n prometheus\n\n# List all resources\nkubectl get all -n prometheus\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#3-verify-ebs-volumes","title":"3. Verify EBS Volumes","text":"<p>When you install prometheus using helm, it dynamically provisions two EBS volumes as follows:</p> <ol> <li>EBS volume of size <code>8 Gi</code> for <code>prometheus-server</code></li> <li>EBS volume of size <code>2 Gi</code> for <code>prometheus-alertmanager</code></li> </ol> <p>Go to AWS console and verify if the EBS volumes were created. You can also add <code>Name</code> tags manually to the EBS volumes to make it easier to reconize the purpose of the voumes.</p> <p>Note that in our installation we have overriden the default <code>8 Gi</code> setting for <code>prometheus-server</code> to <code>20 Gi</code> using the <code>--set</code> command.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#4-access-prometheus-server","title":"4. Access Prometheus Server","text":"<p>Let's use the <code>kubectl port-forward</code> command to access Prometheus on our local host machine as follows:</p> <pre><code>kubectl port-forward -n prometheus deploy/prometheus-server 8080:9090\n</code></pre> <p>This forwards the port <code>9090</code> of the container to port <code>8080</code> on local host machine.</p> <p>Open any browser on your local host machine and hit <code>localhost:8080</code>. You will see the Prometheus UI.</p> <p>Prometheus collects node, pods, and service metrics automatically using Prometheus service discovery configurations.</p> <p>Click on <code>Status -&gt; Targets</code> to view the targets that Prometheus registered. You can see the following targets:</p> <ul> <li><code>kubernetes-nodes</code></li> <li><code>kubernetes-pods</code></li> <li><code>kubernetes-service-endpoints</code></li> </ul>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#5-create-ingress-for-prometheus","title":"5. Create Ingress for Prometheus","text":"<p>We will use AWS load balancer and subdomain to access Prometheus. For that we are going to create an ingress resource.</p> <code>prometheus-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus-ingress\n  namespace: prometheus\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: prometheus.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: prometheus-server\n            port:\n              number: 80\n</code></pre> <p>Apply the manifest to create ingress for Prometheus:</p> <pre><code>kubectl apply -f prometheus-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress -n prometheus\n</code></pre> <p>Note</p> <p>We are using an ingress group for load balancer to have a shared load balancer because using more load balancers will be expensive since load balancers are charged hourly.</p> <p>The ingress uses an existing internet-facing load balancer from another ingress in the same group. If there isn't one, it'll create a new load balancer and attach the SSL certificate to it.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Open any browser on your local host machine and hit the Prometheus host URL to access prometheus:</p> <pre><code>https://prometheus.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#step-3-deploy-grafana","title":"Step 3: Deploy Grafana","text":"<p>Now that Prometheus is ready, let's deploy Grafana for visualization.</p> <p>In this example, we are primarily going to use the default values but we are also overriding several parameters. In production you might want to override other grafana values using <code>--set</code> command.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#1-create-prometheus-data-source-for-grafana","title":"1. Create Prometheus Data Source for Grafana","text":"<p>First, create a YAML file called <code>grafana.yaml</code> as follows:</p> <code>grafana.yaml</code> <pre><code>datasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n    - name: Prometheus\n      type: prometheus\n      url: http://prometheus-server.prometheus.svc.cluster.local\n      access: proxy\n      isDefault: true\n</code></pre> <p>The <code>grafana.yaml</code> defines the data source that Grafana would use. In this case it is the Prometheus we deployed.</p> <p>Note</p> <p>When services communicate in kubernetes, they need to provide the Fully Qualified Domain Name (FQDN) which has the following syntax:</p> <p><pre><code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local\n</code></pre> If services are in the same namespace, using just <code>&lt;service-name&gt;</code> suffices for communication within the cluster.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#2-install-grafana","title":"2. Install Grafana","text":"<pre><code># Create namespace\nkubectl create namespace grafana\n\n# Install grafana\nhelm install grafana grafana/grafana \\\n    --namespace grafana \\\n    --set persistence.enabled=true \\\n    --set adminPassword='RP6xkxD' \\\n    --set plugins='{grafana-piechart-panel}' \\\n    --values grafana.yaml\n</code></pre> <p>The output should look something like this:</p> <p> </p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#3-verify-grafana-installation","title":"3. Verify Grafana Installation","text":"<pre><code># List helm installations\nhelm list -n grafana\n\n# List all resources\nkubectl get all -n grafana\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#4-access-grafana","title":"4. Access Grafana","text":"<p>Let's use the <code>kubectl port-forward</code> command to access Grafana on our local host machine as follows:</p> <pre><code>kubectl port-forward -n grafana deploy/grafana 8081:3000\n</code></pre> <p>This forwards the port <code>3000</code> of the container to port <code>8081</code> on local host machine.</p> <p>Open any browser on your local host machine and hit <code>localhost:8081</code>. You will see the Grafana UI.</p> <p>When logging in, use the username <code>admin</code> and get the <code>password</code> by running the following command:</p> <pre><code>kubectl get secret --namespace grafana grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#5-create-ingress-for-grafana","title":"5. Create Ingress for Grafana","text":"<p>We will use AWS load balancer and subdomain to access Grafana. For that we are going to create an ingress resource.</p> <code>grafana-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: grafana\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: grafana.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: grafana\n            port:\n              number: 80\n</code></pre> <p>Apply the manifest to create ingress for Grafana:</p> <pre><code>kubectl apply -f grafana-ingress.yml\n</code></pre> <p>Verify ingress:</p> <pre><code>kubectl get ingress -n grafana\n</code></pre> <p>Note</p> <p>We are using an ingress group for load balancer to have a shared load balancer because using more load balancers will be expensive since load balancers are charged hourly.</p> <p>The ingress uses an existing internet-facing load balancer from another ingress in the same group. If there isn't one, it'll create a new load balancer and attach the SSL certificate to it.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Open any browser on your local host machine and hit the Grafana host URL to access Grafana:</p> <pre><code>https://grafana.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#clean-up","title":"Clean Up","text":"<p>If you no longer need Prometheus and Grafana, you can use the following commands to uninstall them:</p> <pre><code># Uninstall prometheus\nhelm uninstall prometheus -n prometheus\n\n# Uninstall grafana\nhelm uninstall grafana -n grafana\n\n# Delete namespace prometheus\nkubectl delete ns prometheus\n\n# Delete namespace grafana\nkubectl delete ns grafana\n</code></pre> <p>Deleting a namespace will delete all the resources in the namespace.</p>"},{"location":"kubernetes-on-eks/monitoring/monitoring-using-prometheus-and-grafana/#increase-prometheus-server-persistent-volume-size","title":"Increase Prometheus Server Persistent Volume Size","text":"<p>If you need more storage for <code>prometheus-server</code>, you can increase it by following the instructions below.</p> <p>First we need to make sure the storageclass that the <code>prometheus-server</code> uses allows volume expansion:</p> <pre><code># set nano as kube editor\nexport KUBE_EDITOR=nano\n\n# Open the editor\nkubectl edit sc gp2\n\n# Edit the storageclass by adding the following line at the end\nallowVolumeExpansion: true\n</code></pre> <p>Now, we can increase the prometheus server persistent volume by upgrading the helm chart:</p> <pre><code># Increase prometheus-server persistent volume size\nhelm upgrade prometheus prometheus-community/prometheus --namespace prometheus --set server.persistentVolume.size=50Gi\n</code></pre> <p>References:</p> <ul> <li>Monitoring using Prometheus and Grafana</li> <li>Prometheus values.yml</li> <li>Grafana values.yml</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/","title":"Configure Internal Network Load Balancer","text":"<p>You can set the value of <code>service.beta.kubernetes.io/aws-load-balancer-scheme</code> annotation to <code>internal</code> to create an internal network load balancer.</p> <p>Note</p> <p>If you have microservices on instances that are registered with a Network Load Balancer, you can't use the load balancer to provide communication between them unless the load balancer is <code>internet-facing</code> or the instances are registered by IP address. For more information, see Connections time out for requests from a target to its load balancer.</p> <p>Therefore, we will use <code>ip</code> target type for this example.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/#step-2-create-a-loadbalancer-service","title":"Step 2: Create a LoadBalancer Service","text":"<p>Let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Must specify this annotation\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internal # Default is internal\n    # Health Check\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: traffic-port\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: '10'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: '2' # ignored\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: '2'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: '2'\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Note that we are offloading the reconciliation to AWS Load Balancer Controller using the <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/#step-3-verify-aws-resources-in-aws-console","title":"Step 3: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the NLB type. It should be internal.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p> <p>Also, verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/#step-4-access-app-using-internal-load-balancer-dns","title":"Step 4: Access App Using Internal Load Balancer DNS","text":"<p>Because the network load balancer is internal, access to our app from outside the VPC is restricted. To overcome this, let's create a pod that we can use to access the load balancer and, in turn, our app. Since the pod will reside within the same VPC, we will be able to access our app.</p> <p>First, let's create a pod as follows:</p> <code>nginx-pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre> <p>Apply the manifest to create the pod:</p> <pre><code>kubectl apply -f nginx-pod.yml\n</code></pre> <p>Now, let's start a shell session inside the nginx container and hit the internal load balancer url:</p> <pre><code># Start a shell session inside the nginx container\nkubectl exec -it nginx -- bash\n\n# Hit the load balancer url using CURL\ncurl &lt;internal-nlb-dns&gt;\n</code></pre> <p>You'll see the response from the app.</p> <p>Tip</p> <p>Set <code>target-type</code> to <code>instance</code> and try to access the internal nlb dns from nginx pod. It will succeed for some of the requests while it will fail for others.</p> <p>Change the <code>target-type</code> to <code>ip</code> and try to access the internal nlb dns. It should work fine without any timeouts.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/internal-nlb/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n\u2502   |-- nginx-pod.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>NLB Timeout Error Reason</li> <li>Connections time out for requests from a target to its load balancer</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/introduction-to-nlb-using-aws-load-balancer-controller/","title":"Introduction to NLB using AWS Load Balancer Controller","text":"<p>By default, kubernetes service resources of type <code>LoadBalancer</code> gets reconciled by the kubernetes controller built into the CloudProvider component of the <code>kube-controller-manager</code> or the <code>cloud-controller-manager</code> (a.k.a. the <code>in-tree controller</code>).</p> <p>In order to let AWS Load Balancer Controller (LBC) manage the reconciliation for kubernetes services resources of type <code>LoadBalancer</code>, you need to offload the reconciliation from <code>in-tree controller</code> to AWS Load Balancer Controller explicitly.</p> <p>You can offload the reconciliation to AWS Load Balancer Controller in two ways:</p> <ol> <li>By specifying the <code>spec.loadBalancerClass</code> and set it to <code>service.k8s.aws/nlb</code></li> <li>By specifying the <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation and set it to <code>external</code> or <code>nlb-ip</code></li> </ol> <p>The AWS Load Balancer Controller manages kubernetes services in a compatible way with the legacy aws cloud provider. The annotation <code>service.beta.kubernetes.io/aws-load-balancer-type</code> or <code>spec.loadBalancerClass</code> is used to determine which controller reconciles the service.</p> <p>If <code>spec.loadBalancerClass</code> is set or the annotation value is <code>nlb-ip</code> or <code>external</code>, legacy cloud provider ignores the service resource (provided it has the correct patch) so that the AWS Load Balancer controller can take over. For all other values of the annotation, the legacy cloud provider will handle the service.</p> <p>Note</p> <p>The annotation <code>service.beta.kubernetes.io/aws-load-balancer-type</code> should be specified during service creation and not edited later. The value <code>nlb-ip</code> is deprecated and might be removed later. Use the value <code>external</code> instead.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/introduction-to-nlb-using-aws-load-balancer-controller/#nlb-target-type","title":"NLB Target Type","text":"<p>You must provide <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> annotation if you are using <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation to offload the reconciliation to AWS Load Balancer Controller.</p> <p>If you configure <code>spec.loadBalancerClass</code>, the <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> defaults to <code>instance</code>.</p> <p>References:</p> <ul> <li>NLB Using Load Balancer Controller</li> <li>NLB Annotations</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/","title":"Network Load Balancer With External DNS","text":"<p>You can use the `external-dns.alpha.kubernetes.io/hostname`` annotation to let ExternalDNS manage the Route 53 record for the kubernetes service.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#step-2-create-a-loadbalancer-service","title":"Step 2: Create a LoadBalancer Service","text":"<p>Let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Must specify this annotation\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Default is internal\n    # Health Check\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: traffic-port\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: '10'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: '2' # ignored\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: '2'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: '2'\n    # TLS\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-south-1:170476043077:certificate/2d88e035-cde7-472a-9cd3-6b6ce6ece961\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\n    #External DNS\n    external-dns.alpha.kubernetes.io/hostname: api.example.com # give your domain name here (Optional)\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 443 # Creates a listener with port 443\n      targetPort: 5000\n</code></pre> <p>Be sure to replace the value of <code>alb.ingress.kubernetes.io/certificate-arn</code> with the <code>ARN</code> of the SSL certificate you created.</p> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Note that we are offloading the reconciliation to AWS Load Balancer Controller using the <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#step-3-verify-aws-resources-in-aws-console","title":"Step 3: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the health check configuration of the target group.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p> <p>Verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre> <p>Also, go to AWS Route 53 and verify the record (<code>api.example.com</code>) that was added by ExternalDNS.</p> <p>You can also check the events that external-dns pod performs:</p> <pre><code>kubectl logs -f &lt;external-dns-pod&gt; -n external-dns\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#step-4-access-app-using-route-53-dns","title":"Step 4: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttps://api.example.com/\n\n# Health path\nhttps://api.example.com/health\n\n# Random generator path\nhttps://api.example.com/random\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-external-dns/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/","title":"Network Load Balancer With Health Check","text":"<p>Health check on target groups can be controlled with following annotations:</p> Annotation Function <code>alb.ingress.kubernetes.io/healthcheck-protocol</code> specifies the protocol used when performing health check on targets. <code>alb.ingress.kubernetes.io/healthcheck-port</code> specifies the port used when performing health check on targets. When using <code>target-type: instance</code> with a service of type <code>NodePort</code>, the healthcheck port can be set to <code>traffic-port</code> to automatically point to the correct port. <code>alb.ingress.kubernetes.io/healthcheck-path</code> specifies the HTTP path when performing health check on targets. <code>alb.ingress.kubernetes.io/healthcheck-interval-seconds</code> specifies the interval(in seconds) between health check of an individual target. <code>alb.ingress.kubernetes.io/healthcheck-timeout-seconds</code> specifies the timeout(in seconds) during which no response from a target means a failed health check. <code>alb.ingress.kubernetes.io/success-codes</code> specifies the HTTP or gRPC status code that should be expected when doing health checks against the specified health check path. <code>alb.ingress.kubernetes.io/healthy-threshold-count</code> specifies the consecutive health checks successes required before considering an unhealthy target healthy. <code>alb.ingress.kubernetes.io/unhealthy-threshold-count</code> specifies the consecutive health check failures required before considering a target unhealthy. <p>The Load Balancer Controller currently ignores the <code>timeout</code> configuration due to the limitations on the AWS NLB. The default <code>timeout</code> for <code>TCP</code> is 10s and <code>HTTP</code> is 6s.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/#step-2-create-a-loadbalancer-service","title":"Step 2: Create a LoadBalancer Service","text":"<p>Let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Must specify this annotation\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Default is internal\n    # Health Check\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: traffic-port\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: '10'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: '2' # ignored\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: '2'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: '2'\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Note that we are offloading the reconciliation to AWS Load Balancer Controller using the <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/#step-3-verify-aws-resources-in-aws-console","title":"Step 3: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the health check configuration of the target group that ingress created.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p> <p>Also, verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/#step-4-access-app-via-network-load-balancer-dns","title":"Step 4: Access App Via Network Load Balancer DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the load balancer DNS and verify if everything is working properly.</p> <p>Access the load balancer DNS by entering it in your browser. You can get the load balancer DNS either from the AWS console or the Ingress configuration.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\n&lt;nlb-dns&gt;/\n\n# Health path\n&lt;nlb-dns&gt;/health\n\n# Random generator path\n&lt;nlb-dns&gt;/random\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-health-check/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>Health Check Annotations</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/","title":"Network Load Balancer With Multiple Listeners","text":"<p>You can create multiple listener rules in NLB by specifying multiple <code>ports</code> in the kubernetes service definition.</p> <p>For each listener, one target group will be created.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#step-2-create-a-loadbalancer-service","title":"Step 2: Create a LoadBalancer Service","text":"<p>Let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Must specify this annotation\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Default is internal\n    # Health Check\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: traffic-port\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: '10'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: '2' # ignored\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: '2'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: '2'\n    # TLS\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-south-1:170476043077:certificate/2d88e035-cde7-472a-9cd3-6b6ce6ece961\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - name: http\n      port: 443 # Creates a listener with port 443\n      targetPort: 5000\n    - name: https\n      port: 80 # Creates a listener with port 80\n      targetPort: 5000\n</code></pre> <p>Be sure to replace the value of <code>alb.ingress.kubernetes.io/certificate-arn</code> with the <code>ARN</code> of the SSL certificate you created.</p> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Note that we are offloading the reconciliation to AWS Load Balancer Controller using the <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation.</p> <p>For each item in the <code>.spec.ports</code> a listener rule and corresponding target group will be created.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#step-3-verify-aws-resources-in-aws-console","title":"Step 3: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the listener rules and target groups that were created.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p> <p>Also, verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#step-4-add-record-in-route-53","title":"Step 4: Add Record in Route 53","text":"<p>Go to AWS Route 53 and add an <code>A</code> record (e.g <code>api.example.com</code>) for your domain that points to the Network Load Balancer. You can use alias to point the subdomain to the network load balancer that was created.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#step-5-access-app-using-route-53-dns","title":"Step 5: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttp://api.example.com/\nhttps://api.example.com/\n\n# Health path\nhttp://api.example.com/health\nhttps://api.example.com/health\n\n# Random generator path\nhttp://api.example.com/random\nhttps://api.example.com/random\n</code></pre> <p>How about redirecting HTTP to HTTPS?</p> <p>AWS Network Load Balancer cannot handle layer 7 thus cannot redirect <code>HTTP</code> to <code>HTTPS</code>.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-multiple-listeners/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/","title":"Network Load Balancer With TLS","text":"<p>SSL support can be controlled with the following annotations:</p> Annotation Function <code>alb.ingress.kubernetes.io/certificate-arn</code> specifies the <code>ARN</code> of one or more certificate managed by AWS Certificate Manager. The first certificate in the list will be added as default certificate. And remaining certificate will be added to the optional certificate list. <code>alb.ingress.kubernetes.io/ssl-policy</code> specifies the Security Policy that should be assigned to the ALB, allowing you to control the protocol and ciphers. This is optional and defaults to <code>ELBSecurityPolicy-2016-08</code>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#step-2-create-a-loadbalancer-service","title":"Step 2: Create a LoadBalancer Service","text":"<p>Let's create a <code>LoadBalancer</code> service as follows:</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Must specify this annotation\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Default is internal\n    # Health Check\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: traffic-port\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: '10'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: '2' # ignored\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: '2'\n    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: '2'\n    # TLS\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:ap-south-1:170476043077:certificate/2d88e035-cde7-472a-9cd3-6b6ce6ece961\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: '443'\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 443 # Creates a listener with port 443\n      targetPort: 5000\n</code></pre> <p>Be sure to replace the value of <code>alb.ingress.kubernetes.io/certificate-arn</code> with the <code>ARN</code> of the SSL certificate you created.</p> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre> <p>Note that we are offloading the reconciliation to AWS Load Balancer Controller using the <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#step-3-verify-aws-resources-in-aws-console","title":"Step 3: Verify AWS Resources in AWS Console","text":"<p>Visit the AWS console and verify the resources created by AWS Load Balancer Controller.</p> <p>Pay close attention to the health check configuration of the target group.</p> <p>Note that the Load Balancer takes some time to become <code>Active</code>.</p> <p>Also, verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#step-4-add-record-in-route-53","title":"Step 4: Add Record in Route 53","text":"<p>Go to AWS Route 53 and add an <code>A</code> record (e.g <code>api.example.com</code>) for your domain that points to the Network Load Balancer. You can use alias to point the subdomain to the network load balancer that was created.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#step-5-access-app-using-route-53-dns","title":"Step 5: Access App Using Route 53 DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the subdomain you created in Route 53 and verify if everything is working properly.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\nhttps://api.example.com/\n\n# Health path\nhttps://api.example.com/health\n\n# Random generator path\nhttps://api.example.com/random\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/nlb-with-tls/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>SSL Annotations</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/","title":"Offload Reconciliation of NLB to LBC Using Annotation","text":"<p>In this demo, we will use <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation to offload the reconciliation of Network Load Balancer (NLB) to AWS Load Balancer Controller (LBC).</p> <p>Note</p> <p>If you configure <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code>, you must provide the <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> annotation and set it to either <code>instance</code> or <code>ip</code>.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Now, let's create a <code>LoadBalancer</code> service but this time we'll offload the reconciliation of NLB to LBC using <code>.spec.loadBalancerClass</code> field.</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Must specify this annotation\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Default is internal\nspec:\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Note</p> <p>The <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> annotation is mandatory when you use the <code>service.beta.kubernetes.io/aws-load-balancer-type: external</code> annotation to offload the reconciliation of NLB to LBC.</p> <p>Also, to create an internet-facing NLB, the following annotation is required on your service:</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/#step-3-verify-the-network-load-balancer-nlb-in-aws-console","title":"Step 3: Verify the Network Load Balancer (NLB) in AWS Console","text":"<p>Visit AWS Console and verify that a network load balancer (NLB) was created.</p> <p>Also, verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/#step-4-access-app-via-network-load-balancer-dns","title":"Step 4: Access App Via Network Load Balancer DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the load balancer DNS and verify if everything is working properly.</p> <p>Access the load balancer DNS by entering it in your browser. You can get the load balancer DNS either from the AWS console or the service configuration.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\n&lt;nlb-dns&gt;/\n\n# Health path\n&lt;nlb-dns&gt;/health\n\n# Random generator path\n&lt;nlb-dns&gt;/random\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-annotation/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/","title":"Offload Reconciliation of NLB to LBC Using loadBalancerClass","text":"<p>In this demo, we will use <code>spec.loadBalancerClass</code> to offload the reconciliation of Network Load Balancer (NLB) to AWS Load Balancer Controller (LBC).</p> <p>Note</p> <p>If you configure <code>spec.loadBalancerClass</code>, the <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> defaults to <code>instance</code>.</p>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/#docker-images","title":"Docker Images","text":"<p>Here is the Docker Image used in this tutorial: reyanshkharga/nodeapp:v1</p> <p>Note</p> <p>reyanshkharga/nodeapp:v1 runs on port <code>5000</code> and has the following routes:</p> <ul> <li><code>GET /</code> Returns host info and app version</li> <li><code>GET /health</code> Returns health status of the app</li> <li><code>GET /random</code> Returns a randomly generated number between 1 and 10</li> </ul>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<p>First, let's create a deployment as follows:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo\n  template:\n    metadata:\n      labels:\n        app: demo\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:v1\n        ports:\n          - containerPort: 5000\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/#step-2-create-a-service","title":"Step 2: Create a Service","text":"<p>Now, let's create a <code>LoadBalancer</code> service but this time we'll offload the reconciliation of NLB to LBC using <code>.spec.loadBalancerClass</code> field.</p> <code>my-service.yml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nlb-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-name: my-nlb\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance # Optional\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Default is internal\nspec:\n  loadBalancerClass: service.k8s.aws/nlb\n  type: LoadBalancer\n  selector:\n    app: demo\n  ports:\n    - port: 80\n      targetPort: 5000\n</code></pre> <p>Note</p> <p>The <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> annotation is optional and defaults to <code>instance</code> when you use the <code>.spec.loadBalancerClass</code> field to offload the reconciliation of NLB to LBC.</p> <p>Also, to create an internet-facing NLB, the following annotation is required on your service:</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n</code></pre> <p>Apply the manifest to create the service:</p> <pre><code>kubectl apply -f my-service.yml\n</code></pre> <p>Verify service:</p> <pre><code>kubectl get svc\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/#step-3-verify-the-network-load-balancer-nlb-in-aws-console","title":"Step 3: Verify the Network Load Balancer (NLB) in AWS Console","text":"<p>Visit AWS Console and verify that a network load balancer (NLB) was created.</p> <p>Also, verify that the NLB was created by <code>AWS Load Balancer Controller</code>. You can check the events in the logs as follows:</p> <pre><code>kubectl logs -f deploy/aws-load-balancer-controller -n aws-load-balancer-controller --all-containers=true\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/#step-4-access-app-via-network-load-balancer-dns","title":"Step 4: Access App Via Network Load Balancer DNS","text":"<p>Once the load balancer is in <code>Active</code> state, you can hit the load balancer DNS and verify if everything is working properly.</p> <p>Access the load balancer DNS by entering it in your browser. You can get the load balancer DNS either from the AWS console or the service configuration.</p> <p>Try accessing the following paths:</p> <pre><code># Root path\n&lt;nlb-dns&gt;/\n\n# Health path\n&lt;nlb-dns&gt;/health\n\n# Random generator path\n&lt;nlb-dns&gt;/random\n</code></pre>"},{"location":"kubernetes-on-eks/nlb-using-aws-load-balancer-controller/offload-reconciliation-of-nlb-to-lbc-using-loadbalancerclass/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/private-eks-nodegroup/create-private-eks-nodegroup/","title":"Create Private EKS NodeGroup","text":"<p>So far, our focus has been on using public NodeGroups. However, in a production environment, it is crucial to ensure the security of your nodes by making them private and restricting public access.</p> <p>From now on, our approach will involve using private nodegroups as the preferred method.</p> <p>First, we will create a private nodegroup, and subsequently, we will delete the existing public nodegroup.</p>"},{"location":"kubernetes-on-eks/private-eks-nodegroup/create-private-eks-nodegroup/#step-1-verify-existing-nodegroups-and-nodes","title":"Step 1: Verify Existing NodeGroups and Nodes","text":"<p>Make sure the AWS CLI is configured and the profile is exported if you are using a named profile:</p> <pre><code>export AWS_PROFILE=&lt;aws-profile-name&gt;\n</code></pre> <p>Verify existing nodegroups and nodes:</p> <pre><code># List nodegroups\neksctl get nodegroups --cluster &lt;cluster-name&gt;\n\n# List worker nodes\nkubectl get nodes\n</code></pre>"},{"location":"kubernetes-on-eks/private-eks-nodegroup/create-private-eks-nodegroup/#step-2-create-a-private-eks-nodegroup","title":"Step 2: Create a Private EKS NodeGroup","text":"<p>We will use <code>eksctl</code> to create a private NodeGroup.</p> <p>We will use a configuration file since it requires numerous parameters, although you can also do it via the command line.</p> <p>You can reuse the <code>cluster.yml</code> file we used earlier to create cluster and public EKS nodegroup. Simply apply the following modifications:</p> <ol> <li>Make a copy of <code>cluster.yml</code> and name it anything you like. Let's name it <code>private-nodegroup.yml</code>.</li> <li>Remove the <code>version</code> field from the metadata object. We only need cluster <code>name</code> and <code>region</code>.</li> <li>Remove the top-level <code>iam</code> object. This is needed only when we create the cluster.</li> <li>In the <code>managedNodeGroups</code> change the <code>name</code> field. Let's name it <code>private-nodegroup</code>.</li> <li>In the <code>managedNodeGroups</code> change the <code>privateNetworking</code> field to <code>true</code> since we want our worker nodes to be present in private subnets.</li> </ol> <p>The modified file should look similar to the below:</p> <code>private-nodegroup.yml</code> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: my-cluster\n  region: ap-south-1\n  # version: \"1.28\"\n\navailabilityZones:\n  - ap-south-1a\n  - ap-south-1b\n  - ap-south-1c\n\n# iam:\n#   withOIDC: true\n\nmanagedNodeGroups:\n  - name: private-nodegroup\n    privateNetworking: true\n    instanceType: t3.medium\n    minSize: 2\n    maxSize: 2\n    volumeSize: 20\n    ssh:\n      allow: true\n      publicKeyName: my-eks-key\n    iam:\n      withAddonPolicies:\n        imageBuilder: true\n        autoScaler: true\n        externalDNS: true\n        certManager: true\n        appMesh: true\n        appMeshPreview: true\n        ebs: true\n        fsx: true\n        efs: true\n        awsLoadBalancerController: true\n        xRay: true\n        cloudWatch: true\n</code></pre> <p>Apply the config to create private nodegroup in our eks cluster:</p> <pre><code># Create private nodegroup\neksctl create nodegroup --config-file=private-nodegroup.yml\n</code></pre>"},{"location":"kubernetes-on-eks/private-eks-nodegroup/create-private-eks-nodegroup/#step-3-verify-the-private-nodegroup-and-nodes","title":"Step 3: Verify the Private NodeGroup and Nodes","text":"<pre><code># List nodegroups\neksctl get nodegroups --cluster &lt;cluster-name&gt;\n\n# List worker nodes\nkubectl get nodes\n</code></pre>"},{"location":"kubernetes-on-eks/private-eks-nodegroup/create-private-eks-nodegroup/#step-4-delete-public-nodes-and-nodegroup","title":"Step 4: Delete Public Nodes and NodeGroup","text":"<p>Once the nodes from the private nodegroup are in <code>Ready</code> state we can go ahead and delete our public nodegroup.</p> <p>To safely delete the public nodegroup in your Amazon EKS cluster, follow these steps:</p> <ol> <li> <p>Confirm that all necessary applications and services are running smoothly:</p> <pre><code># List pods in all namespaces\nkubectl get pods -A -o wide\n\n# List services in all namespaces\nkubectl get svc -A\n</code></pre> </li> <li> <p>Cordon the public nodes to prevent new pods from being scheduled on them. You can use the following command to cordon each node in the public nodegroup:</p> <pre><code>kubectl cordon &lt;node_name&gt;\n</code></pre> </li> <li> <p>Drain the public nodes to gracefully evict any running workloads. You can use the following command to drain each node:</p> <pre><code>kubectl drain &lt;node_name&gt; --ignore-daemonsets --delete-emptydir-data\n</code></pre> </li> <li> <p>Verify that all pods (except Daemonset pods) have been successfully moved to the private nodes by running the following command:</p> <pre><code># List nodes\nkubectl get nodes\n\n# List pods\nkubectl get pods -A -o wide\n</code></pre> </li> <li> <p>Once all pods have been evacuated, you can delete the public nodegroup safely:</p> <pre><code># Delete public nodegroup\neksctl delete nodegroup &lt;public-nodegroup-name&gt; --cluster &lt;cluster-name&gt;\n</code></pre> </li> <li> <p>Verify nodes and nodegroups:</p> <pre><code># List nodegroups\neksctl get nodegroups --cluster &lt;cluster-name&gt;\n\n# List worker nodes\nkubectl get nodes\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/","title":"Associate Pod With Service Account","text":"<p>Let's see how <code>service account</code> is used to grant pods the permissions to call kubernetes API and manage resources.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-1-create-a-pod","title":"Step 1: Create a Pod","text":"<p>Let's create pods. We'll use deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-2-verify-deployment-and-pods","title":"Step 2: Verify Deployment and Pods","text":"<pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-3-describe-pod","title":"Step 3: Describe Pod","text":"<pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>You'll notice the pod has the <code>default</code> service account assigned to it even though we didn't specify it explicitly.</p> <p>Now, let's take a look at the volume mounts.</p> <pre><code>kubectl get pod &lt;pod-name&gt; -o yaml\n</code></pre> <p>You'll observe that <code>token</code>, <code>ca.crt</code>, and <code>namespace</code> files are mounted in the pod at <code>/var/run/secrets/kubernetes.io/serviceaccount</code>.</p> <p>The tokens are obtained directly through the <code>TokenRequest</code> API, and are mounted at the path <code>/var/run/secrets/kubernetes.io/serviceaccount</code> into pods using a projected volume. Also, these tokens will be automatically invalidated when their associated pod is deleted.</p> <p>Note</p> <p>A <code>projected</code> volume is a volume that projects several existing volume sources into the same directory.</p> <p>By having the service account and token available within the pod, you can utilize them to securely communicate with the kubernetes API or other services that require authentication within the cluster.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-4-verify-volume-mounts","title":"Step 4: Verify Volume Mounts","text":"<pre><code># Start a shell session inside the pod\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# List mounted files\nls /var/run/secrets/kubernetes.io/serviceaccount\n</code></pre> <p>You'll see the <code>ca.crt</code>, <code>namespace</code>, and <code>token</code> files as discussed in the previous step.</p> <ul> <li><code>ca.cert</code>: The trust anchor needed to validate the certificate presented by the kubernetes API Server in this cluster.</li> <li><code>namespace</code>: Namespace the secret is in.</li> <li><code>token</code>: A bearer token that you can attach to API requests. This has a structure of a JSON Web Token (JWT).</li> </ul>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-5-decode-certificate-file","title":"Step 5: Decode Certificate File","text":"<p>Let's decode the <code>ca.crt</code> file:</p> <pre><code># Get the content of ca.crt\ncat ca.crt\n</code></pre> <p>Copy the content of the <code>ca.crt</code>.</p> <p>Visit SSL Certificate Decoder and paste the content of certificate. Next, click on decode.</p> <p>You'll see the detailed information about the certificate.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-6-decode-token-file","title":"Step 6: Decode Token File","text":"<p>Let's decode the <code>token</code> file:</p> <pre><code># Get the content of token\ncat token\n</code></pre> <p>Copy the content of the <code>token</code>.</p> <p>Visit jwt.io and paste the content of <code>token</code>.</p> <p>You'll see the detailed information about the <code>token</code> such as expiry (exp), issued at time (iat), issuer (iss), subject (sub) and other details.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-7-access-the-kubernetes-api","title":"Step 7: Access the Kubernetes API","text":"<p>List kubernetes services:</p> <pre><code>kubectl get svc\n</code></pre> <p>You'll see a service named <code>kubernetes</code>.</p> <p>The <code>kubernetes</code> service in the <code>default</code> namespace is a service which forwards requests to the kubernetes master ( Typically kubernetes API server).</p> <p>Now, let's start a shell session inside the container and try to access kubernetes API:</p> <pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Change working directory\ncd /var/run/secrets/kubernetes.io/serviceaccount\n\n# Store the token in a variable\nTOKEN=$(cat token)\n\n# Verify the content of TOKEN variable\necho $TOKEN\n</code></pre> <p>Call the Kubernetes API:</p> <pre><code>curl https://kubernetes/api --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre> <p>A response from the kubernetes API server is returned with status code <code>200</code>.</p> <p>Now, call the <code>v1</code> API:</p> <pre><code>curl https://kubernetes/api/v1 --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre> <p>Again a response from the Kubernetes API server is returned with status code <code>200</code>.</p> <p>Call the <code>pods</code> API:</p> <pre><code>curl https://kubernetes/api/v1/namespaces/default/pods --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre> <p>A response from the kubernetes API server is returned with status code <code>403 (Forbidden)</code> and an error message that says:</p> <pre><code>\"pods is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\"\n</code></pre> <p>This is because the service account (the identity) the pod uses doesn't have permission to call the pods api.</p> <p>Here are the steps you can follow to assign permissions to a service account:</p> <ul> <li>Create a role using kubernetes <code>Role</code> object</li> <li>Bind the service account to the <code>Role</code> using kubernetes <code>RoleBinding</code> object</li> </ul> <p>Since <code>default</code> service account is assigned to each pod that is created when you don't provide the service account name, modifying the permission of this service account is not recommended.</p> <p>We'll create a new service account and then modify permission of that service account.</p> <p>Let's clean up first:</p> <pre><code>kubectl delete -f my-deployment.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#step-8-create-service-account-with-permission-to-call-pods-api","title":"Step 8: Create Service Account with Permission to Call Pods API","text":"<p>Let's create a <code>service account</code> that has permission to call the kubernetes pods API and then assign this service account to a pod.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#1-create-a-service-account","title":"1. Create a Service Account","text":"<code>my-service-account.yml</code> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n</code></pre> <p>Apply the manifest to create the service account:</p> <pre><code>kubectl apply -f service-account.yml\n</code></pre> <p>Verify the service account:</p> <pre><code>kubectl get sa\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#2-create-pods","title":"2. Create Pods","text":"<p>Let's create pods that use the service account we created in the previous step. We'll use deployment to create pods:</p> <code>my-deployment.yml</code> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: my-service-account\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre> <p>Apply the manifest to create the deployment:</p> <pre><code>kubectl apply -f my-deployment.yml\n</code></pre> <p>Verify the deployment and pods:</p> <pre><code># List deployments\nkubectl get deployments\n\n# List pods\nkubectl get pods\n\n# Describe pod\nkubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>You'll notice that <code>my-service-account</code> is assigned to the pods.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#3-try-to-access-the-kubernetes-api","title":"3. Try to Access the Kubernetes API","text":"<pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Change working directory\ncd /var/run/secrets/kubernetes.io/serviceaccount\n\n# Store the token in a variable\nTOKEN=$(cat token)\n\n# Verify the content of TOKEN variable\necho $TOKEN\n</code></pre> <p>Call the Kubernetes API:</p> <pre><code>curl https://kubernetes/api --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre> <p>A response from the kubernetes API server is returned with status code <code>200</code>.</p> <p>Call the <code>v1</code> API: <pre><code>curl https://kubernetes/api/v1 --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre></p> <p>Again a response from the Kubernetes API server is returned with status code <code>200</code>.</p> <p>Call the <code>pods</code> API:</p> <pre><code>curl https://kubernetes/api/v1/namespaces/default/pods --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre> <p>A response from the kubernetes <code>API server</code> is returned with status code <code>403 (Forbidden)</code> and an error message that says:</p> <pre><code>\"pods is forbidden: User \\\"system:serviceaccount:default:default\\\" cannot list resource \\\"pods\\\" in API group \\\"\\\" in the namespace \\\"default\\\"\"\n</code></pre> <p>This is because the service account still doesn't have the permission to call the <code>pods</code> API.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#4-create-a-role","title":"4. Create a Role","text":"<p>Let's create a role that has permissions to call the pods API. For this, we need to create a <code>Role</code> kubernetes object as follows:</p> <code>my-role.yml</code> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: my-role\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>Apply the manifest to create the role:</p> <pre><code>kubectl apply -f my-role.yml\n</code></pre> <p>Verify the role:</p> <pre><code># List roles\nkubectl get roles\n\n# Describe the role\nkubectl describe role my-role\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#5-bind-the-role-to-the-service-account","title":"5. Bind the Role to the Service Account","text":"<p>Now, let's bind the role to the service account we created. For this, we need to create a <code>RoleBinding</code> kubernetes object as follows:</p> <code>my-rolebinding.yml</code> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: my-rolebinding\nsubjects:\n  - kind: ServiceAccount\n    name: my-service-account\nroleRef:\n  kind: Role\n  name: my-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Apply the manifest to create the role binding:</p> <pre><code>kubectl apply -f my-rolebinding.yml\n</code></pre> <p>Verify the role bindings:</p> <pre><code># List rolebindings\nkubectl get rolebindings\n\n# Describe the rolebinding\nkubectl describe rolebinding my-rolebinding\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#6-access-the-kubernetes-api-again","title":"6. Access the Kubernetes API Again","text":"<pre><code># Start a shell session inside the container\nkubectl exec -it &lt;pod-name&gt; -- bash\n\n# Change working directory\ncd /var/run/secrets/kubernetes.io/serviceaccount\n\n# Store the token in a variable\nTOKEN=$(cat token)\n\n# Verify the content of TOKEN variable\necho $TOKEN\n</code></pre> <p>Call the Kubernetes API: <pre><code>curl https://kubernetes/api --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre></p> <p>A response from the kubernetes API server is returned with status code <code>200</code>.</p> <p>Call the v1 API: <pre><code>curl https://kubernetes/api/v1 --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre></p> <p>Again a response from the Kubernetes API server is returned with status code <code>200</code>.</p> <p>Call the pods API:</p> <pre><code>curl https://kubernetes/api/v1/namespaces/default/pods --header \"Authorization: Bearer $TOKEN\" --cacert ./ca.crt\n</code></pre> <p>This time you will get a response with status code <code>200</code>. This is because the <code>service account</code> the pods use has access to the pods API.</p> <p>Here's a pictorial representation of how pods use a service account to access kubernetes resources:</p> <p> </p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/associate-pod-with-service-account/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- my-deployment.yml\n\u2502   |-- my-service-account.yml\n\u2502   |-- my-role.yml\n\u2502   |-- my-rolebinding.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre> <p>References:</p> <ul> <li>SSL Certificate Decoder</li> <li>jwt.io</li> </ul>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/create-service-account/","title":"Create and Manage Service Accounts","text":"<p>Let's see how we can create and manage Service Accounts.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/create-service-account/#step-1-create-a-service-account","title":"Step 1: Create a Service Account","text":"<p>Let's create a service account in the <code>default</code> namespace:</p> <code>my-service-account.yml</code> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n</code></pre> <p>Apply the manifest to create the Service Account:</p> <pre><code>kubectl apply -f my-service-account.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/create-service-account/#step-2-list-service-accounts","title":"Step 2: List Service Accounts","text":"<p>List service accounts in the <code>default</code> namespace:</p> <pre><code>kubectl get sa\n{OR}\nkubectl get serviceaccount\n{OR}\nkubectl get serviceaccounts\n</code></pre> <p>Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms. The following commands produce the same output:</p> <pre><code>kubectl get sa \nkubectl get serviceaccount\nkubectl get serviceaccounts\n</code></pre> <p>Note</p> <p><code>serviceaccount</code> is abbreviated as <code>sa</code>.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/create-service-account/#step-3-describe-a-service-account","title":"Step 3: Describe a Service Account","text":"<pre><code>kubectl describe serviceaccount my-service-account\n{OR}\nkubectl describe serviceaccount/my-service-account\n</code></pre> <p>Note</p> <p>In kubernetes versions <code>1.23</code> and earlier, when a <code>service account</code> is created, a <code>secret</code> is also generated to store the associated <code>token</code>.</p> <p>Starting from kubernetes version <code>1.24</code>, a significant change is implemented regarding service accounts and token generation. Unlike previous versions, service accounts no longer generate tokens as secrets by default.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/create-service-account/#step-4-delete-a-service-account","title":"Step 4: Delete a Service Account","text":"<p>Delete the service account:</p> <pre><code>kubectl delete serviceaccount my-service-account\n{OR}\nkubectl delete serviceaccount/my-service-account\n</code></pre>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/introduction-to-service-account/","title":"Introduction to Service Account","text":"<p>A <code>Service Account</code> in kubernetes provides an identity for processes that run in a pod.</p> <p>You use kubernetes <code>Service Accounts</code> when you need to manage kubernetes resources (nodes, pods, deployments etc.) from inside a pod.</p> <p>To access kubernetes <code>API Server</code> you need an <code>authentication token</code>. The processes that are running inside your containers use a <code>Service Account</code> to authenticate with the <code>API server</code>.</p> <p>Just like a user account represents a human, a <code>Service Account</code> represents and provides an identity to your pods.</p> <p>Each pod you create has a <code>Service Account</code> assigned to it even if you don't explicitly provide a <code>Service Account</code> name. If you don't provide the <code>Service Account</code> name ubernetes will assign a default <code>Service Account</code> for the pod.</p> <p>References:</p> <ul> <li>Kubernetes Service Accounts</li> </ul>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/the-default-service-account/","title":"The Default Service Account","text":"<p>Every namespace has a <code>default</code> service account. If you don't specify a different service account for a pod within a namespace, it will automatically use the <code>default</code> service account for API access and resource interactions.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/the-default-service-account/#step-1-list-service-accounts-in-default-namespace","title":"Step 1: List Service Accounts in Default Namespace","text":"<pre><code>kubectl get sa\n{OR}\nkubectl get serviceaccount\n{OR}\nkubectl get serviceaccounts\n</code></pre> <p>You'll see a service account named <code>default</code>.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/the-default-service-account/#step-2-create-a-namespace-and-verify-the-default-service-account","title":"Step 2: Create a Namespace and Verify the Default Service Account","text":"<p>Now, let's create a namespace called <code>dev</code> and list service accounts in that namespace:</p> <pre><code># Create namespace dev\nkubectl create ns dev\n\n# List service accounts in dev namespace\nkubectl get sa -n dev\n</code></pre> <p>You'll see a service account named <code>default</code> in the <code>dev</code> namespace.</p>"},{"location":"kubernetes-on-eks/service-account-in-kubernetes/the-default-service-account/#clean-up","title":"Clean Up","text":"<p>Delete the <code>dev</code> namespace we created:</p> <pre><code>kubectl delete ns dev\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/canary-deployment-using-istio/","title":"Canary Deployment Using Istio","text":"<p>Canary deployment is a gradual release strategy in software development. It involves rolling out new application versions to a small subset of users before a full release, allowing developers to monitor performance and address issues before reaching the entire user base.</p> <p>Suppose you have two versions of an application. Let's say <code>v1</code> and <code>v2</code>. You may want to serve version <code>v2</code> of the application only to a subset of users (like those with the cookie <code>version=v2</code> set in the header \u2014 often injected by developers for particular users). How do you do that?</p> <p>Istio makes canary deployment simple and straightforward.</p> <p>For that you first create a istio <code>DestinationRule</code> that defines the subsets of applications you have (<code>v1</code> and <code>v2</code> in this case).</p> <p>Once you have defined the subsets, you can configure Istio <code>VirtualService</code> to split traffic between them based on matching conditions. For instance, if the cookie <code>version=v2</code> is found in the header, you can route the traffic to <code>v2</code>; otherwise, direct it to <code>v1</code>.</p> <pre><code>graph LR\n  A(Traffic) --&gt; G(Gateway);\n  G --&gt; B(Virtual Service);\n  B --&gt;|\"If cookie 'version=v2' \n  is set\"| C(\"Destination Rule\n  (subeset v2)\");\n  B --&gt;|\"Else\"| D(\"Destination Rule\n  (subeset v1)\");\n  C --&gt; E(Reviews - v2);\n  D --&gt; F(Reviews - v1);</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/canary-deployment-using-istio/#step-1-deploy-application-with-multiple-versions","title":"Step 1: Deploy Application With Multiple Versions","text":"<p>First, let's deploy two versions of the application: v1 and v2, along with other Istio components like Destination Rule, Virtual Service, and Gateway.</p> <code>00-namespace.yml</code> <code>nginx-deployment-v1.yml</code> <code>nginx-deployment-v2.yml</code> <code>nginx-service.yml</code> <code>gateway.yml</code> <code>destination-rule.yml</code> <code>virtual-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: canary-deployment\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-v1\n  namespace: canary-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-v2\n  namespace: canary-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v2\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v2\n        imagePullPolicy: Always\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: canary-deployment\nspec:\n  type: ClusterIP\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nginx-gateway\n  namespace: canary-deployment\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nginx-app.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: nginx-destinationrule\n  namespace: canary-deployment\nspec:\n  host: nginx-service\n  subsets:\n    - name: v1\n      labels:\n        version: \"v1\"\n    - name: v2\n      labels:\n        version: \"v2\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: nginx-virtualservice\n  namespace: canary-deployment\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"nginx-app.example.com\"\n  gateways:\n  - nginx-gateway\n  http:\n  - match:\n    - headers:\n        cookie:\n          regex: \"^(.*?;)?(version=v2)(;.*)?$\"\n    route:\n    - destination:\n        host: nginx-service\n        subset: v2\n  - route:\n    - destination:\n        host: nginx-service\n        subset: v1\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> annotation in virtual service with the istio load balancer DNS.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment-v1.yml\n\u2502   |-- nginx-deployment-v2.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- destination-rule.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes and istio objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>Verify if the istio proxies are created for the application:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/canary-deployment-using-istio/#step-2-verify-if-cookie-is-working-as-expected","title":"Step 2: Verify if Cookie is Working as Expected","text":"<p>Usually developers can inject cookies for specific users but in this case we'll do it manually via the browser.</p> <p>hit the host url in the browser and then do the following:</p> <p>Go to inspect -&gt; application -&gt; cookies (expand it) -&gt; click on your site url -&gt; add a cookie</p> <pre><code># Add cookie\nversion=v2\n</code></pre> <p>Now open a new tab and hit the host url again. You'll see that only the <code>v2</code> version of the nginx app is served.</p>"},{"location":"kubernetes-on-eks/service-mesh/canary-deployment-using-istio/#step-3-generate-load-and-verify-traffic-distribution-in-kiali","title":"Step 3: Generate Load and Verify Traffic Distribution in Kiali","text":"<p>First hit the host url without any cookie and view the traffic distrubution in kiali. You'll see that 100% of the traffic is sent to <code>v1</code> of the nginx app.</p> <p>After some time also hit the url with cookies and view the traffic distribution in kiali. With correct cookie, you will see that the traffic is also sent to <code>v2</code> version of the app.</p> <p>Let's also use a script to generate some traffic and verify the traffic distribution in kiali.</p> <p>First, let's write a script that generates traffic:</p> <code>generate-traffic.sh</code> <pre><code>#!/bin/bash\nwhile true\ndo\n  curl -s -f -o /dev/null \"https://nginx-app.example.com\"\n  echo \"sleeping for 0.5 seconds\"\n  sleep 0.5\n  curl -s -f -o /dev/null \"https://nginx-app.example.com\" --cookie \"version=v2\"\n  echo \"sleeping for 0.5 seconds\"\n  sleep 0.5\ndone\n</code></pre> <p>Now, let's execute the script to generate traffic:</p> <pre><code># Give execute permission to script\nchmod +x generate-traffic.sh\n\n# Execute script\n./generate-traffic.sh\n</code></pre> <p>Wait for about 5 minutes and then view trafic distribution graph in kiali. Verify that the traffic is equally distributed between <code>v1</code> and <code>v2</code> versions of the app.</p>"},{"location":"kubernetes-on-eks/service-mesh/canary-deployment-using-istio/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment-v1.yml\n\u2502   |-- nginx-deployment-v2.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- destination-rule.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's delete all the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/circuit-breaking-using-istio/","title":"Circuit Breaking Using Istio","text":"<p>Circuit breaking is an important pattern for creating resilient microservice applications. Circuit breaking allows you to write applications that limit the impact of failures, latency spikes, and other undesirable effects of network peculiarities.</p> <p>circuit breaking in Istio is handled by <code>DestinationRule</code>.</p>"},{"location":"kubernetes-on-eks/service-mesh/circuit-breaking-using-istio/#how-does-circuit-breaker-work","title":"How Does Circuit Breaker Work?","text":"<p>A circuit breaker typically operates in three main states: <code>Closed</code>, <code>Open</code>, and <code>Half-Open</code>.</p> <p> </p> <ol> <li> <p>Closed</p> <p>Normal operation, requests pass through, system monitors for issues.</p> <p>If requests are successful, it remains in the <code>Closed</code> state. However, if the Failure Threshold is exceeded, it transitions to the <code>Open</code> state.</p> </li> <li> <p>Open</p> <p>Detected issue, blocks all incoming requests immediately.</p> <p>Once in the <code>Open</code> state, the circuit breaker undergoes a reset after a pre-defined delay. Following the reset, it transitions to the <code>Half-Open</code> state.</p> </li> <li> <p>Half-Open</p> <p>Testing phase, allows limited requests to check if the issue is resolved before deciding to fully reopen or stay closed.</p> <p>If the request is successful, it transitions to the <code>Closed</code> state; if unsuccessful, it transitions back to the <code>Open</code> state.</p> </li> </ol> <p>Let's see it in action!</p>"},{"location":"kubernetes-on-eks/service-mesh/circuit-breaking-using-istio/#step-1-deploy-application","title":"Step 1: Deploy Application","text":"<p>First, let's deploy the application and other Istio components:</p> <code>00-namespace.yml</code> <code>nodeapp-deployment.yml</code> <code>nodeapp-service.yml</code> <code>gateway.yml</code> <code>destination-rule.yml</code> <code>virtual-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: circuit-breaker\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nodeapp-deployment\n  namespace: circuit-breaker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n        version: v1\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:buggy\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nodeapp-service\n  namespace: circuit-breaker\nspec:\n  type: ClusterIP\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nodeapp-gateway\n  namespace: circuit-breaker\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nodeapp.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: nodeapp-destinationrule\n  namespace: circuit-breaker\nspec:\n  host: nodeapp-service\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 10             # Maximum number of HTTP1/TCP connections to a destination host. Default 2^32-1.\n      http:\n        http1MaxPendingRequests: 10   # Maximum number of requests that will be queued while waiting for a ready connection pool connection. Default 1024\n    outlierDetection:\n      consecutive5xxErrors: 1         # 1 consecutive upstream 5xx error\n      interval: 1s\n      baseEjectionTime: 1m            # Eject upstream host for 1 minute\n      maxEjectionPercent: 100         # Eject 100% of the upstream hosts\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: nodeapp-virtualservice\n  namespace: circuit-breaker\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"nodeapp.example.com\"\n  gateways:\n  - nodeapp-gateway\n  http:\n  - route:\n    - destination:\n        host: nodeapp-service\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> annotation in virtual service with the istio load balancer DNS.</p> <p>Note</p> <p>The <code>reyanshkharga/nodeapp:buggy</code> application intermittently throws an intentional error with HTTP status code <code>502</code>.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nodeapp-deployment.yml\n\u2502   |-- nodeapp-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- destination-rule.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes and istio objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>Verify if the istio proxies are created for the application:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/circuit-breaking-using-istio/#step-2-generate-traffic-using-fortio","title":"Step 2: Generate Traffic Using Fortio","text":"<p>Fortio is a fast, small (4Mb docker image, minimal dependencies), reusable, embeddable go library as well as a command line tool and server process, the server includes a simple web UI and REST API to trigger run and see graphical representation of the results (both a single latency graph and a multiple results comparative min, max, avg, qps and percentiles graphs).</p> <p>Deploy a fortio pod as follows:</p> <code>fortio-traffic-generator.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name:  traffic-generator\n  namespace: circuit-breaker\n  labels:\n    app: traffic-generator\n    version: v1\nspec:\n  containers:\n  - name: fortio\n    image: fortio/fortio\n</code></pre> <pre><code>kubectl apply -f fortio-traffic-generator.yml\n</code></pre> <p>Now, let's generate traffic on our nodeapp service using fortio as follows:</p> <pre><code># Set fortio pod name\nexport FORTIO_POD=traffic-generator\n\n# Call nodeapp service using fortio pod (Try this multiple times)\nkubectl exec $FORTIO_POD -n circuit-breaker -c fortio -- /usr/bin/fortio curl -quiet http://nodeapp-service\n</code></pre> <p>You'll notice that as soon as the application throws a <code>5xx</code> error (<code>502</code> in this case), the circuit breaker outlier detector ejects the pod from the load balancing pool for specified <code>baseEjectionTime</code> and in that time period all the requests fail. This is circuit breaking in action.</p> <p>Wait for <code>baseEjectionTime</code> and try again.</p>"},{"location":"kubernetes-on-eks/service-mesh/circuit-breaking-using-istio/#step-3-view-prometheus-metrics","title":"Step 3: View Prometheus Metrics","text":"<p>You can see the <code>UO (upstream overflow)</code> metric in prometheus using the following query:</p> <pre><code>sum(istio_requests_total{destination_app=\"nodeapp\", source_app=\"traffic-generator\"}) by (response_code, response_flags, source_app)\n</code></pre> <p><code>U0</code> flag indicates that requests were rejected due to circuit breaker configuration.</p>"},{"location":"kubernetes-on-eks/service-mesh/circuit-breaking-using-istio/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nodeapp-deployment.yml\n\u2502   |-- nodeapp-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- destination-rule.yml\n\u2502   |-- virtual-service.yml\n\u2502   |-- fortio-traffic-generator.yml\n</code></pre> <p>Let's delete all the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/configure-external-dns-for-istio/","title":"Configure External DNS to Use Istio Gateway and Virtual Service","text":"<p>In this tutorial we are going to update the existing external-dns setup so that it seamlessly integrates with the istio gateway and virtual service. </p> <p>With this update ExternalDNS will automatically add records in Route 53 based on hosts defined in Istio Gateway or Virtual Service.</p>"},{"location":"kubernetes-on-eks/service-mesh/configure-external-dns-for-istio/#step-1-update-the-externaldns-manifest","title":"Step 1: Update the ExternalDNS Manifest","text":"<p>Add the following arguments in the external-dns Deployment:</p> <pre><code>--source=istio-gateway\n--source=istio-virtualservice\n</code></pre> <p>Also, in the <code>ClusterRole</code> append a new rule for istio as follows:</p> <pre><code>- apiGroups: [\"networking.istio.io\"]\n  resources: [\"gateways\", \"virtualservices\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n</code></pre> <p>The final YAML manifest file should looke like this:</p> <code>external-dns.yml</code> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\", \"pods\", \"nodes\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\", \"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"networking.istio.io\"]\n  resources: [\"gateways\", \"virtualservices\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\n  labels:\n    app.kubernetes.io/name: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n    spec:\n      serviceAccountName: external-dns\n      securityContext:\n        fsGroup: 65534\n      containers:\n      - name: external-dns\n        image: bitnami/external-dns:0.13.1\n        # must specify env AWS_REGION in AWS china regions\n        # env:\n        # - name: AWS_REGION\n        #   value: cn-north-1\n        args:\n        - --source=service\n        - --source=ingress\n        - --source=istio-gateway        # choose one\n        - --source=istio-virtualservice # or both\n        # - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        # - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        # - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/configure-external-dns-for-istio/#step-2-apply-the-manifest","title":"Step 2: Apply the Manifest","text":"<p>Apply the manifest to update the ExternalDNS:</p> <pre><code>kubectl apply -f updated-external-dns.yaml\n</code></pre> <p>References:</p> <ul> <li>Configuring ExternalDNS to Use Istio Gateway and/or Virtual Service</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/delete-book-management-microservice/","title":"Delete Book Management Microservices","text":"<p>Let's delete the resources we created for book management microservices and clean up our environment.</p>"},{"location":"kubernetes-on-eks/service-mesh/delete-book-management-microservice/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-genres-db\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- configmap.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n|   |-- book-details-db\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- configmap.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n|   |-- book-genres\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- gateway.yml\n\u2502   |   |-- virtualservice.yml\n|   |-- book-details\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- gateway.yml\n\u2502   |   |-- virtualservice.yml\n|   |-- book-web\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- gateway.yml\n\u2502   |   |-- virtualservice.yml\n</code></pre> <p>Let's delete all the resources we created:</p> <pre><code>kubectl delete -f manifests/ --recursive\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/delete-istio/","title":"Delete Istio","text":"<p>Delete all the resources created by Istio since we'll be re-installing it in a way that will allow Istio to work with AWS application load balancer.</p> <p>Remember, by default Istio creates an AWS classic load balancer. We can configure Istio to create application load balancer instead of classic load balancer. We'll see how in the next section.</p>"},{"location":"kubernetes-on-eks/service-mesh/delete-istio/#uninstall-istio","title":"Uninstall Istio","text":"<pre><code># Uninstall istio\nistioctl uninstall --purge\n</code></pre> <p>Also, make sure to delete all the ingress resources we created for prometheus, grafana, kiali and jaegar. We'll use Istio gateway this time instead of ingress.</p> <p>We'll also delete book management microservices that we created. We'll recreate them using istio gateway.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-and-external-dns/","title":"Deploy Application With Istio and ExternalDNS","text":"<p>Now that we have updated our ExternalDNS setup, let's deploy a simple application using traffic management API resources such as Gateway and Virtual Service along with ExternalDNS configurations.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-and-external-dns/#step-1-view-istio-proxy-configuration","title":"Step 1: View Istio Proxy Configuration","text":"<p>Note down the current proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>It should look something like this:</p> <pre><code>NAME     VHOST NAME     DOMAINS     MATCH                  VIRTUAL SERVICE\n         backend        *           /stats/prometheus*     \n         backend        *           /healthz/ready*  \n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-and-external-dns/#step-2-deploy-the-application","title":"Step 2: Deploy the Application","text":"<p>Prepare the manifest files for our application as follows:</p> <code>app.yml</code> <code>gateway.yml</code> <code>virtualservice.yml</code> <pre><code>---         \napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test\n  labels:\n    istio-injection: enabled\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deploy\n  namespace: test\n  labels:\n    app: test-app\n    version: v1\nspec: \n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n        version: v1\n    spec: \n      containers:\n      - name: web\n        image: reyanshkharga/nodeapp:v1\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 5000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-svc\n  namespace: test\nspec:   \n  selector:\n    app: test-app\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 5000\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: test-gateway\n  namespace: test\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"test.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: test-virtualservice\n  namespace: test\n  # We need to add this annotation for ExternalDNS to automatically add the record to Route53\n  # The current ExternalDNS implementation for istio doesn't support automatic picking of load balancer dns\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"test.example.com\"\n  gateways:\n  - test-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: test-svc\n        port:\n          number: 80\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- app.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtualservice.yml\n</code></pre> <p>Apply the manifests to deploy the application with istio gateway and virtual service:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>This will create the following kubernetes and Istio resources:</p> <ul> <li>Kubernetes namespace</li> <li>Kubernetes Deployment</li> <li>Kubernetes Service</li> <li>Istio Gateway</li> <li>Istio Virtual Service</li> </ul> <p>Note</p> <p>The current implementation of ExternalDNS for istio is not fully mature. The current ExternalDNS implementation for istio doesn't support automatic picking of load balancer DNS.</p> <p>We need to provide the application load balancer DNS for ExternalDNS to automatically add the record to Route53. We can do so using the <code>external-dns.alpha.kubernetes.io/target</code> annotation.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-and-external-dns/#step-3-view-the-updated-istio-proxy-configuration","title":"Step 3: View the Updated Istio Proxy Configuration","text":"<p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>It should looks something like this:</p> <pre><code>NAME        VHOST NAME            DOMAINS            MATCH                 VIRTUAL SERVICE\nhttp.8080   test.example.com:80   test.example.com   /*                    test-virtualservice.test\n            backend               *                  /stats/prometheus*     \n            backend               *                  /healthz/ready*  \n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-and-external-dns/#step-4-verify-dns-record-in-route-53","title":"Step 4: Verify DNS Record in Route 53","text":"<p>Go to AWS Route 53 and verify if a DNS record that points <code>test.example.com</code> to the load balancer was created.</p> <p>Open any browser and visit <code>test.example.com</code> to verify app.</p> <p>Here's how the traffic flows across Istio and kubernetes resources:</p> <pre><code>graph LR\n  A(User) --&gt; B(\"Application Load Balancer\n  (associated with istio ingress gateway)\");\n  B --&gt; C(\"test-gateway\n  (Istio Gateway)\");\n  C --&gt; D(\"test-virtualservice\n  (Istio Virtual Service)\");\n  D --&gt; E(test-svc);</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-and-external-dns/#clean-up","title":"Clean Up","text":"<p>Delete the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-gateway-and-virtual-service/","title":"Deploy Application With Istio Gateway and Virtual Service","text":"<p>Now that we have a good understanding of traffic management in Istio, let's deploy a simple application using traffic management API resources such as Gateway and Virtual Service.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-gateway-and-virtual-service/#step-1-view-istio-proxy-configuration","title":"Step 1: View Istio Proxy Configuration","text":"<p>Note down the current proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-gateway-and-virtual-service/#step-2-deploy-the-application","title":"Step 2: Deploy the Application","text":"<p>Prepare the manifest files for our application as follows:</p> <code>app.yml</code> <code>gateway.yml</code> <code>virtualservice.yml</code> <pre><code>---         \napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test\n  labels:\n    istio-injection: enabled\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deploy\n  namespace: test\n  labels:\n    app: test-app\n    version: v1\nspec: \n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n        version: v1\n    spec: \n      containers:\n      - name: web\n        image: reyanshkharga/nodeapp:v1\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 5000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-svc\n  namespace: test\nspec:   \n  selector:\n    app: test-app\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 5000\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: test-gateway\n  namespace: test\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"test.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: test-virtualservice\n  namespace: test\nspec: \n  hosts:\n  - \"test.example.com\"\n  gateways:\n  - test-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: test-svc\n        port:\n          number: 80\n</code></pre> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- app.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtualservice.yml\n</code></pre> <p>Apply the manifests to deploy the application with istio gateway and virtual service:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>This will create the following kubernetes and Istio resources:</p> <ul> <li>Kubernetes namespace</li> <li>Kubernetes Deployment</li> <li>Kubernetes Service</li> <li>Istio Gateway</li> <li>Istio Virtual Service</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-gateway-and-virtual-service/#step-3-view-the-updated-istio-proxy-configuration","title":"Step 3: View the Updated Istio Proxy Configuration","text":"<p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-gateway-and-virtual-service/#step-4-add-dns-record-in-route-53","title":"Step 4: Add DNS Record in Route 53","text":"<p>Go to AWS Route 53 and add a record that points <code>test.example.com</code> to the load balancer created by Istio Ingress Gateway.</p> <p>Open any browser and visit <code>test.example.com</code> to verify app.</p> <p>Here's how the traffic flows across Istio and kubernetes resources:</p> <pre><code>graph LR\n  A(User) --&gt; B(\"Application Load Balancer\n  (associated with istio ingress gateway)\");\n  B --&gt; C(\"test-gateway\n  (Istio Gateway)\");\n  C --&gt; D(\"test-virtualservice\n  (Istio Virtual Service)\");\n  D --&gt; E(test-svc);</code></pre> <p>In the next section we will update <code>ExternalDNS</code> to make it work with Istio so that DNS records are automatically created in Route 53 based on hosts provided in Istio virtual services.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-app-with-istio-gateway-and-virtual-service/#clean-up","title":"Clean Up","text":"<p>Delete the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/","title":"Deploy Book Management Microservices With Istio Service Mesh","text":"<p>Now that we understand how services can become part of the Istio service mesh, let's create a book management microservice application using Istio service mesh.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/book-management:book-details</li> <li>reyanshkharga/book-management:book-genres</li> <li>reyanshkharga/book-management:book-web</li> </ul> <p>Note</p> <ol> <li> <p><code>reyanshkharga/book-management:book-genres</code> is a Node.js backend app that uses MongoDB to store and retrieve data, providing a list of books for each genre.</p> <p>Environment Variables:</p> <ul> <li><code>MONGODB_URI</code> (Required)</li> </ul> </li> <li> <p><code>reyanshkharga/book-management:book-details</code> is a Node.js backend app that uses MongoDB to store and retrieve data, providing details for a given book. It also calls the <code>book-genres</code> microservice to return the list of books for a given genre.</p> <p>Environment variables:</p> <ul> <li><code>MONGODB_URI</code> (Required)</li> <li><code>REACT_APP_AGENRES_API_ENDPOINTPI_ENDPOINT</code> (Required)</li> </ul> </li> <li> <p><code>reyanshkharga/book-management:book-web</code> is the frontend for book management application.</p> <p>Environment variables:</p> <ul> <li><code>REACT_APP_API_ENDPOINT</code> (Required)</li> </ul> </li> </ol>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#objective","title":"Objective","text":"<p>We are going to deploy the following microservices on our EKS kubernetes cluster:</p> <ol> <li>Book Details Database microservice</li> <li>Book Genres Database microservice</li> <li>Book Details microservice</li> <li>Book Genres microservice</li> <li>Book Web microservice</li> </ol> <p>The following diagram illustrates the communication between microservices:</p> <pre><code>graph LR\n  A(Book Web) --&gt; B(Book Details);\n  B --&gt; C(Book Genres);\n  B -.-&gt; BD[(\"Book Details \n    Database\")];\n  C -.-&gt; CD[(\"Book Genres \n    Database\")];</code></pre> <p>Note</p> <p>We will use the same load balancer for all microservices because using more load balancers will be expensive since load balancers are charged hourly. We can achieve this using IngressGroup.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#step-1-deploy-book-genres-database-microservice","title":"Step 1: Deploy Book Genres Database Microservice","text":"<p>Let's create the kubernetes objects for our Book Genres Database microservice as follows:</p> <code>00-namespace.yml</code> <code>storageclass.yml</code> <code>pvc.yml</code> <code>configmap.yml</code> <code>deployment-and-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-genres-db\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: book-genres-db-storageclass\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  tagSpecification_1: \"Name=eks-book-genres-db-storage\"\n  tagSpecification_2: \"CreatedBy=aws-ebs-csi-driver\"\nreclaimPolicy: Delete\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: book-genres-db-pvc\n  namespace: book-genres-db\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: book-genres-db-storageclass\n  resources:\n    requests:\n      storage: 4Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: book-genres-mongo-initdb\n  namespace: book-genres-db\ndata:\n  mongo-init.sh: |-\n    mongo &lt;&lt;EOF\n    use books\n    db.genres.insert({_id: 1, \"books\": [1,2,3,4]})\n    db.genres.insert({_id: 2, \"books\": [5,6,7]})\n    db.genres.insert({_id: 3, \"books\": [8]})\n    EOF\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-genres-db-deployment\n  namespace: book-genres-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-genres-db\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-genres-db\n        version: v1\n    spec:\n      containers:\n      - name: book-genres-db\n        image: mongo:5.0.2\n        ports:\n          - containerPort: 27017\n        volumeMounts:\n        - name: book-genres-db-storage\n          mountPath: /data/db\n        - name: mongo-initdb\n          mountPath: /docker-entrypoint-initdb.d\n      volumes:\n      - name: book-genres-db-storage\n        persistentVolumeClaim:\n          claimName: book-genres-db-pvc\n      - name: mongo-initdb\n        configMap:\n          name: book-genres-mongo-initdb\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-genres-db-service\n  namespace: book-genres-db\nspec:\n  type: ClusterIP\n  selector:\n    app: book-genres-db\n    version: v1\n  ports:\n    - port: 27017\n      targetPort: 27017\n      name: tcp\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-genres-db\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- configmap.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Genres Database microservice:</p> <pre><code>kubectl apply -f book-genres-db/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-genres-db</code></li> <li>A <code>StorageClass</code> (SC) for dynamic provisioning of persistent volume</li> <li>A <code>PersistentVolumeClaim</code> (PVC) in the <code>book-genres-db</code> namespace</li> <li>MongoDB deployment in the <code>book-genres-db</code> namespace</li> <li>MongoDB service in the <code>book-genres-db</code> namespace</li> </ol> <p>We are using Amazon EBS to persist the MongoDB data. EBS is provisioned dynamically using AWS EBS-CSI driver.</p> <p>With persistent volume even if the MongoDB pod goes down the data will remain intact. When the new pod comes up we'll have the access to the same data.</p> <p>We are also using configmap to populate data in the MongoDB database.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-genres-db namespace\nkubectl get all -n book-genres-db\n\n# List StorageClass\nkubectl get sc\n\n# List PersistentVolume\nkubectl get pv\n\n# List PersistenvVolumeClaim\nkubectl get pvc -n book-genres-db\n</code></pre> <p>Verify if MongoDB is working as expected:</p> <pre><code># Start a shell session inside the book-genres-db container\nkubectl exec -it &lt;mongodb-pod-name&gt; -n book-genres-db -- bash\n\n# Start the mongo Shell to interact with MongoDB\nmongo\n\n# List Databases\nshow dbs\n\n# Switch to a Database\nuse &lt;db-name&gt;\n\n# List collections\nshow collections\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#step-2-deploy-book-details-database-microservice","title":"Step 2: Deploy Book Details Database Microservice","text":"<p>Let's create the kubernetes objects for our Book Details Database microservice as follows:</p> <code>00-namespace.yml</code> <code>storageclass.yml</code> <code>pvc.yml</code> <code>configmap.yml</code> <code>deployment-and-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-details-db\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: book-details-db-storageclass\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  tagSpecification_1: \"Name=eks-book-details-db-storage\"\n  tagSpecification_2: \"CreatedBy=aws-ebs-csi-driver\"\nreclaimPolicy: Delete\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: book-details-db-pvc\n  namespace: book-details-db\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: book-details-db-storageclass\n  resources:\n    requests:\n      storage: 4Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: book-details-mongo-initdb\n  namespace: book-details-db\ndata:\n  mongo-init.sh: |-\n    mongo &lt;&lt;EOF\n    use books\n    db.details.insert({_id: 1, \"title\": \"Book1\", \"copies_sold\": 100})\n    db.details.insert({_id: 2, \"title\": \"Book2\", \"copies_sold\": 200})\n    db.details.insert({_id: 3, \"title\": \"Book3\", \"copies_sold\": 300})\n    db.details.insert({_id: 4, \"title\": \"Book4\", \"copies_sold\": 400})\n    db.details.insert({_id: 5, \"title\": \"Book5\", \"copies_sold\": 500})\n    db.details.insert({_id: 6, \"title\": \"Book6\", \"copies_sold\": 600})\n    db.details.insert({_id: 7, \"title\": \"Book7\", \"copies_sold\": 700})\n    db.details.insert({_id: 8, \"title\": \"Book8\", \"copies_sold\": 800})\n    EOF\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-details-db-deployment\n  namespace: book-details-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-details-db\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-details-db\n        version: v1\n    spec:\n      containers:\n      - name: book-details-db\n        image: mongo:5.0.2\n        ports:\n          - containerPort: 27017\n        volumeMounts:\n        - name: book-details-db-storage\n          mountPath: /data/db\n        - name: mongo-initdb\n          mountPath: /docker-entrypoint-initdb.d\n      volumes:\n      - name: book-details-db-storage\n        persistentVolumeClaim:\n          claimName: book-details-db-pvc\n      - name: mongo-initdb\n        configMap:\n          name: book-details-mongo-initdb\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-details-db-service\n  namespace: book-details-db\nspec:\n  type: ClusterIP\n  selector:\n    app: book-details-db\n    version: v1\n  ports:\n    - port: 27017\n      targetPort: 27017\n      name: tcp\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-details-db\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- configmap.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Details Database microservice:</p> <pre><code>kubectl apply -f book-details-db/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-details-db</code></li> <li>A <code>StorageClass</code> (SC) for dynamic provisioning of persistent volume</li> <li>A <code>PersistentVolumeClaim</code> (PVC) in the <code>book-details-db</code> namespace</li> <li>MongoDB deployment in the <code>book-details-db</code> namespace</li> <li>MongoDB service in the <code>book-details-db</code> namespace</li> </ol> <p>We are using Amazon EBS to persist the MongoDB data. EBS is provisioned dynamically using AWS EBS-CSI driver.</p> <p>With persistent volume even if the MongoDB pod goes down the data will remain intact. When the new pod comes up we'll have the access to the same data.</p> <p>We are also using configmap to populate data in the MongoDB database.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-details-db namespace\nkubectl get all -n book-details-db\n\n# List StorageClass\nkubectl get sc\n\n# List PersistentVolume\nkubectl get pv\n\n# List PersistenvVolumeClaim\nkubectl get pvc -n book-details-db\n</code></pre> <p>Verify if MongoDB is working as expected:</p> <pre><code># Start a shell session inside the book-details-db container\nkubectl exec -it &lt;mongodb-pod-name&gt; -n book-details-db -- bash\n\n# Start the mongo Shell to interact with MongoDB\nmongo\n\n# List Databases\nshow dbs\n\n# Switch to a Database\nuse &lt;db-name&gt;\n\n# List collections\nshow collections\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#step-3-deploy-book-genres-microservice","title":"Step 3: Deploy Book Genres Microservice","text":"<p>Let's create the kubernetes objects for our Book Genres microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>ingress.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-genres\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-genres-deployment\n  namespace: book-genres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-genres\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-genres\n        version: v1\n    spec:\n      containers:\n      - name: book-genres\n        image: reyanshkharga/book-management:book-genres\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        env:\n        - name: MONGODB_URI\n          value: mongodb://book-genres-db-service.book-genres-db.svc.cluster.local:27017\n        - name: BUGGY_CODE\n          value: \"false\"\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-genres-service\n  namespace: book-genres\nspec:\n  type: ClusterIP\n  selector:\n    app: book-genres\n    version: v1\n  ports:\n    - port: 80\n      targetPort: 5000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: book-genres-ingress\n  namespace: book-genres\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: book-genres.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: book-genres-service\n            port:\n              number: 80\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-genres\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Genres microservice:</p> <pre><code>kubectl apply -f book-genres/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-genres</code></li> <li>Book Genres deployment in the <code>book-genres</code> namespace</li> <li>Book Genres service in the <code>book-genres</code> namespace</li> <li>Ingress for Book Genres service</li> </ol> <p>The ingress creates an internet-facing load balancer and the SSL certificate is attached to the load balancer.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-genres namespace\nkubectl get all -n book-genres\n\n# List ingress in book-genres namespace\nkubectl get ing -n book-genres\n</code></pre> <p>Go to AWS console and verify if the load balancer was created and a record was added to Route 53 for the host specified in ingress.</p> <p>Open any browser on your local host machine and hit the URL to access the book genres service:</p> <pre><code>https://book-genres.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#step-4-deploy-book-details-microservice","title":"Step 4: Deploy Book Details Microservice","text":"<p>Let's create the kubernetes objects for our Book Details microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>ingress.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-details\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-details-deployment\n  namespace: book-details\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-details\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-details\n        version: v1\n    spec:\n      containers:\n      - name: book-details\n        image: reyanshkharga/book-management:book-details\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        env:\n        - name: MONGODB_URI\n          value: mongodb://book-details-db-service.book-details-db.svc.cluster.local:27017\n        - name: BUGGY_CODE\n          value: \"false\"\n        - name: GENRES_API_ENDPOINT\n          value: http://book-genres-service.book-genres.svc.cluster.local\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-details-service\n  namespace: book-details\nspec:\n  type: ClusterIP\n  selector:\n    app: book-details\n    version: v1\n  ports:\n    - port: 80\n      targetPort: 5000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: book-details-ingress\n  namespace: book-details\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: book-details.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: book-details-service\n            port:\n              number: 80\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-details\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Details microservice:</p> <pre><code>kubectl apply -f book-details/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-details</code></li> <li>Book Details deployment in the <code>book-details</code> namespace</li> <li>Book Details service in the <code>book-details</code> namespace</li> <li>Ingress for Book Details service</li> </ol> <p>The ingress creates an internet-facing load balancer and the SSL certificate is attached to the load balancer.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-details namespace\nkubectl get all -n book-details\n\n# List ingress in book-details namespace\nkubectl get ing -n book-details\n</code></pre> <p>Go to AWS console and verify if the existing load balancer was used and a record was added to Route 53 for the host specified in ingress.</p> <p>Open any browser on your local host machine and hit the URL to access the book details service:</p> <pre><code>https://book-details.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-book-management-microservices-with-service-mesh/#step-5-deploy-book-web-microservice","title":"Step 5: Deploy Book Web Microservice","text":"<p>Let's create the kubernetes objects for our Book Web microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>ingress.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-web\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-web-deployment\n  namespace: book-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-web\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-web\n        version: v1\n    spec:\n      containers:\n      - name: book-web\n        image: reyanshkharga/book-management:book-web\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n        env:\n        - name: REACT_APP_API_ENDPOINT\n          value: https://book-details.example.com\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-web-service\n  namespace: book-web\nspec:\n  type: ClusterIP\n  selector:\n    app: book-web\n    version: v1\n  ports:\n    - port: 80\n      targetPort: 3000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: book-web-ingress\n  namespace: book-web\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: book-web.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: book-web-service\n            port:\n              number: 80\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-web\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- ingress.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Web microservice:</p> <pre><code>kubectl apply -f book-web/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-web</code></li> <li>Book Web deployment in the <code>book-web</code> namespace</li> <li>Book Web service in the <code>book-web</code> namespace</li> <li>Ingress for Book Web service</li> </ol> <p>The ingress creates an internet-facing load balancer and the SSL certificate is attached to the load balancer.</p> <p>Note that the certificate is automatically discovered with hostnames from the ingress resource. Also, a Route 53 record is added for the host. This is all done by the AWS Load Balancer Controller and ExternalDNS.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-web namespace\nkubectl get all -n book-web\n\n# List ingress in book-web namespace\nkubectl get ing -n book-web\n</code></pre> <p>Go to AWS console and verify if the existing load balancer was used and a record was added to Route 53 for the host specified in ingress.</p> <p>Open any browser on your local host machine and hit the URL to access the book web service:</p> <pre><code>https://book-web.example.com\n</code></pre> <p> </p> <p>Verify if everything is properly and you can interact with book web frontend service and get the book and genre details.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-grafana/","title":"Deploy Istio Gateway for Grafana","text":"<p>Recall that we deleted the ingress we created for Grafana. Now, let's enable external access for Grafana using the Istio Gateway.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-grafana/#step-1-view-istio-proxy-configuration","title":"Step 1: View Istio Proxy Configuration","text":"<p>Note down the current proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-grafana/#step-2-deploy-istio-gateway-for-grafana","title":"Step 2: Deploy Istio Gateway for Grafana","text":"<p>Prepare istio manifests for Grafana as follows:</p> <code>grafna-gateway.yml</code> <code>grafna-virtualservice.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: grafana-gateway\n  namespace: grafana\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"grafana.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: grafana-virtualservice\n  namespace: grafana\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"grafana.example.com\"\n  gateways:\n  - grafana-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: grafana\n        port:\n          number: 80\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- grafna-gateway.yml\n\u2502   |-- grafna-virtualservice.yml\n</code></pre> <p>Apply the manifests to deploy the istio gateway for grafna:</p> <pre><code>kubectl apply -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-grafana/#step-3-view-the-updated-istio-proxy-configuration","title":"Step 3: View the Updated Istio Proxy Configuration","text":"<p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-grafana/#step-4-verify-dns-record-in-route-53","title":"Step 4: Verify DNS Record in Route 53","text":"<p>Go to AWS Route 53 and verify whether a DNS record was created for the host we defined in the virtual service, pointing to the Istio load balancer.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-grafana/#step-5-access-grafana","title":"Step 5: Access Grafana","text":"<p>Open any browser on your local host machine and hit the Grafana host URL to access grafna:</p> <pre><code>https://grafna.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-jaegar/","title":"Deploy Istio Gateway for Jaegar","text":"<p>Recall that we deleted the ingress we created for Jaegar. Now, let's enable external access for Jaegar using the Istio Gateway.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-jaegar/#step-1-view-istio-proxy-configuration","title":"Step 1: View Istio Proxy Configuration","text":"<p>Note down the current proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-jaegar/#step-2-deploy-istio-gateway-for-jaegar","title":"Step 2: Deploy Istio Gateway for Jaegar","text":"<p>Prepare istio manifests for Jaegar as follows:</p> <code>jaegar-gateway.yml</code> <code>jaegar-virtualservice.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: jaegar-gateway\n  namespace: istio-system\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"jaegar.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: jaegar-virtualservice\n  namespace: istio-system\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"jaegar.example.com\"\n  gateways:\n  - jaegar-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: tracing\n        port:\n          number: 80\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- jaegar-gateway.yml\n\u2502   |-- jaegar-virtualservice.yml\n</code></pre> <p>Apply the manifests to deploy the istio gateway for jaegar:</p> <pre><code>kubectl apply -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-jaegar/#step-3-view-the-updated-istio-proxy-configuration","title":"Step 3: View the Updated Istio Proxy Configuration","text":"<p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-jaegar/#step-4-verify-dns-record-in-route-53","title":"Step 4: Verify DNS Record in Route 53","text":"<p>Go to AWS Route 53 and verify whether a DNS record was created for the host we defined in the virtual service, pointing to the Istio load balancer.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-jaegar/#step-5-access-jaegar","title":"Step 5: Access Jaegar","text":"<p>Open any browser on your local host machine and hit the Jaegar host URL to access jaegar:</p> <pre><code>https://jaegar.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-kiali/","title":"Deploy Istio Gateway for Kiali","text":"<p>Recall that we deleted the ingress we created for Kiali. Now, let's enable external access for Kiali using the Istio Gateway.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-kiali/#step-1-view-istio-proxy-configuration","title":"Step 1: View Istio Proxy Configuration","text":"<p>Note down the current proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-kiali/#step-2-deploy-istio-gateway-for-kiali","title":"Step 2: Deploy Istio Gateway for Kiali","text":"<p>Prepare istio manifests for Kiali as follows:</p> <code>kiali-gateway.yml</code> <code>kiali-virtualservice.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: kiali-gateway\n  namespace: istio-system\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"kiali.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: kiali-virtualservice\n  namespace: istio-system\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"kiali.example.com\"\n  gateways:\n  - kiali-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: kiali\n        port:\n          number: 20001\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- kiali-gateway.yml\n\u2502   |-- kiali-virtualservice.yml\n</code></pre> <p>Apply the manifests to deploy the istio gateway for kiali:</p> <pre><code>kubectl apply -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-kiali/#step-3-view-the-updated-istio-proxy-configuration","title":"Step 3: View the Updated Istio Proxy Configuration","text":"<p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-kiali/#step-4-verify-dns-record-in-route-53","title":"Step 4: Verify DNS Record in Route 53","text":"<p>Go to AWS Route 53 and verify whether a DNS record was created for the host we defined in the virtual service, pointing to the Istio load balancer.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-kiali/#step-5-access-kiali","title":"Step 5: Access Kiali","text":"<p>Open any browser on your local host machine and hit the Kiali host URL to access kiali:</p> <pre><code>https://kiali.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-prometheus/","title":"Deploy Istio Gateway for Prometheus","text":"<p>Recall that we deleted the ingress we created for Prometheus. Now, let's enable external access for Prometheus using the Istio Gateway.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-prometheus/#step-1-view-istio-proxy-configuration","title":"Step 1: View Istio Proxy Configuration","text":"<p>Note down the current proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-prometheus/#step-2-deploy-istio-gateway-for-prometheus","title":"Step 2: Deploy Istio Gateway for Prometheus","text":"<p>Prepare istio manifests for Prometheus as follows:</p> <code>prometheus-gateway.yml</code> <code>prometheus-virtualservice.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: prometheus-gateway\n  namespace: prometheus\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"prometheus.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: prometheus-virtualservice\n  namespace: prometheus\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"prometheus.example.com\"\n  gateways:\n  - prometheus-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: prometheus-server\n        port:\n          number: 80\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- prometheus-gateway.yml\n\u2502   |-- prometheus-virtualservice.yml\n</code></pre> <p>Apply the manifests to deploy the istio gateway for prometheus:</p> <pre><code>kubectl apply -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-prometheus/#step-3-view-the-updated-istio-proxy-configuration","title":"Step 3: View the Updated Istio Proxy Configuration","text":"<p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-prometheus/#step-4-verify-dns-record-in-route-53","title":"Step 4: Verify DNS Record in Route 53","text":"<p>Go to AWS Route 53 and verify whether a DNS record was created for the host we defined in the virtual service, pointing to the Istio load balancer.</p>"},{"location":"kubernetes-on-eks/service-mesh/deploy-istio-gateway-for-prometheus/#step-5-access-prometheus","title":"Step 5: Access Prometheus","text":"<p>Open any browser on your local host machine and hit the Prometheus host URL to access prometheus:</p> <pre><code>https://prometheus.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-istio-with-application-load-balancer/","title":"Install Istio With Application Load Balancer","text":"<p>By default, Istio creates an AWS Classic Load Balancer during installation. However, we'll be setting up Istio to utilize the AWS Application Load Balancer due to its superior features compared to the Classic Load Balancer.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-istio-with-application-load-balancer/#step-1-install-istio-with-nodeport-service-type","title":"Step 1: Install Istio with NodePort Service Type","text":"<p>By default, Istio creates an AWS Classic Load Balancer during installation because the <code>istio-ingressgateway</code> service type is set to <code>LoadBalancer</code>.</p> <p>We'll change the service type to <code>NodePort</code>, so no load balancer is created at first. Then, we'll create an ingress object for the <code>istio-ingressgateway</code> service, which will create an AWS Application Load Balancer using the AWS Load Balancer Controller.</p> <p>Install Istio with service type set to NodePort:</p> <pre><code># Install istio\nistioctl install --set profile=default --set values.gateways.istio-ingressgateway.type=NodePort -y\n</code></pre> <p>The output should look similar to the below:</p> <pre><code>\u2714 Istio core installed                                                            \n\u2714 Istiod installed                                                                \n\u2714 Ingress gateways installed                                                      \n\u2714 Installation complete\n</code></pre> <p>Verify the service type of <code>istio-ingressgateway</code> service:</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-istio-with-application-load-balancer/#step-2-configure-istio-ingressgateway-service-to-use-application-load-balancer","title":"Step 2: Configure istio-ingressgateway Service to Use Application Load Balancer","text":"<ol> <li> <p>Note down the <code>nodePort</code> value of istio-ingressgateway service:</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system -o yaml\n</code></pre> <p>Note down the value of <code>nodePort</code> from <code>.spec.ports</code> that corresponds to <code>status-port</code>.</p> <pre><code>...\nspec:\n  ...\n  ports:\n  - name: status-port\n    nodePort: 30594 # Note down this value\n    port: 15021\n    protocol: TCP\n    targetPort: 15021\n...\n</code></pre> </li> <li> <p>Note down the health check path of istio-ingressgateway service:</p> <pre><code>kubectl get deploy/istio-ingressgateway -n istio-system -o yaml\n</code></pre> <p>Note down the health check path for the <code>readinessProbe</code>.</p> <pre><code>...\nreadinessProbe:\n    failureThreshold: 30\n    httpGet:\n    path: /healthz/ready # Note down this value\n    port: 15021\n    scheme: HTTP\n...\n</code></pre> </li> <li> <p>Edit the istio-ingressgateway service to add alb annotations:</p> <p>Edit the <code>istio-ingressgateway</code> service by adding annotations that aws application load balancer controller can use to configure health check for this target.</p> <p>Note</p> <p>The default kubectl editor is vim. You can change it to nano as follows:</p> <pre><code># Configure kubectl to use nano editor\nexport KUBE_EDITOR=nano\n</code></pre> <p>Use <code>kubectl edit</code> command to edit the <code>istio-ingressgateway</code> service:</p> <pre><code>kubectl edit svc istio-ingressgateway -n istio-system\n</code></pre> <p>Edit the service by adding the following annotations in <code>.metadata.annotations</code>:</p> <pre><code>alb.ingress.kubernetes.io/healthcheck-port: \"30594\"\nalb.ingress.kubernetes.io/healthcheck-path: /healthz/ready\n</code></pre> <p>Make sure to change the <code>healthcheck-port</code> value to the <code>nodePort</code> value you noted earlier. The same goes for <code>healthcheck-path</code>. Make sure to change it to the health check path you recorded.</p> </li> </ol> <p>Now, describe the service to check if everything is fine:</p> <pre><code>kubectl describe svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-istio-with-application-load-balancer/#step-3-create-ingress-for-the-istio-ingressgateway-service","title":"Step 3: Create Ingress for the istio-ingressgateway Service","text":"<p>Now, let's create and deploy ingress for the <code>istio-ingressgateway</code> service which in turn will create an application load balancer that sends traffic to <code>istio-ingressgateway</code> service.</p> <code>istio-ingressgateway-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: istio-ingressgateway\n  namespace: istio-system\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: istio-load-balancer\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # SSL Certificate Annotation\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:170476043077:certificate/2d88e035-cde7-472a-9cd3-6b6ce6ece961\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: istio-ingressgateway\n            port:\n              number: 80\n</code></pre> <p>Make sure to replace the <code>certificate-arn</code> with the arn of certificate you created in ACM.</p> <p>Apply the manifest to create ingress:</p> <pre><code># Create ingress\nkubectl apply -f istio-ingressgateway-ingress.yml\n</code></pre> <p>List ingress resources:</p> <pre><code># List ingress\nkubectl get ing -n istio-system\n</code></pre> <p>Verify that a target group and an application load balancer was created. Also, verify that the targets are in healthy state.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-istio/","title":"Install Istio in EKS Kubernetes Cluster","text":"<p>Now that we know what Istio is and how it works, let's install it in our EKS kubernetes cluster. Later on, we'll explore its features more deeply in the following sections.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-istio/#step-1-download-istio","title":"Step 1: Download Istio","text":"<ol> <li> <p>Download installation file:</p> <pre><code>curl -L https://istio.io/downloadIstio | sh -\n</code></pre> <p>Tip</p> <p>The command above downloads the latest release (numerically) of Istio. You can pass variables on the command line to download a specific version or to override the processor architecture. For example, to download Istio 1.20.0 for the x86_64 architecture, run:</p> <pre><code>curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.0 TARGET_ARCH=x86_64 sh -\n</code></pre> </li> <li> <p>Change directory to the Istio package directory. For example, if the package is <code>istio-1.20.0</code>:</p> <pre><code>cd istio-1.20.0\n</code></pre> <p>Verify that the directory contains <code>bin/</code> folder. This contains the <code>istioctl</code> client binary.</p> <p>Note</p> <p><code>istioctl</code> is a command-line utility used to configure and manage Istio service mesh within kubernetes environments.</p> </li> <li> <p>Add <code>istioctl</code> client to your path:</p> <pre><code># Add to path temporarily\nexport PATH=$PWD/bin:$PATH\n\n{OR}\n\n# Add to path permanently\nsudo cp bin/istioctl /usr/bin/\n</code></pre> </li> <li> <p>Verfy <code>istioctl</code> version:</p> <pre><code>istioctl version --remote=false\n</code></pre> <p>If everything is fine it should print out the <code>istioctl</code> version.</p> </li> </ol>"},{"location":"kubernetes-on-eks/service-mesh/install-istio/#step-2-install-istio","title":"Step 2: Install Istio","text":"<ol> <li> <p>Check if your EKS cluster is ready to install Istio:</p> <pre><code>istioctl x precheck\n</code></pre> <p>If your EKS cluster passes the pre-installation verification check you should see the following output:</p> <pre><code>\u2714 No issues found when checking the cluster. Istio is safe to install or upgrade!\nTo get started, check out https://istio.io/latest/docs/setup/getting-started/\n</code></pre> </li> <li> <p>List available istio configuration profiles:</p> <p>Istio configuration profiles are predefined sets of configuration settings that allow users to easily customize and fine-tune Istio's behavior within their service mesh.</p> <pre><code># List configuration profiles\nistioctl profile list\n</code></pre> <p>This lists all the configuration profiles that can be used while installing istio.</p> <p>In this course, we will use the <code>default</code> profile, which is recommended for production deployment.</p> </li> <li> <p>Install Istio:</p> <p>Let's install istio with configuration from <code>default</code> profile.</p> <pre><code>istioctl install --set profile=default -y\n</code></pre> <p>If istio is installed successfully, you should see the output similar to below:</p> <pre><code>\u2714 Istio core installed\n\u2714 Istiod installed\n\u2714 Egress gateways installed\n\u2714 Ingress gateways installed\n\u2714 Installation complete\n</code></pre> </li> <li> <p>Verify Istio installation:</p> <p>Istio components are deployed in <code>istio-system</code> namespace. Let's list pods in <code>istio-system</code> namespace.</p> <pre><code>kubectl get pods -n istio-system\n</code></pre> <p>Verify if all the pods are running.</p> <p>Note</p> <p><code>Istiod</code> makes up the control plane and is responsible for injecting the sidecar proxies into our services that are part of the mesh.</p> </li> </ol>"},{"location":"kubernetes-on-eks/service-mesh/install-istio/#uninstall-istio","title":"Uninstall Istio","text":"<p>If you no longer need Istio in your EKS kubernetes cluster, you can uninstall it using following command:</p> <pre><code># Uninstall istio\nistioctl uninstall --purge\n</code></pre> <p>This will remove all istio components from your kubernetes cluster.</p> <p>References:</p> <ul> <li>Download Istio</li> <li>Istio Configuration Profiles</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/install-jaegar/","title":"Install Jaegar","text":"<p>Jaeger is an open source end to end distributed tracing system, allowing users to monitor and troubleshoot transactions in complex distributed systems.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-jaegar/#step-1-prepare-yaml-manifest-for-jaegar","title":"Step 1: Prepare YAML Manifest for Jaegar","text":"<p>Copy <code>samples/addons/jaegar.yml</code> manifest from Istio download package.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-jaegar/#step-2-deploy-the-manifest-to-install-jaegar","title":"Step 2: Deploy the Manifest to Install Jaegar","text":"<p>Let's apply the modified manifest to install Jaegar:</p> <pre><code>kubectl apply -f jaegar.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-jaegar/#step-3-access-jaegar-dashboard-locally","title":"Step 3: Access Jaegar Dashboard Locally","text":"<p>Let's use <code>kubectl port-forward</code> to access Jaegar dashboard locally:</p> <pre><code># Check the port jaegar is exposed at\nkubectl get svc -n istio-system | grep tracing\n\n# Forward port 80 of jaegar service on port 20002 of the local host machine\nkubectl port-forward svc/tracing 20002:80 -n istio-system\n</code></pre> <p>Open any browser on your local host machine and visit <code>localhost:20002</code>. You should see the Jaegar dashboard.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-jaegar/#step-4-deploy-ingress-for-jaegar","title":"Step 4: Deploy Ingress for Jaegar","text":"<p>We'll use an ingress to access Jaegar externally using a load balancer.</p> <code>jaegar-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: jaegar-ingress\n  namespace: istio-system\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: jaegar.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: tracing\n            port:\n              number: 80\n</code></pre> <p>Apply the manifest to create ingress for Jaegar:</p> <pre><code>kubectl apply -f jaegar-ingress.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-jaegar/#step-5-verify-jaegar-installation","title":"Step 5: Verify Jaegar Installation","text":"<p>Visit the Jaegar host (<code>jaegar.example.com</code>) to verify whether you can access the Jaegar Dashboard and view traces.</p> <p> </p> <p>References:</p> <ul> <li>Jaegar</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/install-kiali/","title":"Install Kiali","text":"<p>Kiali is an observability console for Istio with service mesh configuration and validation capabilities. It helps you understand the structure and health of your service mesh by monitoring traffic flow to infer the topology and report errors.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-kiali/#step-1-prepare-yaml-manifest-for-kiali","title":"Step 1: Prepare YAML Manifest for Kiali","text":"<p>Copy <code>samples/addons/kiali.yml</code> manifest from Istio download package. We will make some modification to this file before deploying.</p> <p>Edit the <code>kiali.yml</code> by adding grafana and prometheus url in the kiali configmap. It should look like the below:</p> <pre><code>external_services:\n  ...\n  grafana:\n    enabled: true\n    in_cluster_url: http://grafana.grafana:80\n    url: https://grafana.example.com\n  prometheus:\n    url: http://prometheus-server.prometheus:80\n  jaegar:\n    url: http://tracing.istio-system:80\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-kiali/#step-2-deploy-the-manifest-to-install-kiali","title":"Step 2: Deploy the Manifest to Install Kiali","text":"<p>Let's apply the modified manifest to install Kiali:</p> <pre><code>kubectl apply -f kiali.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-kiali/#step-3-access-kiali-dashboard-locally","title":"Step 3: Access Kiali Dashboard Locally","text":"<p>Let's use <code>kubectl port-forward</code> to access Kiali dashboard locally:</p> <pre><code># Check the port kiali is exposed at\nkubectl get svc -n istio-system | grep kiali\n\n# Forward port 20001 of kiali service on port 20001 of the local host machine\nkubectl port-forward svc/kiali 20001:20001 -n istio-system\n</code></pre> <p>Open any browser on your local host machine and visit <code>localhost:20001</code>. You should see the Kiali dashboard.</p>"},{"location":"kubernetes-on-eks/service-mesh/install-kiali/#step-4-deploy-ingress-for-kiali","title":"Step 4: Deploy Ingress for Kiali","text":"<p>We'll use an ingress to access Kiali externally using a load balancer.</p> <code>kiali-ingress.yml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kiali-ingress\n  namespace: istio-system\n  annotations:\n    # Load Balancer Annotations\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/load-balancer-name: my-load-balancer\n    alb.ingress.kubernetes.io/target-type: ip\n    # Health Check Annotations\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\n    alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n    alb.ingress.kubernetes.io/healthcheck-path: /\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '5'\n    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '2'\n    alb.ingress.kubernetes.io/success-codes: '200'\n    alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\n    # Listerner Ports Annotation\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\n    # SSL Redicrect Annotation\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    # IngressGroup\n    alb.ingress.kubernetes.io/group.name: my-group\nspec:\n  ingressClassName: alb\n  rules:\n  - host: kiali.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kiali\n            port:\n              number: 20001\n</code></pre> <p>Apply the manifest to create ingress for Kiali:</p> <pre><code>kubectl apply -f kiali-ingress.yml\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/install-kiali/#step-5-verify-kiali-installation","title":"Step 5: Verify Kiali Installation","text":"<p>Visit the Kiali host (<code>kiali.example.com</code>) to verify whether you can access the Kiali Dashboard and observe the interaction between microservices.</p> <code>Kiali Overview</code> <code>Kiali Graph</code> <code>Kiali Applications</code> <code>Kiali Services</code> <p><p> </p></p> <p><p> </p></p> <p><p> </p></p> <p><p> </p></p> <p>At this moment, the Kiali graph might seem unclear because we're currently utilizing kubernetes ingress for our microservices instead of the native gateway and virtual service in Istio. Once we update our microservices to use the Istio gateway and virtual service, you'll see a more accurate graph illustrating the interactions between our microservices.</p> <p>Here's a glimpse of how kiali service graph will look like when we update our microservices:</p> <p> </p> <p>Notice how <code>book-details-service</code> calls <code>book-genres-service</code> and both services independently calls their respective databases.</p> <p>You won't see <code>book-web-service</code> calling <code>book-details-service</code> in the graph because <code>book-web-service</code> uses the public API to call <code>book-details-service</code>.</p> <p>References:</p> <ul> <li>Kiali</li> <li>Kiali - Modify external_services ConfigMap</li> <li>Kiali - Prometheus Configuration</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/","title":"Introduction to Istio","text":"<p>Istio is an implementation of service mesh for observability, security in depth, and management that speeds deployment cycles.</p> <p>It is an open source service mesh that layers transparently onto existing distributed applications.</p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#istio-architecture","title":"Istio Architecture","text":"<p>Istio is logically splitted into a <code>control plane</code> and a <code>data plane</code>.</p> <p> </p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#control-plane","title":"Control Plane","text":"<p>The control plane is the brain of the main network that manages, controls, and supervises the network of microservies.</p> <p>The control plane manages and configures the proxies to route traffic. Additionally, the control plane configures <code>Mixers</code> to enforce policies and collect telemetry.</p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#data-plane","title":"Data Plane","text":"<p>The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars.</p> <p>These proxies mediate and control all network communication between microservices along with <code>Mixer</code>, a general-purpose policy and telemetry hub.</p> <p>The sidecars deployed within the services and acting as proxy form the service mesh network.</p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#building-blocks-of-istio","title":"Building Blocks of Istio","text":"Component Description Group <code>Pilot</code> Responsible for service discovery and configuring envoy sidecar proxies Control plane <code>Galley</code> Configuration ingestion for Istio components Control plane <code>Sidecar injector</code> Inside envoy sidecar for enabled namespaces Control plane <code>Citadel</code> Automated key and certificate management Control plane <code>Policy</code> Policy enforcement Control plane <code>Telemetry</code> Gather telemetry data Control plane <code>Ingresss Gateway</code> Manage inbound connection to the service mesh Control plane <code>Egress Gateway</code> Manage outbound connection from the service mesh Control plane <code>Istio CNI</code> Network initialisation Control plane <code>Prometheus</code> Metrics collections Control plane <code>Core DNS</code> DNS resolution in a multicluster gateways deployment Control plane <code>Cert Manager</code> Issuance and renewal of TLS certificates Control plane <code>Grafana</code> Grafana    Monitoring dashboard Dashboard <code>Jaeger</code> Distributed tracing Dashboard <code>Kiali</code> Observability dashboard Dashboard <code>Envoy proxy</code> Proxy injected as a sidecar Data plane <ul> <li> <p>The <code>ingress controller</code> is responsible for allowing and redirecting the inbound traffic to the services running inside the service mesh.</p> </li> <li> <p>The <code>egress controller</code> is responsible for allowing outbound traffic from the service mesh. If an application should connect, for example, to an external database or service, such configuration should be explicitly defined for the egress controller.</p> </li> <li> <p><code>Pilot</code> and <code>Galley</code> are responsible for the mesh configuration. They pull data from Kubernetes API Server and mix it with the local configuration defined within the mesh then push the configuration to different proxies forming the mesh.</p> </li> <li> <p><code>Citadel</code> push tls certificate to services enabling mutual TLS.</p> </li> <li> <p><code>Mixer</code> has two roles: gather metrics from the different components and enforce policy by double checking each request. In a high level deployment scenario Telemetry and Policy check should be deployed separately.</p> </li> <li> <p><code>Dashboards</code> gather metrics from the telemetry service and display it in a user friendly format.</p> </li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#upstream-vs-downstream","title":"Upstream vs Downstream","text":"<p><code>Upstream</code> connections are the service Envoy is initiating the connection to, while <code>Downstream</code> connections are the client that is initiating a request through Envoy.</p> <p> </p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#what-is-envoy","title":"What is Envoy?","text":"<p>Envoy is a high-performance, open-source proxy designed for cloud-native applications. It's built to be a universal data plane and is commonly used in Service Mesh architectures. Envoy acts as an intermediary for all network communication between microservices within a system.</p> <p>It's highly configurable and excels at handling tasks like load balancing, routing, authentication, monitoring, and more. Envoy is known for its reliability, observability, and extensive features that help manage and secure communication between services in complex, distributed systems.</p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-istio/#how-does-istio-work","title":"How Does Istio Work?","text":"<p>Istio uses a proxy to intercept all your network traffic, allowing a broad set of application-aware features based on configuration you set.</p> <p>An Envoy proxy is deployed along with each service that you start in your cluster, or runs alongside services running on VMs.</p> <p>The control plane takes your desired configuration, and its view of the services, and dynamically programs the proxy servers, updating them as the rules or the environment changes.</p> <p> </p> <p>References:</p> <ul> <li>What is Istio?</li> <li>Istio Architecture</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-service-mesh/","title":"Introduction to Service Mesh","text":"<p>A service mesh is a dedicated infrastructure layer that you can add to your applications. It allows you to transparently add capabilities like observability, traffic management, and security, without adding them to your own code.</p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-service-mesh/#life-before-service-mesh","title":"Life Before Service Mesh","text":"<p>Let's say you are building a service-based architecture consisting of multiple microservices. Any given microservice may interact with many other microservices.</p> <p>For service to service communication between microservices the developers must implement the following:</p> <ol> <li> <p>Retry Logic and Circuit Breaking</p> <p>Let's say service <code>A</code> makes a call to service <code>B</code>.</p> <ul> <li>If the call to service <code>B</code> fails, service <code>A</code> may want to retry</li> <li>Service <code>A</code> must have retry logic in the codebase (frequency, interval etc.)</li> <li>The retry logic may depend on the microservices involved in the picture</li> <li>Service <code>A</code> must detect failures and encapsulate the logic of preventing a failure from constantly recurring (Circuit Breaking)</li> </ul> </li> <li> <p>Authentication and Authorization</p> <p>Let's say service <code>A</code> makes a call to service <code>B</code>.</p> <ul> <li>You may want authentication between service <code>A</code> and <code>B</code></li> <li>Service <code>A</code> and service <code>B</code> must have authentication logic in the codebase</li> </ul> </li> <li> <p>Mutual TLS</p> <p>Let's say service <code>A</code> makes a call to service <code>B</code>.</p> <ul> <li>You may want service <code>A</code> and service <code>B</code> to interact over <code>HTTPS</code> instead of <code>HTTP</code></li> <li>The certificate for each service should be issued, maintained, and rotated for each service</li> </ul> </li> <li> <p>Monitoring</p> <p>Let's say service <code>A</code> makes a call to service <code>B</code>.</p> <ul> <li>You may want to know the number of requests/sec the service <code>A</code> sends</li> <li>You may also want to know the number of requests/sec the service <code>B</code> receives</li> <li>You may want to know metrics like latency, HTTP 200, 4xx, and 5xx count</li> <li>The developer must write logic to expose these metrics</li> </ul> </li> <li> <p>Distributed Tracing</p> <p>Let's say service <code>A</code> makes a call to service <code>B</code> and service <code>B</code> in turn call service <code>C</code> and <code>D</code>.</p> <ul> <li>You may want to trace requests down to each service to figure out where the latency may be</li> <li>You may want to visualize the flow of requests between microservices</li> </ul> </li> <li> <p>Traffic Splitting</p> <p>Let's say service <code>A</code> makes a call to service <code>B</code>. Service <code>B</code> has two versions <code>B1</code> and <code>B2</code>.</p> <ul> <li>You may want to send only 10% of the traffic to <code>B1</code> and rest 90% to <code>B2</code></li> </ul> </li> </ol> <p> </p> <p>There are many other features like load balancing, rate limiting, service registry/discovery that the developer should implement.</p> <p>Consider implementing these features for all microservices. This is the Code Oriented Pattern which is very painful and not an easy task. The complexity grows as more and more microservices are added. And of course, this is not scalable for the following reasons:</p> <ul> <li>Since each microservice might be written in different programming languages, implementing these features for each microservice becomes language-specific.</li> <li>Prone to errors during implementation.</li> <li>Difficult to upgrade each microservice as the system grows.</li> <li>Different teams within the same organization may adopt different implementations.</li> </ul> <p>That's where service mesh comes into the picture!</p> <p>Service mesh commonly uses sidecar pattern and provides you these features out of the box without adding any additional code in your microservices.</p>"},{"location":"kubernetes-on-eks/service-mesh/introduction-to-service-mesh/#sidecar-pattern","title":"Sidecar Pattern","text":"<p>Sidecar pattern enables applications to be composed of heterogeneous components and technologies. This pattern is named Sidecar because it resembles a sidecar attached to a motorcycle.</p> <p>In the pattern, the sidecar is attached to a parent application and provides supporting features for the application. Thus, all microservices concerns will be handled by the sidecar in homogenous way and injected on the fly by platform when deploying the application.</p> <p> </p> <p>References:</p> <ul> <li>What is a Service Mesh?</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/istio-metrics-and-monitoring/","title":"Istio Metrics and Monitoring","text":"<p>Proxies that are running as sidecar containers in our application exposes rich telemetry about all the network traffic between these services.</p> <p>We can visualize these metrics using Prometheus, Grafana, Kiali, and Jaegar. Istio provide these tools as addons and can be simply installed using the following command:</p> <pre><code>kubectl apply -f samples/addons/&lt;tool-name&gt;.yaml\n</code></pre> <p>Since we already have Prometheus and Grafana installed in our cluster, we are going to use that and not install it via Istio.</p> <p>Note</p> <p>If you don't already have prometheus and grafana installed in your cluster, you can install them from istio addons as follows:</p> <pre><code># Install grafana istio addon\nkubectl apply -f samples/addons/prometheus.yaml\n\n# Install grafana istio addon\nkubectl apply -f samples/addons/grafana.yaml\n</code></pre> <p>This will have the required dashboards pre-installed. You don't need to import them as we had to do in our existing grafana deployment.</p>"},{"location":"kubernetes-on-eks/service-mesh/istio-metrics-and-monitoring/#step-1-update-prometheus","title":"Step 1: Update Prometheus","text":"<p>In order for prometheus to pick istio metrics correctly, we need to make a small change to our prometheus deployment.</p> <p>Edit <code>prometheus-server</code> configmap as follows:</p> <pre><code># Set kubectl editor to nano\nexport KUBE_EDITOR=nano\n\n# Edit the configmap\nkubectl edit configmap prometheus-server -n prometheus\n</code></pre> <p>Make the following changes in <code>prometheus.yml</code> section:</p> <ol> <li>Replace all occurances of <code>(.+?)(?::\\d+)?;(\\d+)</code> with <code>([^:]+)(?::\\d+)?;(\\d+)</code></li> <li>Change the <code>scrape_interval</code> to <code>15s</code></li> </ol> <p>Now, restart the <code>prometheus-server</code> deployment:</p> <pre><code>kubectl rollout restart deploy/prometheus-server -n prometheus\n</code></pre> <p>Verify if the pods are running without error:</p> <pre><code>kubectl get pods -n prometheus\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/istio-metrics-and-monitoring/#step-2-import-istio-dashboards-in-grafana","title":"Step 2: Import Istio Dashboards in Grafana","text":"<p>Next, let's import Istio dashboards in our Grafana deployment. We'll use a script to that.</p> <code>import-istio-grafana-dashboards.sh</code> <pre><code># Address of Grafana\nGRAFANA_HOST=\"https://grafana.example.com\"\n# Login credentials, if authentication is used\nGRAFANA_CRED=\"admin:RP6xkxD\"\n# The name of the Prometheus data source to use\nGRAFANA_DATASOURCE=\"Prometheus\"\n# The version of Istio to deploy\nVERSION=1.20.0\n# Import all Istio dashboards\nfor DASHBOARD in 7639 11829 7636 7630 7645 13277; do\n    REVISION=\"$(curl -s https://grafana.com/api/dashboards/${DASHBOARD}/revisions -s | jq \".items[] | select(.description | contains(\\\"${VERSION}\\\")) | .revision\")\"\n    curl -s https://grafana.com/api/dashboards/${DASHBOARD}/revisions/${REVISION}/download &gt; /tmp/dashboard.json\n    echo \"Importing $(cat /tmp/dashboard.json | jq -r '.title') (revision ${REVISION}, id ${DASHBOARD})...\"\n    curl -s -k -u \"$GRAFANA_CRED\" -XPOST \\\n        -H \"Accept: application/json\" \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\\\"dashboard\\\":$(cat /tmp/dashboard.json),\\\"overwrite\\\":true, \\\n            \\\"inputs\\\":[{\\\"name\\\":\\\"DS_PROMETHEUS\\\",\\\"type\\\":\\\"datasource\\\", \\\n            \\\"pluginId\\\":\\\"prometheus\\\",\\\"value\\\":\\\"$GRAFANA_DATASOURCE\\\"}]}\" \\\n        $GRAFANA_HOST/api/dashboards/import\n    echo -e \"\\nDone\\n\"\ndone\n</code></pre> <p>Make sure to replace the values of <code>GRAFANA_HOST</code> and <code>GRAFANA_CRED</code> variables with the values specific to your Grafana set up.</p> <p>Now, run the script to import dashboards to your Grafana:</p> <pre><code># Give execute permission to the script\nchmod +x import-istio-grafana-dashboards.sh\n\n# Execute the script\n./import-istio-grafana-dashboards.sh\n</code></pre> <p>Go to Grafana console and verify if Istio related dashboards were imported successfully.</p> <p> </p>"},{"location":"kubernetes-on-eks/service-mesh/istio-metrics-and-monitoring/#step-3-generate-traffic-to-gather-istio-metrics","title":"Step 3: Generate Traffic to Gather Istio Metrics","text":"<p>Let's generate traffic for our book management microservices to gather sufficient Istio metrics that we can visualize in Grafana. We'll use selenium to simulate and generate traffic.</p> <p>First, create a python script as follows:</p> <code>main.py</code> <pre><code>from selenium import webdriver\nimport time\n\ndriver = webdriver.Chrome()\nfor i in range(1000):\n    driver.get(\"https://book-web.example.com/books\")\n    time.sleep(0.5)\n    driver.get(\"https://book-web.example.com/books/genre/1\")\n    time.sleep(0.5)\n    driver.get(\"https://book-web.example.com/books/genre/2\")\n    time.sleep(0.5)\n    driver.get(\"https://book-web.example.com/books/genre/3\")\n    time.sleep(0.5)\n\ndriver.close()\n</code></pre> <p>Next, create a virtual environment and install selenium using pip3:</p> <pre><code># Create virtual environment\nvirtualenv venv\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Install selenium\npip3 install selenium\n</code></pre> <p>Now, run the traffic generator:</p> <pre><code>python3 main.py\n</code></pre> <p>Now, head to Grafana and check the Istio dashboards to confirm if you can view Istio metrics.</p> <p>References:</p> <ul> <li>Import Istio Dashboards Script</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/retry-strategy-using-istio/","title":"Retry Strategy Using Istio","text":"<p>A retry setting specifies the maximum number of times an envoy proxy attempts to connect to a service if the initial call fails. Retries can enhance service availability and application performance by making sure that calls don\u2019t fail permanently because of transient problems such as a temporarily overloaded service or network.</p> <p>Retry strategy is implemented using <code>VirtualService</code>.</p> <p>Also, note that the default retry behavior for HTTP requests is to retry twice before returning the error. But you can customize this as we will see later in this section.</p>"},{"location":"kubernetes-on-eks/service-mesh/retry-strategy-using-istio/#step-1-deploy-application","title":"Step 1: Deploy Application","text":"<p>First, let's deploy the application and other Istio components:</p> <code>00-namespace.yml</code> <code>nodeapp-deployment.yml</code> <code>nodeapp-service.yml</code> <code>gateway.yml</code> <code>virtual-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: retry-strategy\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nodeapp-deployment\n  namespace: retry-strategy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nodeapp\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: nodeapp\n        version: v1\n    spec:\n      containers:\n      - name: nodeapp\n        image: reyanshkharga/nodeapp:buggy\n        imagePullPolicy: Always\n        env:\n        - name: BUGGY\n          value: \"503\"\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nodeapp-service\n  namespace: retry-strategy\nspec:\n  type: ClusterIP\n  selector:\n    app: nodeapp\n  ports:\n    - port: 80\n      targetPort: 5000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nodeapp-gateway\n  namespace: retry-strategy\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nodeapp.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: nodeapp-virtualservice\n  namespace: retry-strategy\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"nodeapp.example.com\"\n  gateways:\n  - nodeapp-gateway\n  http:\n  - route:\n    - destination:\n        host: nodeapp-service\n    retries:\n      attempts: 2\n      perTryTimeout: 2s\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> annotation in virtual service with the istio load balancer DNS.</p> <p>Note</p> <p>The application uses <code>reyanshkharga/nodeapp:buggy</code> image with an environment variable <code>BUGGY</code> and the value is set to <code>503</code>. With this configuration, the app returns <code>503</code> error intermittently. More precisely, it generates a random number between 1 and 10 and returns <code>503</code> if the random number is 9.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nodeapp-deployment.yml\n\u2502   |-- nodeapp-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes and istio objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>Verify if the istio proxies are created for the application:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/retry-strategy-using-istio/#step-2-generate-traffic-using-fortio","title":"Step 2: Generate Traffic Using Fortio","text":"<p>Deploy a fortio pod as follows:</p> <code>fortio-traffic-generator.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name:  traffic-generator\n  namespace: circuit-breaker\n  labels:\n    app: traffic-generator\n    version: v1\nspec:\n  containers:\n  - name: fortio\n    image: fortio/fortio\n</code></pre> <pre><code>kubectl apply -f fortio-traffic-generator.yml\n</code></pre> <p>Now, let's generate traffic on our nodeapp service using fortio.</p> <p>Note that since the retry is implemented using VirtualService meaning the traffic routing rules apply when a host is addressed as defined by the VirtualService.</p> <p>So, you need to address the host and not the service when testing using fortio.</p> <pre><code># Set fortio pod name\nexport FORTIO_POD=traffic-generator\n\n# Call nodeapp service using fortio pod\nkubectl exec $FORTIO_POD -n retry-strategy -c fortio -- /usr/bin/fortio curl -quiet https://nodeapp.example.com\n\n# Generate load on nodeapp service using fortio\nkubectl exec $FORTIO_POD -n retry-strategy -c fortio -- /usr/bin/fortio load -c 3 -qps 0 -n 30 -loglevel Warning -quiet https://nodeapp.example.com\n</code></pre> <p>Note</p> <p>The following command sends a total of 30 HTTP requests to the host with 3 concurrent connections. The <code>qps</code> configures queries per second; setting it to 0 indicates sending requests as fast as possible without a specific limit.</p> <pre><code>kubectl exec $FORTIO_POD -n retry-strategy -c fortio -- /usr/bin/fortio load -c 3 -qps 0 -n 30 -loglevel Warning -quiet https://nodeapp.example.com\n</code></pre> <p>You'll notice almost all the requests get a successful response (200) although the service is degraded (intermittent 503).</p> <p>You can check kiali and actually see that the service is degraded (5xx error) but on the client end you get successfull response due to retries.</p> <p>You can customize the retry config using VirtualService depending on your your application and use case.</p>"},{"location":"kubernetes-on-eks/service-mesh/retry-strategy-using-istio/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nodeapp-deployment.yml\n\u2502   |-- nodeapp-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtual-service.yml\n\u2502   |-- fortio-traffic-generator.yml\n</code></pre> <p>Let's delete all the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/","title":"Traffic Management in Istio","text":"<p>Before we go and install Istio with AWS application load balancer, it is important to understand the traffic management in Istio.</p>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/#istio-ingress-gateway","title":"Istio Ingress Gateway","text":"<p>When you install Istio, it creates an Ingress Gateway service that describes a load balancer operating at the edge of the mesh, receiving incoming or outgoing HTTP/TCP connections.</p> <p>Istio Ingress Gateway acts as the entry point for external traffic into the service mesh.</p> <p>By default, the Istio Ingress Gateway service type is set to <code>LoadBalancer</code>, thus creating a classic load balancer in AWS.</p>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/#istio-traffic-management-apis","title":"Istio Traffic Management APIs","text":"<p>While Istio\u2019s basic service discovery and load balancing gives you a working service mesh, it\u2019s far from all that Istio can do. In many cases you might want more fine-grained control over what happens to your mesh traffic.</p> <p>You might want to direct a particular percentage of traffic to a new version of a service as part of A/B testing, or apply a different load balancing policy to traffic for a particular subset of service instances. You might also want to apply special rules to traffic coming into or out of your mesh, or add an external dependency of your mesh to the service registry.</p> <p>You can do all this and more by adding your own traffic configuration to Istio using Istio\u2019s traffic management API.</p> <p>Some of the important traffic management API resources include:</p> <ol> <li>Gateway</li> <li>Virtual Service</li> <li>Destination Rule</li> </ol> <p>In this section, we'll just have an overview of these concepts. We'll see examples later on in another section.</p>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/#istio-gateway","title":"Istio Gateway","text":"<p>The Gateway object in Istio defines the configuration for how traffic should be routed to services within the mesh based on specific criteria.</p> <p>It's a Kubernetes Custom Resource Definition (CRD) used to define the rules and settings for routing traffic to a particular set of services.</p> <p>It specifies the ports, protocols, and hosts (among other criteria) that traffic should be directed to within the service mesh.</p> <p>Here's an example of a Gateway object:</p> <code>my-istio-gateway.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nginx-gateway\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nginx-app.example.com\"\n</code></pre> <p>Istio Gateway vs Kubernetes Ingress</p> <p>Istio <code>Gateway</code> provides more extensive customization and flexibility than kubernetes Ingress, and allows Istio features such as monitoring and route rules to be applied to traffic entering the cluster.</p> <p>With Istio Gateway you can implement the following with ease:</p> <ul> <li>Timeouts</li> <li>Retries</li> <li>Circuit Breaker</li> <li>Fault Injection</li> <li>Traffic Splitting</li> <li>Canary Deployment</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/#istio-virtual-service","title":"Istio Virtual Service","text":"<p>Istio <code>VirtualService</code> works in conjunction with Istio Gateway. it defines the destination service.</p> <p>A Virtual Service defines the rules that control how requests for a service are routed within an Istio service mesh.</p> <p>For example, it can be configured to split traffic among two or more versions of a service as shown below:</p> <code>my-istio-virtual-service.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: review\nspec:\n  hosts:\n    - reviews.example.com\n  http:\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n      weight: 80\n    - destination:\n        host: reviews\n        subset: v2\n      weight: 20\n  ...\n</code></pre> <p>Note</p> <p>For the virtual service described above, you must define the subsets using Destination Rule.</p> <p>Another use case might involve routing traffic to a service based on the URI prefix as shown below:</p> <code>my-istio-virtual-service.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n    - bookinfo.example.com\n  http:\n  - match:\n    - uri:\n        prefix: /reviews\n    route:\n    - destination:\n        host: reviews\n  - match:\n    - uri:\n        prefix: /ratings\n    route:\n    - destination:\n        host: ratings\n  ...\n</code></pre> <p>Each virtual service consists of a set of routing rules that are evaluated in order, letting Istio match each given request to the virtual service to a specific real destination within the mesh.</p>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/#istio-destination-rule","title":"Istio Destination Rule","text":"<p>Istio <code>DestinationRule</code> defines policies that apply to traffic intended for a service after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool.</p> <p>You can think of virtual services as how you route your traffic to a given destination, and then you use destination rules to configure what happens to traffic for that destination. Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic\u2019s \u201creal\u201d destination.</p> <p>In particular, you use destination rules to specify named service subsets, such as grouping all a given service\u2019s instances by version. You can then use these service subsets in the routing rules of virtual services to control the traffic to different instances of your services.</p> <pre><code>graph LR\n  A(Traffic) --&gt; G(Gateway);\n  G --&gt; B(Virtual Service);\n  B --&gt;|50%| C(\"Destination Rule\n  (subeset v1)\");\n  B --&gt;|50%| D(\"Destination Rule\n  (subeset v2)\");\n  C --&gt; E(Reviews - v1);\n  D --&gt; F(Reviews - v2);</code></pre> <p>Here's an example of a Destination that configures traffic policies for the <code>reviews</code> service in Istio. It defines subsets labeled as <code>v1</code>, <code>v2</code>, and <code>v3</code>, each representing different versions. The <code>v1</code> subset uses a <code>RANDOM</code> load balancer strategy, <code>v2</code> utilizes <code>ROUND_ROBIN</code>, while <code>v3</code> likely relies on a default strategy as it's not explicitly defined.</p> <code>my-istio-destination-rule.yml</code> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews\nspec:\n  host: reviews\n  trafficPolicy:\n    loadBalancer:\n      simple: RANDOM\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n  - name: v2\n    labels:\n      version: v2\n    trafficPolicy:\n      loadBalancer:\n        simple: ROUND_ROBIN\n  - name: v3\n    labels:\n      version: v3\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/traffic-management-in-istio/#traffic-flow-in-istio","title":"Traffic Flow in Istio","text":"<p>The figure below depicts the flow of control across configuration resources:</p> <p> </p> <p>References:</p> <ul> <li>Istio Ingress Gateway Workshop</li> <li>Ingress Gateways</li> <li>Istio Traffic Management</li> </ul>"},{"location":"kubernetes-on-eks/service-mesh/traffic-splitting-using-istio/","title":"Traffic Splitting Using Istio","text":"<p>Traffic splitting helps compare different versions of something (like a webpage or marketing campaign) to find the best-performing option, leading to better decisions, improved user experiences, and reduced risks before implementing changes widely.</p> <p>Suppose you have two versions of an application. Let's say <code>v1</code> and <code>v2</code>. You want to send only some traffic to <code>v2</code> before completely switching to <code>v2</code>. How do you do that?</p> <p>Istio allows you to split traffic between different versions of an application.</p> <p>For that you first create a istio <code>DestinationRule</code> that defines the subsets of applications you have (<code>v1</code> and <code>v2</code> in this case).</p> <p>Once you have defined the subsets, you can configure istio <code>VirtualService</code> to split traffic between the subsets. You can mention the weight for each subsets. The weight is the percentage of traffic the subset receives.</p> <p>For example, imagine you have a service called <code>reviews</code>. Currently, it's running version <code>v1</code>. Now, you've made some updates (let's call this version <code>v2</code>). Initially, you only want 20% of the traffic to go to the new <code>v2</code> version of the service, while the rest continues to use the original <code>v1</code> version.</p> <pre><code>graph LR\n  A(Traffic) --&gt; G(Gateway);\n  G --&gt; B(Virtual Service);\n  B --&gt;|80%| C(\"Destination Rule\n  (subeset v1)\");\n  B --&gt;|20%| D(\"Destination Rule\n  (subeset v2)\");\n  C --&gt; E(Reviews - v1);\n  D --&gt; F(Reviews - v2);</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/traffic-splitting-using-istio/#step-1-deploy-application-with-multiple-versions","title":"Step 1: Deploy Application With Multiple Versions","text":"<p>First, let's deploy two versions of the application: v1 and v2, along with other Istio components like Destination Rule, Virtual Service, and Gateway.</p> <code>00-namespace.yml</code> <code>nginx-deployment-v1.yml</code> <code>nginx-deployment-v2.yml</code> <code>nginx-service.yml</code> <code>gateway.yml</code> <code>destination-rule.yml</code> <code>virtual-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: traffic-split\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-v1\n  namespace: traffic-split\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-v2\n  namespace: traffic-split\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v2\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v2\n        imagePullPolicy: Always\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: traffic-split\nspec:\n  type: ClusterIP\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nginx-gateway\n  namespace: traffic-split\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nginx-app.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: nginx-destinationrule\n  namespace: traffic-split\nspec:\n  host: nginx-service\n  subsets:\n    - name: v1\n      labels:\n        version: \"v1\"\n    - name: v2\n      labels:\n        version: \"v2\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: nginx-virtualservice\n  namespace: traffic-split\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"nginx-app.example.com\"\n  gateways:\n  - nginx-gateway\n  http:\n  - route:\n    - destination:\n        host: nginx-service\n        subset: v1\n      weight: 80\n    - destination:\n        host: nginx-service\n        subset: v2\n      weight: 20\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> annotation in virtual service with the istio load balancer DNS.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment-v1.yml\n\u2502   |-- nginx-deployment-v2.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- destination-rule.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes and istio objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>Verify if the istio proxies are created for the application:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/traffic-splitting-using-istio/#step-2-generate-load-and-verify-traffic-distribution-in-kiali","title":"Step 2: Generate Load and Verify Traffic Distribution in Kiali","text":"<p>Let's generate some traffic and verify the traffic distribution in kiali.</p> <p>First, let's write a script that generates traffic:</p> <code>generate-traffic.sh</code> <pre><code>#!/bin/bash\nwhile true\ndo\n  curl -s -f -o /dev/null \"https://nginx-app.example.com\"\n  echo \"sleeping for 0.5 seconds\"\n  sleep 0.5\ndone\n</code></pre> <p>Now, let's execute the script to generate traffic:</p> <pre><code># Give execute permission to script\nchmod +x generate-traffic.sh\n\n# Execute script\n./generate-traffic.sh\n</code></pre> <p>Wait for about 5 minutes and then view trafic distribution graph in kiali. Verify that the traffic distribution closely matches the defined weights for each subsets: 80% to <code>v1</code> and 20% to <code>v2</code>.</p>"},{"location":"kubernetes-on-eks/service-mesh/traffic-splitting-using-istio/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment-v1.yml\n\u2502   |-- nginx-deployment-v2.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- destination-rule.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's delete all the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/","title":"Update Book Management Microservices With Istio Gateway","text":"<p>Let's update book management microservices that we created earlier. We will remove all kubernetes ingress objects and use istio gateways and virtual services instead.</p>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#prerequisite","title":"Prerequisite","text":"<p>To follow this tutorial, you'll require a domain and, additionally, an SSL certificate for the domain and its subdomains.</p> <ol> <li> <p>Register a Route 53 Domain</p> <p>Go to AWS Console and register a Route 53 domain. You can opt for a cheaper TLD (top level domain) such as <code>.link</code></p> <p>Note</p> <p>It usually takes about 10 minutes but it might take about an hour for the registered domain to become available.</p> </li> <li> <p>Request a Public Certificate</p> <p>Visit AWS Certificate Manager in AWS Console and request a public certificate for your domain and all the subdomains. For example, if you registered for a domain <code>example.com</code> then request certificate for <code>example.com</code> and <code>*.example.com</code></p> <p>Note</p> <p>Make sure you request the certificate in the region where your EKS cluster is in.</p> </li> <li> <p>Validate the Certificate</p> <p>Validate the requested certificate by adding <code>CNAME</code> records in Route 53. It is a very simple process. Go to the certificate you created and click on <code>Create records in Route 53</code>. The <code>CNAMEs</code> will be automatically added to Route 53.</p> <p>Note</p> <p>It usually takes about 5 minutes but it might take about an hour for the certificate to be ready for use.</p> </li> </ol> <p>Now that you have everything you need, let's move on to the demonstration.</p>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#docker-images","title":"Docker Images","text":"<p>Here are the Docker Images used in this tutorial:</p> <ul> <li>reyanshkharga/book-management:book-details</li> <li>reyanshkharga/book-management:book-genres</li> <li>reyanshkharga/book-management:book-web</li> </ul> <p>Note</p> <ol> <li> <p><code>reyanshkharga/book-management:book-genres</code> is a Node.js backend app that uses MongoDB to store and retrieve data, providing a list of books for each genre.</p> <p>Environment Variables:</p> <ul> <li><code>MONGODB_URI</code> (Required)</li> </ul> </li> <li> <p><code>reyanshkharga/book-management:book-details</code> is a Node.js backend app that uses MongoDB to store and retrieve data, providing details for a given book. It also calls the <code>book-genres</code> microservice to return the list of books for a given genre.</p> <p>Environment variables:</p> <ul> <li><code>MONGODB_URI</code> (Required)</li> <li><code>REACT_APP_AGENRES_API_ENDPOINTPI_ENDPOINT</code> (Required)</li> </ul> </li> <li> <p><code>reyanshkharga/book-management:book-web</code> is the frontend for book management application.</p> <p>Environment variables:</p> <ul> <li><code>REACT_APP_API_ENDPOINT</code> (Required)</li> </ul> </li> </ol>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#objective","title":"Objective","text":"<p>We are going to deploy the following microservices on our EKS kubernetes cluster:</p> <ol> <li>Book Details Database microservice</li> <li>Book Genres Database microservice</li> <li>Book Details microservice</li> <li>Book Genres microservice</li> <li>Book Web microservice</li> </ol> <p>The following diagram illustrates the communication between microservices:</p> <pre><code>graph LR\n  A(Book Web) --&gt; B(Book Details);\n  B --&gt; C(Book Genres);\n  B -.-&gt; BD[(\"Book Details \n    Database\")];\n  C -.-&gt; CD[(\"Book Genres \n    Database\")];</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#step-1-deploy-book-genres-database-microservice","title":"Step 1: Deploy Book Genres Database Microservice","text":"<p>Let's create the kubernetes objects for our Book Genres Database microservice as follows:</p> <code>00-namespace.yml</code> <code>storageclass.yml</code> <code>pvc.yml</code> <code>configmap.yml</code> <code>deployment-and-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-genres-db\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: book-genres-db-storageclass\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  tagSpecification_1: \"Name=eks-book-genres-db-storage\"\n  tagSpecification_2: \"CreatedBy=aws-ebs-csi-driver\"\nreclaimPolicy: Delete\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: book-genres-db-pvc\n  namespace: book-genres-db\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: book-genres-db-storageclass\n  resources:\n    requests:\n      storage: 4Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: book-genres-mongo-initdb\n  namespace: book-genres-db\ndata:\n  mongo-init.sh: |-\n    mongo &lt;&lt;EOF\n    use books\n    db.genres.insert({_id: 1, \"books\": [1,2,3,4]})\n    db.genres.insert({_id: 2, \"books\": [5,6,7]})\n    db.genres.insert({_id: 3, \"books\": [8]})\n    EOF\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-genres-db-deployment\n  namespace: book-genres-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-genres-db\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-genres-db\n        version: v1\n    spec:\n      containers:\n      - name: book-genres-db\n        image: mongo:5.0.2\n        ports:\n          - containerPort: 27017\n        volumeMounts:\n        - name: book-genres-db-storage\n          mountPath: /data/db\n        - name: mongo-initdb\n          mountPath: /docker-entrypoint-initdb.d\n      volumes:\n      - name: book-genres-db-storage\n        persistentVolumeClaim:\n          claimName: book-genres-db-pvc\n      - name: mongo-initdb\n        configMap:\n          name: book-genres-mongo-initdb\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-genres-db-service\n  namespace: book-genres-db\nspec:\n  type: ClusterIP\n  selector:\n    app: book-genres-db\n    version: v1\n  ports:\n    - port: 27017\n      targetPort: 27017\n      name: tcp\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-genres-db\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- configmap.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Genres Database microservice:</p> <pre><code>kubectl apply -f book-genres-db/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-genres-db</code></li> <li>A <code>StorageClass</code> (SC) for dynamic provisioning of persistent volume</li> <li>A <code>PersistentVolumeClaim</code> (PVC) in the <code>book-genres-db</code> namespace</li> <li>MongoDB deployment in the <code>book-genres-db</code> namespace</li> <li>MongoDB service in the <code>book-genres-db</code> namespace</li> </ol> <p>We are using Amazon EBS to persist the MongoDB data. EBS is provisioned dynamically using AWS EBS-CSI driver.</p> <p>With persistent volume even if the MongoDB pod goes down the data will remain intact. When the new pod comes up we'll have the access to the same data.</p> <p>We are also using configmap to populate data in the MongoDB database.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-genres-db namespace\nkubectl get all -n book-genres-db\n\n# List StorageClass\nkubectl get sc\n\n# List PersistentVolume\nkubectl get pv\n\n# List PersistenvVolumeClaim\nkubectl get pvc -n book-genres-db\n</code></pre> <p>Verify if MongoDB is working as expected:</p> <pre><code># Start a shell session inside the book-genres-db container\nkubectl exec -it &lt;mongodb-pod-name&gt; -n book-genres-db -- bash\n\n# Start the mongo Shell to interact with MongoDB\nmongo\n\n# List Databases\nshow dbs\n\n# Switch to a Database\nuse &lt;db-name&gt;\n\n# List collections\nshow collections\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#step-2-deploy-book-details-database-microservice","title":"Step 2: Deploy Book Details Database Microservice","text":"<p>Let's create the kubernetes objects for our Book Details Database microservice as follows:</p> <code>00-namespace.yml</code> <code>storageclass.yml</code> <code>pvc.yml</code> <code>configmap.yml</code> <code>deployment-and-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-details-db\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: book-details-db-storageclass\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  tagSpecification_1: \"Name=eks-book-details-db-storage\"\n  tagSpecification_2: \"CreatedBy=aws-ebs-csi-driver\"\nreclaimPolicy: Delete\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: book-details-db-pvc\n  namespace: book-details-db\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: book-details-db-storageclass\n  resources:\n    requests:\n      storage: 4Gi\n</code></pre> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: book-details-mongo-initdb\n  namespace: book-details-db\ndata:\n  mongo-init.sh: |-\n    mongo &lt;&lt;EOF\n    use books\n    db.details.insert({_id: 1, \"title\": \"Book1\", \"copies_sold\": 100})\n    db.details.insert({_id: 2, \"title\": \"Book2\", \"copies_sold\": 200})\n    db.details.insert({_id: 3, \"title\": \"Book3\", \"copies_sold\": 300})\n    db.details.insert({_id: 4, \"title\": \"Book4\", \"copies_sold\": 400})\n    db.details.insert({_id: 5, \"title\": \"Book5\", \"copies_sold\": 500})\n    db.details.insert({_id: 6, \"title\": \"Book6\", \"copies_sold\": 600})\n    db.details.insert({_id: 7, \"title\": \"Book7\", \"copies_sold\": 700})\n    db.details.insert({_id: 8, \"title\": \"Book8\", \"copies_sold\": 800})\n    EOF\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-details-db-deployment\n  namespace: book-details-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-details-db\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-details-db\n        version: v1\n    spec:\n      containers:\n      - name: book-details-db\n        image: mongo:5.0.2\n        ports:\n          - containerPort: 27017\n        volumeMounts:\n        - name: book-details-db-storage\n          mountPath: /data/db\n        - name: mongo-initdb\n          mountPath: /docker-entrypoint-initdb.d\n      volumes:\n      - name: book-details-db-storage\n        persistentVolumeClaim:\n          claimName: book-details-db-pvc\n      - name: mongo-initdb\n        configMap:\n          name: book-details-mongo-initdb\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-details-db-service\n  namespace: book-details-db\nspec:\n  type: ClusterIP\n  selector:\n    app: book-details-db\n    version: v1\n  ports:\n    - port: 27017\n      targetPort: 27017\n      name: tcp\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-details-db\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- configmap.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- storageclass.yml\n\u2502   |   |-- pvc.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Details Database microservice:</p> <pre><code>kubectl apply -f book-details-db/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-details-db</code></li> <li>A <code>StorageClass</code> (SC) for dynamic provisioning of persistent volume</li> <li>A <code>PersistentVolumeClaim</code> (PVC) in the <code>book-details-db</code> namespace</li> <li>MongoDB deployment in the <code>book-details-db</code> namespace</li> <li>MongoDB service in the <code>book-details-db</code> namespace</li> </ol> <p>We are using Amazon EBS to persist the MongoDB data. EBS is provisioned dynamically using AWS EBS-CSI driver.</p> <p>With persistent volume even if the MongoDB pod goes down the data will remain intact. When the new pod comes up we'll have the access to the same data.</p> <p>We are also using configmap to populate data in the MongoDB database.</p> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-details-db namespace\nkubectl get all -n book-details-db\n\n# List StorageClass\nkubectl get sc\n\n# List PersistentVolume\nkubectl get pv\n\n# List PersistenvVolumeClaim\nkubectl get pvc -n book-details-db\n</code></pre> <p>Verify if MongoDB is working as expected:</p> <pre><code># Start a shell session inside the book-details-db container\nkubectl exec -it &lt;mongodb-pod-name&gt; -n book-details-db -- bash\n\n# Start the mongo Shell to interact with MongoDB\nmongo\n\n# List Databases\nshow dbs\n\n# Switch to a Database\nuse &lt;db-name&gt;\n\n# List collections\nshow collections\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#step-3-deploy-book-genres-microservice","title":"Step 3: Deploy Book Genres Microservice","text":"<p>Let's create the kubernetes objects for our Book Genres microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>gateway.yml</code> <code>virtualservice.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-genres\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-genres-deployment\n  namespace: book-genres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-genres\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-genres\n        version: v1\n    spec:\n      containers:\n      - name: book-genres\n        image: reyanshkharga/book-management:book-genres\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        env:\n        - name: MONGODB_URI\n          value: mongodb://book-genres-db-service.book-genres-db.svc.cluster.local:27017\n        - name: BUGGY_CODE\n          value: \"false\"\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-genres-service\n  namespace: book-genres\nspec:\n  type: ClusterIP\n  selector:\n    app: book-genres\n    version: v1\n  ports:\n    - port: 80\n      targetPort: 5000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: book-genres-gateway\n  namespace: book-genres\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"book-genres.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: book-genres-virtualservice\n  namespace: book-genres\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"book-genres.example.com\"\n  gateways:\n  - book-genres-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: book-genres-service\n        port:\n          number: 80\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Also, make sure to replace the value of external-dns.alpha.kubernetes.io/target with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-genres\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- gateway.yml\n\u2502   |   |-- virtualservice.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Genres microservice:</p> <pre><code>kubectl apply -f book-genres/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-genres</code></li> <li>Book Genres deployment in the <code>book-genres</code> namespace</li> <li>Book Genres service in the <code>book-genres</code> namespace</li> <li>Istio Gateway for Book Genres service</li> <li>Istio Virtual Service for Book Genres service</li> </ol> <p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-genres namespace\nkubectl get all -n book-genres\n\n# List gateways and virtual services\nkubectl get gateway,virtualservice -n book-genres\n</code></pre> <p>Open any browser on your local host machine and hit the URL to access the book genres service:</p> <pre><code>https://book-genres.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#step-4-deploy-book-details-microservice","title":"Step 4: Deploy Book Details Microservice","text":"<p>Let's create the kubernetes objects for our Book Details microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>gateway.yml</code> <code>virtualservice.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-details\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-details-deployment\n  namespace: book-details\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-details\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-details\n        version: v1\n    spec:\n      containers:\n      - name: book-details\n        image: reyanshkharga/book-management:book-details\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 5000\n        env:\n        - name: MONGODB_URI\n          value: mongodb://book-details-db-service.book-details-db.svc.cluster.local:27017\n        - name: BUGGY_CODE\n          value: \"false\"\n        - name: GENRES_API_ENDPOINT\n          value: http://book-genres-service.book-genres.svc.cluster.local\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-details-service\n  namespace: book-details\nspec:\n  type: ClusterIP\n  selector:\n    app: book-details\n    version: v1\n  ports:\n    - port: 80\n      targetPort: 5000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: book-details-gateway\n  namespace: book-details\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"book-details.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: book-details-virtualservice\n  namespace: book-details\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"book-details.example.com\"\n  gateways:\n  - book-details-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: book-details-service\n        port:\n          number: 80\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Also, make sure to replace the value of external-dns.alpha.kubernetes.io/target with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-details\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- gateway.yml\n\u2502   |   |-- virtualservice.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Details microservice:</p> <pre><code>kubectl apply -f book-details/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-details</code></li> <li>Book Details deployment in the <code>book-details</code> namespace</li> <li>Book Details service in the <code>book-details</code> namespace</li> <li>Istio Gateway for Book Details service</li> <li>Istio Virtual Service for Book Details service</li> </ol> <p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-details namespace\nkubectl get all -n book-details\n\n# List gateways and virtual services\nkubectl get gateway,virtualservice -n book-details\n</code></pre> <p>Open any browser on your local host machine and hit the URL to access the book details service:</p> <pre><code>https://book-details.example.com\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#step-5-deploy-book-web-microservice","title":"Step 5: Deploy Book Web Microservice","text":"<p>Let's create the kubernetes objects for our Book Web microservice as follows:</p> <code>00-namespace.yml</code> <code>deployment-and-service.yml</code> <code>gateway.yml</code> <code>virtualservice.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: book-web\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code># Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: book-web-deployment\n  namespace: book-web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: book-web\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: book-web\n        version: v1\n    spec:\n      containers:\n      - name: book-web\n        image: reyanshkharga/book-management:book-web\n        imagePullPolicy: Always\n        ports:\n          - containerPort: 3000\n        env:\n        - name: REACT_APP_API_ENDPOINT\n          value: https://book-details.example.com\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: book-web-service\n  namespace: book-web\nspec:\n  type: ClusterIP\n  selector:\n    app: book-web\n    version: v1\n  ports:\n    - port: 80\n      targetPort: 3000\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: book-web-gateway\n  namespace: book-web\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"book-web.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: book-web-virtualservice\n  namespace: book-web\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"book-web.example.com\"\n  gateways:\n  - book-web-gateway\n  http:\n  - match: \n    - uri:   \n        prefix: /\n    route:\n    - destination:\n        host: book-web-service\n        port:\n          number: 80\n</code></pre> <p>Notice the <code>istio-injection: enabled</code> label in the namespace object. This will ensure all objects in the namespace are part of the Istio service mesh.</p> <p>Also, make sure to replace the value of external-dns.alpha.kubernetes.io/target with the load balancer DNS that was created by ingress we created for Istio.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n|   |-- book-web\n\u2502   |   |-- 00-namespace.yml\n\u2502   |   |-- deployment-and-service.yml\n\u2502   |   |-- gateway.yml\n\u2502   |   |-- virtualservice.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes objects for Book Web microservice:</p> <pre><code>kubectl apply -f book-web/\n</code></pre> <p>This will create the following kubernetes objects:</p> <ol> <li>A namespace named <code>book-web</code></li> <li>Book Web deployment in the <code>book-web</code> namespace</li> <li>Book Web service in the <code>book-web</code> namespace</li> <li>Istio Gateway for Book Web service</li> <li>Istio Virtual Service for Book Web service</li> </ol> <p>View the updated proxy configuration:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>Verify if the resources were created successfully:</p> <pre><code># List all resources in book-web namespace\nkubectl get all -n book-web\n\n# List gateways and virtual services\nkubectl get gateway,virtualservice -n book-web\n</code></pre> <p>Open any browser on your local host machine and hit the URL to access the book web service:</p> <pre><code>https://book-web.example.com\n</code></pre> <p> </p> <p>Verify if everything is properly and you can interact with book web frontend service and get the book and genre details.</p>"},{"location":"kubernetes-on-eks/service-mesh/update-book-management-microservices-with-istio-gateway/#step-6-generate-traffic-to-gather-istio-metrics","title":"Step 6: Generate Traffic to Gather Istio Metrics","text":"<p>Let's generate traffic for our book management microservices to gather sufficient Istio metrics that we can visualize in Grafana. We'll use selenium to simulate and generate traffic.</p> <p>First, create a python script as follows:</p> <code>main.py</code> <pre><code>from selenium import webdriver\nimport time\n\ndriver = webdriver.Chrome()\nfor i in range(1000):\n    driver.get(\"https://book-web.example.com/books\")\n    time.sleep(0.5)\n    driver.get(\"https://book-web.example.com/books/genre/1\")\n    time.sleep(0.5)\n    driver.get(\"https://book-web.example.com/books/genre/2\")\n    time.sleep(0.5)\n    driver.get(\"https://book-web.example.com/books/genre/3\")\n    time.sleep(0.5)\n\ndriver.close()\n</code></pre> <p>Next, create a virtual environment and install selenium using pip3:</p> <pre><code># Create virtual environment\nvirtualenv venv\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Install selenium\npip3 install selenium\n</code></pre> <p>Now, run the traffic generator:</p> <pre><code>python3 main.py\n</code></pre> <p>Now, head to Grafana and check the Istio dashboards to confirm if you can view Istio metrics.</p> <p>Also, view the service graph in Kiali for the updated microservices. It should looks something like this:</p> <p> </p> <p>Notice how <code>book-details-service</code> calls <code>book-genres-service</code> and both services independently calls their respective databases.</p> <p>You won't see <code>book-web-service</code> calling <code>book-details-service</code> in the graph because <code>book-web-service</code> uses the public API to call <code>book-details-service</code>.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/","title":"Using Istio for Service Mesh","text":"<p>Services itself can't be part of the service mesh. They have to opt-in to become part of the Istio service mesh.</p> <p>There are two ways for the services to opt-in to become part of the service mesh:</p> <ol> <li>Through namespace labeling</li> <li>Injecting the sidecar proxies manually to pod or deployment object</li> </ol> <p>Note</p> <p>In either method, the end result is a sidecar proxy injected into the Kubernetes objects.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#opting-in-for-the-mesh-through-namespace-labeling","title":"Opting In for the Mesh Through Namespace Labeling","text":"<p>We can add the label <code>istio-injection=enabled</code> to a namespace so that all the pods running in the namespace will become part of the service mesh.</p> <pre><code>kubectl label namespace &lt;namespace-name&gt; istio-injection=enabled\n</code></pre> <p>You can also add the label directly in the YAML when you create the namespace using declarative approach.</p> <p>This approach of labelling the namespace meshes the entire namespace. Adding this namespace label instructs Istio to automatically inject Envoy sidecar proxies when you deploy your application later.</p> <p>Note</p> <p>Istio will inject sidecar proxies only to the pods that come after adding a label to the namespace. Existing pods won't be added to mesh. You can delete and recreate the pods if you want them to become part of the service mesh.</p> <p>Istio uses an extended version of the Envoy proxy which is a high-performance proxy developed in C++ to mediate all inbound and outbound traffic for all services in the service mesh. Envoy proxies are the only Istio components that interact with data plane traffic.</p> <p>Let's test it out!</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-1-create-a-namespace","title":"Step 1: Create a Namespace","text":"<p>First, let's create a namespace as follows:</p> <pre><code># Create a namespace\nkubectl create ns test\n\n# View the namespace in yaml format\nkubectl get ns test -o yaml\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-2-create-pod-in-the-namespace","title":"Step 2: Create Pod in the Namespace","text":"<p>Next, create a pod in the namespace we created above:</p> <pre><code># Create a pod\nkubectl run nginx --image=nginx -n test\n\n# List pods\nkubectl get pods -n test\n</code></pre> <p>You'll notice that there is only one container running in the pod.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-3-add-label-to-the-namespace","title":"Step 3: Add Label to the Namespace","text":"<p>Now, let's add <code>istio-injection=enabled</code> label to the namespace:</p> <pre><code># Add label to namespace\nkubectl label namespace test istio-injection=enabled\n\n# View the updated namespace in yaml format\nkubectl get ns test -o yaml\n</code></pre> <p>See if sidecar was injected to existing pod:</p> <pre><code># List pods\nkubectl get pods -n test\n</code></pre> <p>Since the label was applied after the pod was created, Istio won't inject the sidecar in the existing pod.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-4-delete-and-recreate-pod","title":"Step 4: Delete and Recreate Pod","text":"<p>Delete the pod and recreate it to see if Istio injects the sidecar proxies:</p> <pre><code># Delete pod\nkubectl delete po nginx -n test\n\n# Recreate pod\nkubectl run nginx --image=nginx -n test\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-5-verify-sidecar","title":"Step 5: Verify Sidecar","text":"<p>Verify if sidecar was injected to the pod:</p> <pre><code># List pods\nkubectl get pods -n test\n</code></pre> <p>This time you'll see two containers running in the nginx pod because Istio injected a sidecar container to our pod.</p> <p>Also, view the pod definition using the following command:</p> <pre><code># View the yaml definition of the pod\nkubectl get pod nginx -n test -o yaml\n</code></pre> <p>On inspection, you will find that a container called <code>istio-proxy</code> is running in the pod. This is the sidecar container that was injected by Istio.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-5-clean-up","title":"Step 5: Clean Up","text":"<p>Delete the resources we created:</p> <pre><code>kubectl delete ns test\n</code></pre> <p>This will delete the namespace <code>test</code> and all resources in it.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#opting-in-for-the-mesh-by-manually-injecting-the-sidecar","title":"Opting In for the Mesh by Manually Injecting the Sidecar","text":"<p>You can update an existing pod or deployment by injecting the sidecar proxies using the following command:</p> <pre><code>kubectl get &lt;deployment-or-pod-name&gt; -n &lt;namespace-name&gt; -o yaml | istioctl kube-inject -f - | kubectl apply -f -\n</code></pre> <p>The command above retrieves the YAML configuration of a kubernetes deployment or pod in a specified namespace, injects Istio sidecar proxy configuration, and applies the modified configuration back to the cluster.</p> <p>This approach enables to you inject sidecar proxies only to certain objects and not the entire namespace. However, it's less preferable due to its tedious nature\u2014you'll need to manually inject the sidecar into each deployment object, making it less scalable and more error-prone.</p> <p>Let's test it out!</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-1-create-a-namespace_1","title":"Step 1: Create a Namespace","text":"<p>First, let's create a namespace as follows:</p> <pre><code># Create a namespace\nkubectl create ns test\n\n# View the namespace in yaml format\nkubectl get ns test -o yaml\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-2-create-deployment-in-the-namespace","title":"Step 2: Create Deployment in the Namespace","text":"<p>Next, create a deployment in the namespace we created above:</p> <pre><code># Create a deployment\nkubectl create deployment nginx --image=nginx --replicas=1 -n test\n\n# List pods\nkubectl get pods -n test\n</code></pre> <p>You'll notice that there is only one container running in the pod.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-3-update-deployment-by-injecting-sidecar","title":"Step 3: Update Deployment by Injecting Sidecar","text":"<p>Now, let's update the deployment by injecting the sidecar proxies.</p> <pre><code>kubectl get deploy/nginx -n test -o yaml | istioctl kube-inject -f - | kubectl apply -f -\n</code></pre> <p>Once the deployment is applied, the pods in the deployment will be updated.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-4-verify-sidecar","title":"Step 4: Verify Sidecar","text":"<p>Verify if sidecar was injected to the pods:</p> <pre><code># List pods\nkubectl get pods -n test\n</code></pre> <p>This time you'll see two containers running in the nginx pod because Istio injected a sidecar container to our pod.</p> <p>Also, view the pod definition using the following command:</p> <pre><code># View the yaml definition of the pod\nkubectl get pod nginx -n test -o yaml\n</code></pre> <p>On inspection, you will find that a container called <code>istio-proxy</code> is running in the pod. This is the sidecar container that was injected by Istio.</p>"},{"location":"kubernetes-on-eks/service-mesh/using-istio-for-service-mesh/#step-5-clean-up_1","title":"Step 5: Clean Up","text":"<p>Delete the resources we created:</p> <pre><code>kubectl delete ns test\n</code></pre> <p>This will delete the namespace <code>test</code> and all resources in it.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-aborts/","title":"Inject Failure Using Aborts","text":"<p>Let's inject the fault for certain percentage of the traffic. We will send a fixed response with HTTP code <code>503</code>.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-aborts/#step-1-deploy-application","title":"Step 1: Deploy Application","text":"<p>First, let's deploy the application and other Istio components:</p> <code>00-namespace.yml</code> <code>nginx-deployment.yml</code> <code>nginx-service.yml</code> <code>gateway.yml</code> <code>virtual-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: fault-injection\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: fault-injection\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: fault-injection\nspec:\n  type: ClusterIP\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nginx-gateway\n  namespace: fault-injection\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nginx-app.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: nginx-virtualservice\n  namespace: fault-injection\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"nginx-app.example.com\"\n  gateways:\n  - nginx-gateway\n  http:\n  - route:\n    - destination:\n        host: nginx-service\n    fault:\n      abort:\n        percentage:\n          value: 80\n        httpStatus: 503\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> annotation in virtual service with the istio load balancer DNS.</p> <p>Observe that we are injecting abort failure with HTTP status code 503 for 80% of the traffic.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes and istio objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>Verify if the istio proxies are created for the application:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>Open the application in any browser and see if you receive <code>503</code> error for some of the requests.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-aborts/#step-2-generate-load-and-verify-failure-percentage-in-kiali","title":"Step 2: Generate Load and Verify Failure Percentage in Kiali","text":"<p>Let's use a script to automate traffic generation and record the failure rate:</p> <code>generate-traffic.sh</code> <pre><code>#!/bin/bash\nwhile true\ndo\n  curl -s -f -o /dev/null \"https://nginx-app.example.com\"\n  echo \"sleeping for 0.5 seconds\"\n  sleep 0.5\ndone\n</code></pre> <p>Now, let's execute the script to generate traffic:</p> <pre><code># Give execute permission to script\nchmod +x generate-traffic.sh\n\n# Execute script\n./generate-traffic.sh\n</code></pre> <p>Wait for about 5 minutes and then view error rate in kiali. You'll notice that about 80% of the traffic fails with <code>503</code> HTTP error code.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-aborts/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's delete all the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-delays/","title":"Inject Failure Using Delays","text":"<p>Let's inject failure using delays for 100% of the traffic.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-delays/#step-1-deploy-application","title":"Step 1: Deploy Application","text":"<p>First, let's deploy the application and other Istio components:</p> <code>00-namespace.yml</code> <code>nginx-deployment.yml</code> <code>nginx-service.yml</code> <code>gateway.yml</code> <code>virtual-service.yml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: delay-injection\n  labels:\n    istio-injection: enabled\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: delay-injection\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: nginx\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: reyanshkharga/nginx:v1\n        imagePullPolicy: Always\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: delay-injection\nspec:\n  type: ClusterIP\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n      name: http\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: nginx-gateway\n  namespace: delay-injection\nspec: \n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"nginx-app.example.com\"\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: nginx-virtualservice\n  namespace: delay-injection\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"istio-load-balancer-1556246780.ap-south-1.elb.amazonaws.com\"\nspec: \n  hosts:\n  - \"nginx-app.example.com\"\n  gateways:\n  - nginx-gateway\n  http:\n  - route:\n    - destination:\n        host: nginx-service\n    fault:\n      delay:\n        percentage:\n          value: 100\n        fixedDelay: 5s\n</code></pre> <p>Make sure to replace the value of <code>external-dns.alpha.kubernetes.io/target</code> annotation in virtual service with the istio load balancer DNS.</p> <p>Observe that we are injecting a delay of 5 seconds for 100% of the traffic.</p> <p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's apply the manifests to create the kubernetes and istio objects:</p> <pre><code>kubectl apply -f manifests/\n</code></pre> <p>Verify if the istio proxies are created for the application:</p> <pre><code># Retrieve proxy configuration\nistioctl proxy-config routes svc/istio-ingressgateway -n istio-system\n</code></pre> <p>Open the application in any browser and see if there is actually a delay in the response. Now, failure depends on the application, i.e., the application can set a timeout for the response and send a failure response if timed out.</p> <p>In our case, we have not implemented a timeout in our application, so you would only notice the delay and not the actual failure due to a timeout.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-delays/#step-2-generate-load-and-verify-delay-in-response","title":"Step 2: Generate Load and Verify Delay in Response","text":"<p>Let's use a script to automate traffic generation and record the delay:</p> <code>generate-traffic.sh</code> <pre><code>#!/bin/bash\nwhile true\ndo \n    echo $(curl https://nginx-app.example.com/ --silent --output /dev/null -w %{time_total}) seconds\ndone\n</code></pre> <p>Now, let's execute the script to generate traffic:</p> <pre><code># Give execute permission to script\nchmod +x generate-traffic.sh\n\n# Execute script\n./generate-traffic.sh\n</code></pre> <p>Observe the response time of each requests.</p> <p>Note</p> <p>You won't see the delay in Kiali because Kiali displays latency, not delay. Latency refers to the time from when a request reaches the server until the response is returned, whereas delay is the time from when a request is initiated to when the response is served.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/inject-failure-using-delays/#clean-up","title":"Clean Up","text":"<p>Assuming your folder structure looks like the one below:</p> <pre><code>|-- manifests\n\u2502   |-- 00-namespace.yml\n\u2502   |-- nginx-deployment.yml\n\u2502   |-- nginx-service.yml\n\u2502   |-- gateway.yml\n\u2502   |-- virtual-service.yml\n</code></pre> <p>Let's delete all the kubernetes and istio resources we created:</p> <pre><code>kubectl delete -f manifests/\n</code></pre>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/introduction-to-fault-injection/","title":"Introduction to Fault Injection","text":"<p>Fault injection testing is a software testing method where the errors and delays are deliberately introduced in a system to check and ensure that the system can withstand and recover from error conditions.</p> <p>You may want to test how delayed response or faults in a service affects your applications. You don't want to experience it in your production environment and therefore you can test your applications with fault injection technique before it goes live.</p> <p>Istio allows you to implement fault injection to test the resiliency of your application. With this feature, you can use application-layer fault injection instead of killing pods, delaying packets, or corrupting packets at the TCP layer.</p> <p>The Fault injection is handled by the VirtualService Istio Object.</p>"},{"location":"kubernetes-on-eks/service-mesh/fault-injection/introduction-to-fault-injection/#types-of-fault-injection","title":"Types of Fault Injection","text":"<p>Istio defines two types of faults injection:</p> <ul> <li><code>Delays</code>: Delays are timing failures such us network latency or overloaded upstreams.</li> <li><code>Aborts</code>: Aborts are crash failures such as HTTP error codes or TCP connection failures.</li> </ul> <p>References:</p> <ul> <li>Fault Injection</li> </ul>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/","title":"Advanced Features in YAML","text":"<p>Now that we're familiar with the basics of YAML, let's explore some advanced features like multiline strings, anchors and aliases, etc.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#multiline-string-in-yaml","title":"Multiline String in YAML","text":"<p>In YAML, you can create a multiline string using the pipe character <code>|</code> or the greater than symbol <code>&gt;</code>.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#multiline-string-using-greater-than-symbol","title":"Multiline String Using Greater Than Symbol","text":"<p>You can use <code>&gt;</code> if you want the line breaks to be stripped out. You'll still have one line break at the end. You can use <code>&gt;-</code> if you want to strip the line break at the end as well.</p> <p>Tip</p> <p>You can use YAML to JSON Converter Tool to see the resulting structure of the multiline string in YAML when it is parsed.</p> <ol> <li> <p>Multiline string using <code>&gt;</code>:     <pre><code># Using the greater than symbol\nmultiline_string: &gt;\n  This is a \n  multiline \n  string \n  in YAML.\n</code></pre></p> <p>YAML will treat the string as:</p> <pre><code>This is a multiline string in YAML.\\n\n</code></pre> </li> <li> <p>Multiline string using <code>&gt;-</code>:     <pre><code># Using the greater than symbol\nmultiline_string: &gt;-\n  This is a \n  multiline \n  string \n  in YAML.\n</code></pre></p> <p>YAML will treat the string as:</p> <pre><code>This is a multiline string in YAML.\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#multiline-string-using-pipe-character","title":"Multiline String Using Pipe Character","text":"<p>You can use <code>|</code> if you want the line breaks to be preserved as <code>\\n</code>. You can use <code>|-</code> if you want to strip the line break at the end.</p> <ol> <li> <p>Multiline string using <code>|</code>:     <pre><code># Using the pipe character\nmultiline_string: |\n  This is a \n  multiline \n  string \n  in YAML.\n</code></pre></p> <p>YAML will treat the string as:</p> <pre><code>This is a \nmultiline \nstring \nin YAML.\\n\n</code></pre> </li> <li> <p>Multiline string using <code>|-</code>:     <pre><code># Using the pipe character\nmultiline_string: |\n  This is a \n  multiline \n  string \n  in YAML.\n</code></pre></p> <p>YAML will treat the string as:</p> <pre><code>This is a \nmultiline \nstring \nin YAML.\n</code></pre> </li> </ol>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#anchors-and-aliases-in-yaml","title":"Anchors and Aliases in YAML","text":"<p>YAML allows you to use <code>anchors</code> and <code>aliases</code> to create references to other parts of the document. This can be useful for avoiding repetition and simplifying complex structures.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#anchor","title":"Anchor","text":"<p>An <code>anchor</code> is a name assigned to a specific value in the YAML document. It's indicated using the <code>&amp;</code> character followed by the <code>anchor</code> name. For example:</p> <pre><code># Define an anchor for a list of values\nlist: &amp;my_list\n  - apple\n  - banana\n  - mango\n</code></pre> <p>In this example, the anchor name is <code>my_list</code> and it refers to the list of values <code>[\"apple\", \"banana\", \"mango\"]</code>.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#alias","title":"Alias","text":"<p>An <code>alias</code> is a reference to an anchor in the YAML document. It's indicated using the <code>*</code> character followed by the <code>anchor</code> name. For example:</p> <pre><code># Use an alias to reference the list of values\nfruits: *my_list\n</code></pre> <p>In this example, the <code>fruits</code> list contains an <code>alias</code> to the <code>my_list</code> anchor. When this YAML document is parsed, the <code>alias</code> will be replaced with the corresponding <code>anchor</code> value, resulting in the following list:</p> <pre><code>fruits:\n  - apple\n  - banana\n  - orange\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/advanced-yaml-features/#using-anchor-and-alias","title":"Using Anchor and Alias","text":"<p>Here's an example document that uses <code>anchor</code> and <code>alias</code>:</p> <code>my-document.yml</code> <pre><code>list: &amp;my_list\n  - apple\n  - banana\n  - mango\n\nfruits: *my_list\n</code></pre> <p>The above YAML is equivalent to the following YAML document:</p> <code>my-equivalent-document.yml</code> <pre><code>list:\n  - apple\n  - banana\n  - mango\n\nfruits:\n  - apple\n  - banana\n  - mango\n</code></pre> <p>Tip</p> <p>You can covert the first document to JSON and then convert the resulting JSON back to YAML to see if they are actually equivalent.</p> <p>References:</p> <ul> <li>YAML to JSON Converter Tool</li> <li>JSON to YAML Converter Tool</li> </ul>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/","title":"Data Types in YAML","text":"<p>In YAML, data types refer to the different kinds of data that can be represented within a YAML document. YAML is a human-readable data serialization format often used for configuration files and data exchange between different programming languages. </p> <p>While YAML itself is not strongly typed like some programming languages, it provides a way to represent various data types, including strings, numbers, booleans, lists, maps (dictionaries), and more.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#scalar-data-type","title":"Scalar Data Type","text":"<p>In YAML, a scalar is a single, atomic value. The value of the scalar can be integer, float, string, or boolean.</p> <p>Here are a few examples of scalars:</p> <pre><code>12\n10.5\nhello\nhello world\n\"hello : world\"\ntrue\nfalse\n2023-12-10\n</code></pre> <p>Note</p> <p>You usually don't need quotes for YAML scalars, but if your data has special characters or could be misunderstood by YAML, use double <code>or single</code> quotes to keep it valid.</p> <p>Here's a YAML document using scalars:</p> <code>student.yml</code> <pre><code>name: Reyansh Kharga\nage: 25\nweight: 67.8\nisPresent: true\nenrollmentDate: 2023-12-10\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#sequence-data-type","title":"Sequence Data Type","text":"<p>In YAML, a sequence is represented by a list of items.</p> <p>There are two ways to represent a sequence in YAML:</p> <ol> <li>Using a hyphen <code>-</code> followed by a space, and each item is placed on a new line with the same indentation level.</li> <li>Using comma-separated items.</li> </ol>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#sequence-using-hyphen","title":"Sequence Using Hyphen","text":"<code>fruits.yml</code> <pre><code>fruits:\n  - apple\n  - banana\n  - mango\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#sequence-using-comma-separated-items","title":"Sequence Using Comma-Separated Items","text":"<code>fruits.yml</code> <pre><code>fruits: [apple, banana, mango]\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#nested-sequence","title":"Nested Sequence","text":"<p>A nested sequence in YAML refers to a situation where you have a sequence that contains another sequence as one of its elements. </p> <p>In simpler terms, it's a list (or array) of items where one or more of those items is another list.</p> <code>fruits.yml</code> <pre><code>fruits:\n  - apples\n  - bananas\n  - oranges\n  - berries:\n    - strawberries\n    - blueberries\n  - grapes\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#map-data-type","title":"Map Data Type","text":"<p>In YAML, the map data type is used to represent an unordered collection of key-value pairs. It is similar to a dictionary in other programming languages like Python.</p> <p>Here are a few examples.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#1-simple-map","title":"1. Simple Map","text":"<p>The following YAML code represents a map with four key-value pairs.</p> <code>student.yml</code> <pre><code>name: Reyansh\nage: 25\nweight: 67.8\nisPresent: true\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#2-complex-map","title":"2. Complex Map","text":"<p>Here's an example of a map with two key-value pairs. The key <code>friends</code> holds a list as its value.</p> <code>person.yml</code> <pre><code>name: Reyansh Kharga\nfriends:\n  - Adam\n  - John\n  - Sachin\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#3-nested-map","title":"3. Nested Map","text":"<p>In this example, <code>name</code> is a nested map with two key-value pairs.</p> <code>person.yml</code> <pre><code>name:\n  firstName: Reyansh\n  lastName: Kharga\nage: 25\nweight: 67.8\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/data-types-in-yaml/#4-more-complex-map","title":"4. More Complex Map","text":"<p>In this example we have multiple level of nesting and includes values which are sequences.</p> <code>pod.yml</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: dev\n  labels:\n    app: my-app\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n    - name: busybox\n      image: busybox:latest\n</code></pre> <p>Note that the item <code>containers</code> is a list of maps.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/introduction-to-yaml/","title":"Introduction to YAML","text":"<p>YAML stands for \"YAML Ain't Markup Language\" (a playful name). It's a human-readable data serialization format.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/introduction-to-yaml/#what-is-data-serialization","title":"What is Data Serialization?","text":"<p>Data serialization is like putting your data into a format that can be easily stored, transmitted, or used by different programming languages. Think of it as packing your information into a neat, transportable box.</p> <p> </p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/introduction-to-yaml/#data-serizlization-languages","title":"Data Serizlization Languages","text":"<p>Some popular data formats for saving and sharing information include <code>JSON</code>, which is easy to use for APIs. <code>XML</code>, which is recognized for its versatility in web services and document storage. <code>YAML</code>, perfect for clear configuration files.</p> <p>Here's a data structure represented in <code>JSON</code>, <code>XML</code>, and <code>YAML</code>.</p> <code>my-document.json</code> <code>my-document.xml</code> <code>my-document.yml</code> <pre><code>{\n  \"Student\": {\n    \"name\": {\n      \"firstName\": \"Adam\",\n      \"lastName\": \"Smith\"\n    },\n    \"friends\": [\n      \"John\",\n      \"Max\",\n      \"Bob\"\n    ]\n  }\n}\n</code></pre> <pre><code>&lt;Student&gt;\n  &lt;name&gt;\n    &lt;firstName&gt;Adam&lt;/firstName&gt;\n    &lt;lastName&gt;Smith&lt;/lastName&gt;\n  &lt;/name&gt;\n  &lt;friends&gt;John&lt;/friends&gt;\n  &lt;friends&gt;Max&lt;/friends&gt;\n  &lt;friends&gt;Bob&lt;/friends&gt;\n&lt;/Student&gt;\n</code></pre> <pre><code>Student:\nname:\n  firstName: Adam\n  lastName: Smith\nfriends:\n  - John\n  - Max\n  - Bob\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/introduction-to-yaml/#why-learn-yaml","title":"Why Learn YAML?","text":"<p>YAML might not be as famous as some other coding languages, but it's a valuable addition to your skill set. </p> <p>Learning YAML can make your life easier if you work with configuration files, data exchange between systems, or even just organizing data. It's all about simplifying complex data structures, making them readable, and reducing errors.</p> <p>YAML is used in some of the most popular tools and technologies in the tech world. For instance:</p> <ul> <li> <p>Kubernetes (K8s): YAML is the common language of Kubernetes. It's used for defining configurations of pods, services, deployments, and more. If you're working with container orchestration, YAML is your go-to.</p> </li> <li> <p>Prometheus: In the world of monitoring and alerting, Prometheus relies on YAML for its configuration. You define what to monitor and how to alert using YAML files.</p> </li> <li> <p>Ansible: This powerful automation tool uses YAML for writing playbooks, which are sets of instructions for configuring and managing systems. YAML makes Ansible playbooks easy to read and write.</p> </li> <li> <p>Helm: Helm is a package manager for Kubernetes, and it uses YAML for defining charts, which are pre-configured applications. With Helm, you can deploy complex applications with a single YAML file.</p> </li> <li> <p>AWS CloudFormation: When it comes to managing AWS infrastructure as code, YAML is commonly used in AWS CloudFormation templates. These templates describe the AWS resources you want to create and configure.</p> </li> </ul> <p>YAML's versatility makes it a go-to choice in various domains, from container orchestration and automation to cloud infrastructure management. Learning YAML opens doors to efficient and structured data management across a wide array of popular tools and platforms. Happy YAML coding!</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/yaml-syntax-and-structure/","title":"YAML Syntax and Structure","text":"<p>In YAML, indentation is used to indicate the structure of a document. </p> <p>Note</p> <p>Indentation refers to the number of spaces or tabs used to align elements in a YAML file.</p> <p>In YAML, spaces are used for indentation, not tabs.</p> <p>The file extension commonly used for YAML files is <code>.yaml</code> or sometimes <code>.yml</code>.</p> <p>If you are using Visual Studio Code as your code editor, make sure to select <code>Indent using spaces</code>. This will ensure that the tabs are considered as spaces. (To indent using spaces, press Cmd+Shift+P on Mac or Ctrl+Shift+P on Windows and Linux, then search for <code>Indent using spaces</code>).</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/yaml-syntax-and-structure/#comment-in-yaml","title":"Comment in YAML","text":"<p>In YAML, you can include comments by using the <code>#</code> symbol. Any text following the <code>#</code> symbol on a line will be treated as a comment and ignored by the parser.</p> <code>my-document.yml</code> <pre><code># This is a comment\nkey: value  # This is another comment\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/yaml-syntax-and-structure/#document-seperator-in-yaml","title":"Document Seperator in YAML","text":"<p>In YAML, <code>---</code> is used as a document separator. It is used to separate multiple documents within a single YAML file.</p> <p>For example, if you have two YAML documents in a single file, you can use <code>---</code> to separate them, as shown below:</p> <code>my-document.yml</code> <pre><code># YAML document 1\nkey1: value1\nkey2: value2\n---\n\n# YAML document 2\nkey3: value3\nkey4: value4\n</code></pre>"},{"location":"kubernetes-on-eks/yaml-fundamentals/yaml-tools-and-resources/","title":"YAML Tools and Resources","text":"<p>There exist excellent online utilities for validating YAML and transforming YAML into JSON. You might find them useful while dealing with YAML.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/yaml-tools-and-resources/#yaml-validator","title":"YAML Validator","text":"<p>You can use the YAML Validator Tool by simply inputting your YAML into the provided box, and it will promptly validate its syntax for you.</p>"},{"location":"kubernetes-on-eks/yaml-fundamentals/yaml-tools-and-resources/#yaml-to-json-converter","title":"YAML to JSON Converter","text":"<p>You can use the YAML to JSON Converter Tool to convert your YAML to JSON by simply inputting your YAML in the input box on the left and you'll immediately get JSON in the output box on the right</p> <p>References:</p> <ul> <li>YAML Validator Tool</li> <li>YAML to JSON Converter Tool</li> </ul>"}]}